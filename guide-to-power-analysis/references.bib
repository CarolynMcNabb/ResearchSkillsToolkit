@misc{10443Failsafe,
  title = {10.4.4.3 {{Fail-safe N}}},
  urldate = {2025-05-11},
  howpublished = {https://handbook-5-1.cochrane.org/chapter\_10/10\_4\_4\_3\_fail\_safe\_n.htm},
  file = {/Users/cristian/Zotero/storage/WBEPGVMU/10_4_4_3_fail_safe_n.html}
}

@misc{10443Failsafe,
  title = {10.4.4.3 {{Fail-safe N}}},
  urldate = {2025-05-11},
  file = {/Users/cristian/Zotero/storage/TT4N6EHN/10_4_4_3_fail_safe_n.html}
}

@misc{10443Failsafe,
  title = {10.4.4.3 {{Fail-safe N}}},
  urldate = {2025-05-11},
  file = {/Users/cristian/Zotero/storage/QDHL3V8N/10_4_4_3_fail_safe_n.html}
}

@article{aaronsonDefiningMeasuringFatigue1999,
  title = {Defining and {{Measuring Fatigue}}},
  author = {Aaronson, Lauren S. and Teel, Cynthia S. and Cassmeyer, Virginia and Neuberger, Geri B. and Pallikkathayil, Leonie and Pierce, Janet and Press, Allan N. and Williams, Phoebe D. and Wingate, Anita},
  year = {1999},
  journal = {Image: the Journal of Nursing Scholarship},
  volume = {31},
  number = {1},
  pages = {45--50},
  issn = {1547-5069},
  doi = {10.1111/j.1547-5069.1999.tb00420.x},
  urldate = {2023-06-20},
  abstract = {In response to a long history of problems with defining and measuring fatigue, the University of Kansas School of Nursing established a Center for Biobehavioral Studies of Fatigue Management to facilitate the study of fatigue in diverse populations. The purpose of this article is to review past efforts to define and measure fatigue and the conceptual problems relevant to currently used measures of fatigue. Several distinct characteristics and corresponding measures of fatigue are identified and a definition and framework for the study of fatigue are discussed. Future research on fatigue must attend to the conceptual distinctions among various measures and the measures of fatigue most appropriate to the goals of a study.},
  langid = {english},
  keywords = {fatigue,health concepts,measurement},
  file = {/Users/cristian/Zotero/storage/FS2H84T6/j.1547-5069.1999.tb00420.html;/Users/cristian/Zotero/storage/JZ99FXD4/j.1547-5069.1999.tb00420.html}
}

@article{aaronsonDefiningMeasuringFatigue1999,
  title = {Defining and {{Measuring Fatigue}}},
  author = {Aaronson, Lauren S. and Teel, Cynthia S. and Cassmeyer, Virginia and Neuberger, Geri B. and Pallikkathayil, Leonie and Pierce, Janet and Press, Allan N. and Williams, Phoebe D. and Wingate, Anita},
  year = {1999},
  journal = {Image: the Journal of Nursing Scholarship},
  volume = {31},
  number = {1},
  pages = {45--50},
  issn = {1547-5069},
  doi = {10.1111/j.1547-5069.1999.tb00420.x},
  urldate = {2023-06-20},
  abstract = {In response to a long history of problems with defining and measuring fatigue, the University of Kansas School of Nursing established a Center for Biobehavioral Studies of Fatigue Management to facilitate the study of fatigue in diverse populations. The purpose of this article is to review past efforts to define and measure fatigue and the conceptual problems relevant to currently used measures of fatigue. Several distinct characteristics and corresponding measures of fatigue are identified and a definition and framework for the study of fatigue are discussed. Future research on fatigue must attend to the conceptual distinctions among various measures and the measures of fatigue most appropriate to the goals of a study.},
  langid = {english},
  keywords = {fatigue,health concepts,measurement},
  file = {/Users/cristian/Zotero/storage/U8G8NR6B/j.1547-5069.1999.tb00420.html;/Users/cristian/Zotero/storage/ZEA4LKIY/j.1547-5069.1999.tb00420.html}
}

@article{aaronsonDefiningMeasuringFatigue1999,
  title = {Defining and {{Measuring Fatigue}}},
  author = {Aaronson, Lauren S. and Teel, Cynthia S. and Cassmeyer, Virginia and Neuberger, Geri B. and Pallikkathayil, Leonie and Pierce, Janet and Press, Allan N. and Williams, Phoebe D. and Wingate, Anita},
  year = {1999},
  journal = {Image: the Journal of Nursing Scholarship},
  volume = {31},
  number = {1},
  pages = {45--50},
  issn = {1547-5069},
  doi = {10.1111/j.1547-5069.1999.tb00420.x},
  urldate = {2023-06-20},
  abstract = {In response to a long history of problems with defining and measuring fatigue, the University of Kansas School of Nursing established a Center for Biobehavioral Studies of Fatigue Management to facilitate the study of fatigue in diverse populations. The purpose of this article is to review past efforts to define and measure fatigue and the conceptual problems relevant to currently used measures of fatigue. Several distinct characteristics and corresponding measures of fatigue are identified and a definition and framework for the study of fatigue are discussed. Future research on fatigue must attend to the conceptual distinctions among various measures and the measures of fatigue most appropriate to the goals of a study.},
  langid = {english},
  keywords = {fatigue,health concepts,measurement},
  file = {/Users/cristian/Zotero/storage/SS89HPBW/j.1547-5069.1999.tb00420.html;/Users/cristian/Zotero/storage/ZMNTE5FV/j.1547-5069.1999.tb00420.html}
}

@article{Abt2020,
  title = {Power, Precision, and Sample Size Estimation in Sport and Exercise Science Research},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Nevill, Alan and Wallace, Eric and Williams, Mark},
  year = {2020},
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {38},
  number = {17},
  pages = {1933--1935},
  publisher = {Routledge},
  doi = {10.1080/02640414.2020.1776002},
  pmid = {32558628},
  file = {/Users/cristian/Zotero/storage/5HM4ISID/Abt et al. - 2020 - Power, precision, and sample size estimation in sp.pdf;/Users/cristian/Zotero/storage/LTDUFQ7K/02640414.2020.html;/Users/cristian/Zotero/storage/QG9JSFLS/02640414.2020.html}
}

@article{Abt2020,
  title = {Power, Precision, and Sample Size Estimation in Sport and Exercise Science Research},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Nevill, Alan and Wallace, Eric and Williams, Mark},
  year = {2020},
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {38},
  number = {17},
  pages = {1933--1935},
  publisher = {Routledge},
  doi = {10.1080/02640414.2020.1776002},
  pmid = {32558628},
  file = {/Users/cristian/Zotero/storage/J36DVCK7/Abt et al. - 2020 - Power, precision, and sample size estimation in sp.pdf;/Users/cristian/Zotero/storage/I975XGPI/02640414.2020.html;/Users/cristian/Zotero/storage/KA29NBZD/02640414.2020.html}
}

@article{Abt2020,
  title = {Power, Precision, and Sample Size Estimation in Sport and Exercise Science Research},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Nevill, Alan and Wallace, Eric and Williams, Mark},
  year = {2020},
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {38},
  number = {17},
  pages = {1933--1935},
  publisher = {Routledge},
  doi = {10.1080/02640414.2020.1776002},
  pmid = {32558628},
  file = {/Users/cristian/Zotero/storage/3ECFYRUJ/Abt et al. - 2020 - Power, precision, and sample size estimation in sp.pdf;/Users/cristian/Zotero/storage/BWHZA22W/02640414.2020.html;/Users/cristian/Zotero/storage/D4WEUIZ8/02640414.2020.html}
}

@article{abtRegisteredReportsJournal2021,
  ids = {abtRegisteredReportsJournal2021b},
  title = {Registered Reports in the {{Journal}} of {{Sports Sciences}}},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Wallace, Eric and Williams, A. Mark},
  year = {2021},
  volume = {39},
  number = {16},
  pages = {1789--1790},
  publisher = {Routledge},
  doi = {10.1080/02640414.2021.1950974},
  pmid = {34379576},
  file = {/Users/cristian/Zotero/storage/ULEKEUKR/Abt et al. - 2021 - Registered Reports in the Journal of Sports Scienc.pdf;/Users/cristian/Zotero/storage/5J22T76V/02640414.2021.html;/Users/cristian/Zotero/storage/NWMVCUSS/02640414.2021.html}
}

@article{abtRegisteredReportsJournal2021,
  ids = {abtRegisteredReportsJournal2021b},
  title = {Registered Reports in the {{Journal}} of {{Sports Sciences}}},
  author = {Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Wallace, Eric and Williams, A. Mark},
  year = {2021},
  volume = {39},
  number = {16},
  pages = {1789--1790},
  publisher = {Routledge},
  doi = {10.1080/02640414.2021.1950974},
  pmid = {34379576},
  file = {/Users/cristian/Zotero/storage/GHZDTPDS/Abt et al. - 2021 - Registered Reports in the Journal of Sports Scienc.pdf;/Users/cristian/Zotero/storage/ISW8PX65/02640414.2021.html;/Users/cristian/Zotero/storage/KUJWM3BN/02640414.2021.html}
}

@article{abtSampleSizeEstimation,
  title = {Sample Size Estimation Revisited},
  author = {Abt, Grant and , Colin, Boreham and , Gareth, Davison and , Robin, Jackson and , Simon, Jobson and , Eric, Wallace and {and Williams}, Mark},
  journal = {Journal of Sports Sciences},
  volume = {0},
  number = {0},
  pages = {1--6},
  publisher = {Routledge},
  issn = {0264-0414},
  doi = {10.1080/02640414.2025.2499403},
  urldate = {2025-06-24},
  pmid = {40329799},
  file = {/Users/cristian/Zotero/storage/VKT5C9UE/Abt et al. - Sample size estimation revisited.pdf}
}

@article{acklandArterialPulsePressure2018,
  title = {Arterial Pulse Pressure and Postoperative Morbidity in High-Risk Surgical Patients},
  author = {Ackland, G.L. and Abbott, T.E.F. and Pearse, R.M. and Karmali, S.N. and Whittle, J. and Minto, G. and Minto, G. and King, A. and Pollak, C. and Williams, C. and Patrick, A. and West, C. and Vickers, E. and Green, R. and Clark, M. and Ackland, G. and Whittle, J. and Paredes, L.G. and Stephens, R.C.M. and Jones, A. and Otto, J. and Lach, A. and {del Arroyo}, A.G. and Toner, A. and Williams, A. and Owen, T. and Pradhu, P. and Hull, D. and Montague, L. and {POM-HR Study Investigators}},
  year = {2018},
  journal = {British Journal of Anaesthesia},
  volume = {120},
  number = {1},
  pages = {94--100},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.bja.2017.11.009}
}

@article{acklandArterialPulsePressure2018,
  title = {Arterial Pulse Pressure and Postoperative Morbidity in High-Risk Surgical Patients},
  author = {Ackland, G.L. and Abbott, T.E.F. and Pearse, R.M. and Karmali, S.N. and Whittle, J. and Minto, G. and Minto, G. and King, A. and Pollak, C. and Williams, C. and Patrick, A. and West, C. and Vickers, E. and Green, R. and Clark, M. and Ackland, G. and Whittle, J. and Paredes, L.G. and Stephens, R.C.M. and Jones, A. and Otto, J. and Lach, A. and {del Arroyo}, A.G. and Toner, A. and Williams, A. and Owen, T. and Pradhu, P. and Hull, D. and Montague, L. and {POM-HR Study Investigators}},
  year = {2018},
  journal = {British Journal of Anaesthesia},
  volume = {120},
  number = {1},
  pages = {94--100},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.bja.2017.11.009}
}

@article{aczelQuantifyingSupportNull2018,
  title = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}: {{An Empirical Investigation}}},
  shorttitle = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}},
  author = {Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F. and {van den Bergh}, Don and Wagenmakers, Eric-Jan},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {357--366},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918773742},
  urldate = {2023-04-28},
  abstract = {In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin \& Review, Journal of Experimental Psychology: General, and Psychological Science (N = 137). In 72\% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5\% of the nonsignificant findings provided strong evidence (i.e., BF01 {$>$} 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/2MSLLWGN/Aczel et al. - 2018 - Quantifying Support for the Null Hypothesis in Psy.pdf}
}

@article{aczelQuantifyingSupportNull2018,
  title = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}: {{An Empirical Investigation}}},
  shorttitle = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}},
  author = {Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F. and {van den Bergh}, Don and Wagenmakers, Eric-Jan},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {357--366},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918773742},
  urldate = {2023-04-28},
  abstract = {In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin \& Review, Journal of Experimental Psychology: General, and Psychological Science (N = 137). In 72\% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5\% of the nonsignificant findings provided strong evidence (i.e., BF01 {$>$} 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/WXCGUV7P/Aczel et al. - 2018 - Quantifying Support for the Null Hypothesis in Psy.pdf}
}

@article{adekunleEffectivenessWarmSaline2021,
  ids = {adekunleEffectivenessWarmSaline2021a},
  title = {Effectiveness of Warm Saline Mouth Bath in Preventing Alveolar Osteitis: {{A}} Systematic Review and Meta-Analysis},
  author = {Adekunle, A.A. and Egbunah, U.P. and Erinoso, O.A. and Adeyemo, W.L.},
  year = {2021},
  journal = {Journal of Cranio-Maxillofacial Surgery},
  volume = {49},
  number = {10},
  pages = {980--988},
  publisher = {Churchill Livingstone},
  doi = {10.1016/j.jcms.2021.09.001}
}

@article{adekunleEffectivenessWarmSaline2021,
  ids = {adekunleEffectivenessWarmSaline2021a},
  title = {Effectiveness of Warm Saline Mouth Bath in Preventing Alveolar Osteitis: {{A}} Systematic Review and Meta-Analysis},
  author = {Adekunle, A.A. and Egbunah, U.P. and Erinoso, O.A. and Adeyemo, W.L.},
  year = {2021},
  journal = {Journal of Cranio-Maxillofacial Surgery},
  volume = {49},
  number = {10},
  pages = {980--988},
  publisher = {Churchill Livingstone},
  doi = {10.1016/j.jcms.2021.09.001}
}

@misc{aertPuniformMetaAnalysisMethods2022,
  title = {Puniform: {{Meta-Analysis Methods Correcting}} for {{Publication Bias}}},
  shorttitle = {Puniform},
  author = {{van Aert}, Robbie C. M.},
  year = {2022},
  month = mar,
  urldate = {2022-05-23},
  abstract = {Provides meta-analysis methods that correct for publication bias and outcome reporting bias. Four methods and a visual tool are currently included in the package. The p-uniform method as described in van Assen, van Aert, and Wicherts (2015) {$<$}https:psycnet.apa.org/record/2014-48759-001{$>$} can be used for estimating the average effect size, testing the null hypothesis of no effect, and testing for publication bias using only the statistically significant effect sizes of primary studies. The second method in the package is the p-uniform* method as described in van Aert and van Assen (2019) {$<$}doi:10.31222/osf.io/zqjr9{$>$}. This method is an extension of the p-uniform method that allows for estimation of the average effect size and the between-study variance in a meta-analysis, and uses both the statistically significant and nonsignificant effect sizes. The third method in the package is the hybrid method as described in van Aert and van Assen (2017) {$<$}doi:10.3758/s13428-017-0967-6{$>$}. The hybrid method is a meta-analysis method for combining an original study and replication and while taking into account statistical significance of the original study. The p-uniform and hybrid method are based on the statistical theory that the distribution of p-values is uniform conditional on the population effect size. The fourth method in the package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert and van Assen (2018) {$<$}doi:10.1371/journal.pone.0175302{$>$}. This method computes posterior probabilities for four true effect sizes (no, small, medium, and large) based on an original study and replication while taking into account publication bias in the original study. The method can also be used for computing the required sample size of the replication akin to power analysis in null hypothesis significance testing. The meta-plot is a visual tool for meta-analysis that provides information on the primary studies in the meta-analysis, the results of the meta-analysis, and characteristics of the research on the effect under study (van Assen et al., 2021). Helper functions to apply the Correcting for Outcome Reporting Bias (CORB) method to correct for outcome reporting bias in a meta-analysis (van Aert \& Wicherts, 2021).},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {MetaAnalysis}
}

@misc{aertPuniformMetaAnalysisMethods2022,
  title = {Puniform: {{Meta-Analysis Methods Correcting}} for {{Publication Bias}}},
  shorttitle = {Puniform},
  author = {{van Aert}, Robbie C. M.},
  year = {2022},
  month = mar,
  urldate = {2022-05-23},
  abstract = {Provides meta-analysis methods that correct for publication bias and outcome reporting bias. Four methods and a visual tool are currently included in the package. The p-uniform method as described in van Assen, van Aert, and Wicherts (2015) {$<$}https:psycnet.apa.org/record/2014-48759-001{$>$} can be used for estimating the average effect size, testing the null hypothesis of no effect, and testing for publication bias using only the statistically significant effect sizes of primary studies. The second method in the package is the p-uniform* method as described in van Aert and van Assen (2019) {$<$}doi:10.31222/osf.io/zqjr9{$>$}. This method is an extension of the p-uniform method that allows for estimation of the average effect size and the between-study variance in a meta-analysis, and uses both the statistically significant and nonsignificant effect sizes. The third method in the package is the hybrid method as described in van Aert and van Assen (2017) {$<$}doi:10.3758/s13428-017-0967-6{$>$}. The hybrid method is a meta-analysis method for combining an original study and replication and while taking into account statistical significance of the original study. The p-uniform and hybrid method are based on the statistical theory that the distribution of p-values is uniform conditional on the population effect size. The fourth method in the package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert and van Assen (2018) {$<$}doi:10.1371/journal.pone.0175302{$>$}. This method computes posterior probabilities for four true effect sizes (no, small, medium, and large) based on an original study and replication while taking into account publication bias in the original study. The method can also be used for computing the required sample size of the replication akin to power analysis in null hypothesis significance testing. The meta-plot is a visual tool for meta-analysis that provides information on the primary studies in the meta-analysis, the results of the meta-analysis, and characteristics of the research on the effect under study (van Assen et al., 2021). Helper functions to apply the Correcting for Outcome Reporting Bias (CORB) method to correct for outcome reporting bias in a meta-analysis (van Aert \& Wicherts, 2021).},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {MetaAnalysis}
}

@misc{aertPuniformMetaAnalysisMethods2022,
  title = {Puniform: {{Meta-Analysis Methods Correcting}} for {{Publication Bias}}},
  shorttitle = {Puniform},
  author = {{van Aert}, Robbie C. M.},
  year = {2022},
  month = mar,
  urldate = {2022-05-23},
  abstract = {Provides meta-analysis methods that correct for publication bias and outcome reporting bias. Four methods and a visual tool are currently included in the package. The p-uniform method as described in van Assen, van Aert, and Wicherts (2015) {$<$}https:psycnet.apa.org/record/2014-48759-001{$>$} can be used for estimating the average effect size, testing the null hypothesis of no effect, and testing for publication bias using only the statistically significant effect sizes of primary studies. The second method in the package is the p-uniform* method as described in van Aert and van Assen (2019) {$<$}doi:10.31222/osf.io/zqjr9{$>$}. This method is an extension of the p-uniform method that allows for estimation of the average effect size and the between-study variance in a meta-analysis, and uses both the statistically significant and nonsignificant effect sizes. The third method in the package is the hybrid method as described in van Aert and van Assen (2017) {$<$}doi:10.3758/s13428-017-0967-6{$>$}. The hybrid method is a meta-analysis method for combining an original study and replication and while taking into account statistical significance of the original study. The p-uniform and hybrid method are based on the statistical theory that the distribution of p-values is uniform conditional on the population effect size. The fourth method in the package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert and van Assen (2018) {$<$}doi:10.1371/journal.pone.0175302{$>$}. This method computes posterior probabilities for four true effect sizes (no, small, medium, and large) based on an original study and replication while taking into account publication bias in the original study. The method can also be used for computing the required sample size of the replication akin to power analysis in null hypothesis significance testing. The meta-plot is a visual tool for meta-analysis that provides information on the primary studies in the meta-analysis, the results of the meta-analysis, and characteristics of the research on the effect under study (van Assen et al., 2021). Helper functions to apply the Correcting for Outcome Reporting Bias (CORB) method to correct for outcome reporting bias in a meta-analysis (van Aert \& Wicherts, 2021).},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {MetaAnalysis}
}

@article{agnoliQuestionableResearchPractices2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172792},
  urldate = {2023-03-02},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  langid = {english},
  keywords = {Behavior,Experimental psychology,Italian people,Psychologists,Psychology,Psychometrics,Questionnaires,United States},
  file = {/Users/cristian/Zotero/storage/VRVTCXPL/Agnoli et al. - 2017 - Questionable research practices among italian rese.pdf}
}

@article{agnoliQuestionableResearchPractices2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172792},
  urldate = {2023-03-02},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  langid = {english},
  keywords = {Behavior,Experimental psychology,Italian people,Psychologists,Psychology,Psychometrics,Questionnaires,United States},
  file = {/Users/cristian/Zotero/storage/D3HMQMCA/Agnoli et al. - 2017 - Questionable research practices among italian rese.pdf}
}

@article{aguinisEffectSizePower2005,
  title = {Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review},
  shorttitle = {Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression},
  author = {Aguinis, Herman and Beaty, James C. and Boik, Robert J. and Pierce, Charles A.},
  year = {2005},
  month = jan,
  journal = {The Journal of Applied Psychology},
  volume = {90},
  number = {1},
  pages = {94--107},
  issn = {0021-9010},
  doi = {10.1037/0021-9010.90.1.94},
  abstract = {The authors conducted a 30-year review (1969-1998) of the size of moderating effects of categorical variables as assessed using multiple regression. The median observed effect size (f(2)) is only .002, but 72\% of the moderator tests reviewed had power of .80 or greater to detect a targeted effect conventionally defined as small. Results suggest the need to minimize the influence of artifacts that produce a downward bias in the observed effect size and put into question the use of conventional definitions of moderating effect sizes. As long as an effect has a meaningful impact, the authors advise researchers to conduct a power analysis and plan future research designs on the basis of smaller and more realistic targeted effect sizes.},
  langid = {english},
  pmid = {15641892},
  keywords = {Humans,Psychology,Regression Analysis,Reproducibility of Results,Sample Size}
}

@article{aguinisEffectSizePower2005,
  title = {Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review},
  shorttitle = {Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression},
  author = {Aguinis, Herman and Beaty, James C. and Boik, Robert J. and Pierce, Charles A.},
  year = {2005},
  month = jan,
  journal = {The Journal of Applied Psychology},
  volume = {90},
  number = {1},
  pages = {94--107},
  issn = {0021-9010},
  doi = {10.1037/0021-9010.90.1.94},
  abstract = {The authors conducted a 30-year review (1969-1998) of the size of moderating effects of categorical variables as assessed using multiple regression. The median observed effect size (f(2)) is only .002, but 72\% of the moderator tests reviewed had power of .80 or greater to detect a targeted effect conventionally defined as small. Results suggest the need to minimize the influence of artifacts that produce a downward bias in the observed effect size and put into question the use of conventional definitions of moderating effect sizes. As long as an effect has a meaningful impact, the authors advise researchers to conduct a power analysis and plan future research designs on the basis of smaller and more realistic targeted effect sizes.},
  langid = {english},
  pmid = {15641892},
  keywords = {Humans,Psychology,Regression Analysis,Reproducibility of Results,Sample Size}
}

@article{ahnReviewMetaanalysesEducation2012,
  title = {A Review of Meta-Analyses in Education: {{Methodological}} Strengths and Weaknesses},
  shorttitle = {A Review of Meta-Analyses in Education},
  author = {Ahn, Soyeon and Ames, Allison J. and Myers, Nicholas D.},
  year = {2012},
  journal = {Review of Educational Research},
  volume = {82},
  number = {4},
  pages = {436--476},
  publisher = {Sage Publications},
  address = {US},
  issn = {1935-1046},
  doi = {10.3102/0034654312458162},
  abstract = {The current review addresses the validity of published meta-analyses in education that determines the credibility and generalizability of study findings using a total of 56 meta-analyses published in education in the 2000s. Our objectives were to evaluate the current meta-analytic practices in education, identify methodological strengths and weaknesses, and provide recommendations for improvements in order to generate a more valid and credible knowledge base of what works in practice. It was found that 56 meta-analyses followed general recommendations fairly well in problem formulation and data collection, but much improvement is needed in data evaluation and analysis. Particularly, lack of information reported as well as little transparency in the use of statistical methods are concerns for generating credible and generalizable meta-analytic findings that can be transformed to educational practices. Recommendations for yielding more reliable and valid inferences from meta-analyses are provided. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Credibility,Education,Meta Analysis,Methodology,Test Validity},
  file = {/Users/cristian/Zotero/storage/ND6SXV64/2012-31558-003.html;/Users/cristian/Zotero/storage/UI637R2E/2012-31558-003.html}
}

@article{ahnReviewMetaanalysesEducation2012,
  title = {A Review of Meta-Analyses in Education: {{Methodological}} Strengths and Weaknesses},
  shorttitle = {A Review of Meta-Analyses in Education},
  author = {Ahn, Soyeon and Ames, Allison J. and Myers, Nicholas D.},
  year = {2012},
  journal = {Review of Educational Research},
  volume = {82},
  number = {4},
  pages = {436--476},
  publisher = {Sage Publications},
  address = {US},
  issn = {1935-1046},
  doi = {10.3102/0034654312458162},
  abstract = {The current review addresses the validity of published meta-analyses in education that determines the credibility and generalizability of study findings using a total of 56 meta-analyses published in education in the 2000s. Our objectives were to evaluate the current meta-analytic practices in education, identify methodological strengths and weaknesses, and provide recommendations for improvements in order to generate a more valid and credible knowledge base of what works in practice. It was found that 56 meta-analyses followed general recommendations fairly well in problem formulation and data collection, but much improvement is needed in data evaluation and analysis. Particularly, lack of information reported as well as little transparency in the use of statistical methods are concerns for generating credible and generalizable meta-analytic findings that can be transformed to educational practices. Recommendations for yielding more reliable and valid inferences from meta-analyses are provided. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Credibility,Education,Meta Analysis,Methodology,Test Validity},
  file = {/Users/cristian/Zotero/storage/JMUXYIAW/2012-31558-003.html;/Users/cristian/Zotero/storage/NR4G5SWZ/2012-31558-003.html}
}

@article{ahokasWaterImmersionMethods2020,
  title = {Water Immersion Methods Do Not Alter Muscle Damage and Inflammation Biomarkers after High-Intensity Sprinting and Jumping Exercise},
  author = {Ahokas, E. K. and Kyr{\"o}l{\"a}inen, H. and Mero, A. A. and Walker, S. and Hanstock, H. G. and Ihalainen, J. K.},
  year = {2020},
  journal = {European Journal of Applied Physiology},
  volume = {120},
  number = {12},
  pages = {2625--2634},
  issn = {1439-6319},
  doi = {10.1007/s00421-020-04481-8},
  urldate = {2023-08-15},
  abstract = {Purpose The aim of this study was to compare the efficacy of three water immersion interventions performed after active recovery compared to active recovery only on the resolution of inflammation and markers of muscle damage post-exercise. Methods Nine physically active men (n = 9; age 20-35~years) performed an intensive loading protocol, including maximal jumps and sprinting on four occasions. After each trial, one of three recovery interventions (10~min duration) was used in a random order: cold-water immersion (CWI, 10 {$^\circ$}C), thermoneutral water immersion (TWI, 24 {$^\circ$}C), contrast water therapy (CWT, alternately 10 {$^\circ$}C and 38 {$^\circ$}C). All of these methods were performed after an active recovery (10~min bicycle ergometer), and were compared to active recovery only (ACT). 5~min, 1, 24, 48, and 96~h after exercise bouts, immune response and recovery were assessed through leukocyte subsets, monocyte chemoattractant protein-1, myoglobin and high-sensitivity C-reactive protein concentrations. Results Significant changes in all blood markers occurred at post-loading (p\,{$<$}\,0.05), but there were no significant differences observed in the recovery between methods. However, retrospective analysis revealed significant trial-order effects for myoglobin and neutrophils (p {$<$} 0.01). Only lymphocytes displayed satisfactory reliability in the exercise response, with intraclass correlation coefficient {$>$} 0.5. Conclusions The recovery methods did not affect the resolution of inflammatory and immune responses after high-intensity sprinting and jumping exercise. It is notable that the biomarker responses were variable within individuals. Thus, the lack of differences between recovery methods may have been influenced by the reliability of exercise-induced biomarker responses. Electronic supplementary material The online version of this article (10.1007/s00421-020-04481-8) contains supplementary material, which is available to authorized users.},
  pmcid = {PMC7674333},
  pmid = {32880050},
  file = {/Users/cristian/Zotero/storage/SQXA7K2I/Ahokas et al. - 2020 - Water immersion methods do not alter muscle damage.pdf}
}

@article{ahokasWaterImmersionMethods2020,
  title = {Water Immersion Methods Do Not Alter Muscle Damage and Inflammation Biomarkers after High-Intensity Sprinting and Jumping Exercise},
  author = {Ahokas, E. K. and Kyr{\"o}l{\"a}inen, H. and Mero, A. A. and Walker, S. and Hanstock, H. G. and Ihalainen, J. K.},
  year = {2020},
  journal = {European Journal of Applied Physiology},
  volume = {120},
  number = {12},
  pages = {2625--2634},
  issn = {1439-6319},
  doi = {10.1007/s00421-020-04481-8},
  urldate = {2023-08-15},
  abstract = {Purpose The aim of this study was to compare the efficacy of three water immersion interventions performed after active recovery compared to active recovery only on the resolution of inflammation and markers of muscle damage post-exercise. Methods Nine physically active men (n = 9; age 20-35 years) performed an intensive loading protocol, including maximal jumps and sprinting on four occasions. After each trial, one of three recovery interventions (10 min duration) was used in a random order: cold-water immersion (CWI, 10 {$^\circ$}C), thermoneutral water immersion (TWI, 24 {$^\circ$}C), contrast water therapy (CWT, alternately 10 {$^\circ$}C and 38 {$^\circ$}C). All of these methods were performed after an active recovery (10 min bicycle ergometer), and were compared to active recovery only (ACT). 5 min, 1, 24, 48, and 96 h after exercise bouts, immune response and recovery were assessed through leukocyte subsets, monocyte chemoattractant protein-1, myoglobin and high-sensitivity C-reactive protein concentrations. Results Significant changes in all blood markers occurred at post-loading (p\,{$<$}\,0.05), but there were no significant differences observed in the recovery between methods. However, retrospective analysis revealed significant trial-order effects for myoglobin and neutrophils (p {$<$} 0.01). Only lymphocytes displayed satisfactory reliability in the exercise response, with intraclass correlation coefficient {$>$} 0.5. Conclusions The recovery methods did not affect the resolution of inflammatory and immune responses after high-intensity sprinting and jumping exercise. It is notable that the biomarker responses were variable within individuals. Thus, the lack of differences between recovery methods may have been influenced by the reliability of exercise-induced biomarker responses. Electronic supplementary material The online version of this article (10.1007/s00421-020-04481-8) contains supplementary material, which is available to authorized users.},
  pmcid = {PMC7674333},
  pmid = {32880050},
  file = {/Users/cristian/Zotero/storage/4ZWP2UF6/Ahokas et al. - 2020 - Water immersion methods do not alter muscle damage.pdf}
}

@article{aibar-almazanEffectsPilatesFall2019,
  title = {Effects of {{Pilates}} on Fall Risk Factors in Community-Dwelling Elderly Women: {{A}} Randomized, Controlled Trial},
  shorttitle = {Effects of {{Pilates}} on Fall Risk Factors in Community-Dwelling Elderly Women},
  author = {{Aibar-Almaz{\'a}n}, Agust{\'i}n and {Mart{\'i}nez-Amat}, Antonio and {Cruz-D{\'i}az}, David and {De la Torre-Cruz}, Manuel J. and {Jim{\'e}nez-Garc{\'i}a}, Jos{\'e} D. and {Zagalaz-Anula}, Noelia and {P{\'e}rez-Herrezuelo}, Isabel and {Hita-Contreras}, Fidel},
  year = {2019},
  month = nov,
  journal = {European Journal of Sport Science},
  volume = {19},
  number = {10},
  pages = {1386--1394},
  issn = {1536-7290},
  doi = {10.1080/17461391.2019.1595739},
  abstract = {OBJECTIVE: the main objective was to analyze the effects that an exercise programme based on the Pilates method would have on balance confidence, fear of falling, and postural control among women {$\geq$}60 years old. METHODS: a total of 110 women (69.15\,{\textpm}\,8.94 years) participated in this randomized, controlled trial that took place in Ja{\'e}n (Spain). The participants were randomly assigned to either a control group (n\,=\,55), which received no intervention, or to a Pilates group (n\,=\,55), which carried out an exercise programme based on the Pilates method in 60-minute sessions for 12 weeks. The Falls Efficacy Scale-International and the activity-specific balance confidence scale were respectively used to assess fear of falling and balance confidence in performing activities of daily living. Postural control was evaluated using a stabilometric platform. RESULTS: Regarding balance confidence, the Pilates group showed higher values compared to the control group (77.52\,{\textpm}\,18.27 vs 72.35\,{\textpm}\,16.39, Cohen's d\,=\,0.030). Women in the Pilates group showed lower fear of falling, compared to those of the control group (22.07\,{\textpm}\,5.73 vs 27.9\,{\textpm}\,6.95, Cohen's d\,=\,0.041). Finally, concerning static balance, participants of the Pilates group experienced statistically significant improvements on the velocity and anteroposterior movements of the centre of pressure with eyes open and closed respectively (Cohen's d\,=\,0.44 and 0.35 respectively). CONCLUSION: A 12-week Pilates training programme has beneficial effects on balance confidence, fear of falling and postural stability, in elderly women.},
  langid = {english},
  pmid = {30990762},
  keywords = {Accidental Falls,Activities of Daily Living,Aged,Exercise Movement Techniques,falls,Fear,fear of falling,Female,Humans,Menopause,Middle Aged,Pilates,postural balance,Postural Balance,Risk Factors}
}

@article{aibar-almazanEffectsPilatesFall2019,
  title = {Effects of {{Pilates}} on Fall Risk Factors in Community-Dwelling Elderly Women: {{A}} Randomized, Controlled Trial},
  shorttitle = {Effects of {{Pilates}} on Fall Risk Factors in Community-Dwelling Elderly Women},
  author = {{Aibar-Almaz{\'a}n}, Agust{\'i}n and {Mart{\'i}nez-Amat}, Antonio and {Cruz-D{\'i}az}, David and {De la Torre-Cruz}, Manuel J. and {Jim{\'e}nez-Garc{\'i}a}, Jos{\'e} D. and {Zagalaz-Anula}, Noelia and {P{\'e}rez-Herrezuelo}, Isabel and {Hita-Contreras}, Fidel},
  year = {2019},
  month = nov,
  journal = {European Journal of Sport Science},
  volume = {19},
  number = {10},
  pages = {1386--1394},
  issn = {1536-7290},
  doi = {10.1080/17461391.2019.1595739},
  abstract = {OBJECTIVE: the main objective was to analyze the effects that an exercise programme based on the Pilates method would have on balance confidence, fear of falling, and postural control among women {$\geq$}60 years old. METHODS: a total of 110 women (69.15\,{\textpm}\,8.94 years) participated in this randomized, controlled trial that took place in Ja{\'e}n (Spain). The participants were randomly assigned to either a control group (n\,=\,55), which received no intervention, or to a Pilates group (n\,=\,55), which carried out an exercise programme based on the Pilates method in 60-minute sessions for 12 weeks. The Falls Efficacy Scale-International and the activity-specific balance confidence scale were respectively used to assess fear of falling and balance confidence in performing activities of daily living. Postural control was evaluated using a stabilometric platform. RESULTS: Regarding balance confidence, the Pilates group showed higher values compared to the control group (77.52\,{\textpm}\,18.27 vs 72.35\,{\textpm}\,16.39, Cohen's d\,=\,0.030). Women in the Pilates group showed lower fear of falling, compared to those of the control group (22.07\,{\textpm}\,5.73 vs 27.9\,{\textpm}\,6.95, Cohen's d\,=\,0.041). Finally, concerning static balance, participants of the Pilates group experienced statistically significant improvements on the velocity and anteroposterior movements of the centre of pressure with eyes open and closed respectively (Cohen's d\,=\,0.44 and 0.35 respectively). CONCLUSION: A 12-week Pilates training programme has beneficial effects on balance confidence, fear of falling and postural stability, in elderly women.},
  langid = {english},
  pmid = {30990762},
  keywords = {Accidental Falls,Activities of Daily Living,Aged,Exercise Movement Techniques,falls,Fear,fear of falling,Female,Humans,Menopause,Middle Aged,Pilates,postural balance,Postural Balance,Risk Factors}
}

@article{al-rawiDissolvingMicroneedlesAntibacterial2022,
  title = {Dissolving Microneedles with Antibacterial Functionalities: {{A}} Systematic Review of Laboratory Studies},
  author = {{Al-Rawi}, N.N. and {Rawas-Qalaji}, M.},
  year = {2022},
  journal = {European Journal of Pharmaceutical Sciences},
  volume = {174},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.ejps.2022.106202}
}

@article{al-rawiDissolvingMicroneedlesAntibacterial2022,
  title = {Dissolving Microneedles with Antibacterial Functionalities: {{A}} Systematic Review of Laboratory Studies},
  author = {{Al-Rawi}, N.N. and {Rawas-Qalaji}, M.},
  year = {2022},
  journal = {European Journal of Pharmaceutical Sciences},
  volume = {174},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.ejps.2022.106202}
}

@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta^2$}, {$\omega^2$} and {$\varepsilon^2$}) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta^2$} in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Sample Size,Statistical Analysis,Statistical Data,Statistical Estimation},
  file = {/Users/cristian/Zotero/storage/8JQJETEU/Albers and Lakens - 2018 - When power analyses based on pilot data are biased.pdf;/Users/cristian/Zotero/storage/JYJTYXKZ/Albers and Lakens - 2018 - When power analyses based on pilot data are biased.pdf;/Users/cristian/Zotero/storage/GUNEIMLT/2017-53838-020.html;/Users/cristian/Zotero/storage/JMZ4M29G/2017-53838-020.html}
}

@article{algarniMixedmethodsSystematicReview2021,
  ids = {algarniMixedmethodsSystematicReview2021a},
  title = {A Mixed-Methods Systematic Review of the Prevalence, Reasons, Associated Harms and Risk-Reduction Interventions of over-the-Counter ({{OTC}}) Medicines Misuse, Abuse and Dependence in Adults},
  author = {Algarni, M. and Hadi, M.A. and Yahyouche, A. and Mahmood, S. and Jalal, Z.},
  year = {2021},
  journal = {Journal of Pharmaceutical Policy and Practice},
  volume = {14},
  number = {1},
  publisher = {BioMed Central Ltd},
  doi = {10.1186/s40545-021-00350-7}
}

@article{algarniMixedmethodsSystematicReview2021,
  ids = {algarniMixedmethodsSystematicReview2021a},
  title = {A Mixed-Methods Systematic Review of the Prevalence, Reasons, Associated Harms and Risk-Reduction Interventions of over-the-{{Counter}} ({{OTC}}) Medicines Misuse, Abuse and Dependence in Adults},
  author = {Algarni, M. and Hadi, M.A. and Yahyouche, A. and Mahmood, S. and Jalal, Z.},
  year = {2021},
  journal = {Journal of Pharmaceutical Policy and Practice},
  volume = {14},
  number = {1},
  publisher = {BioMed Central Ltd},
  doi = {10.1186/s40545-021-00350-7}
}

@article{allenOpenScienceChallenges2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2022-11-16},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  langid = {english},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data},
  file = {/Users/cristian/Zotero/storage/KADHJAUX/Allen and Mehler - 2019 - Open science challenges, benefits and tips in earl.pdf;/Users/cristian/Zotero/storage/2GNJAGIW/article.html;/Users/cristian/Zotero/storage/S28XABMQ/article.html}
}

@article{allenOpenScienceChallenges2019a,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2022-11-16},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  langid = {english},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data},
  file = {/Users/cristian/Zotero/storage/7EIEDIDA/Allen and Mehler - 2019 - Open science challenges, benefits and tips in earl.pdf;/Users/cristian/Zotero/storage/AQ7WB5V3/article.html;/Users/cristian/Zotero/storage/X9Y9QIBQ/article.html}
}

@article{almirallAuditoryVisualReaction1987,
  title = {Auditory and {{Visual Reaction Time}} in {{Adults}} during {{Long Performance}}},
  author = {Almirall, Helena and Guti{\'e}rrez, Emilia},
  year = {1987},
  month = oct,
  journal = {Perceptual and Motor Skills},
  volume = {65},
  number = {2},
  pages = {543--552},
  publisher = {SAGE Publications Inc},
  issn = {0031-5125},
  doi = {10.2466/pms.1987.65.2.543},
  urldate = {2024-07-29},
  abstract = {Better to understand the relation of auditory to visual reaction time (RT) tasks during a long period (320 min.) as well as the evolution of these performances along time, 24 subjects were tested. RT tasks were delivered at 10-min. intervals for 320 min. Correlations between auditory and visual RTs and cross-correlation functions were calculated. Our results confirmed their association, which was maintained throughout the experiment. There were no significant differences in the mean RT by sex or time of day; however, differences were found for the trend slope between type of stimulus and sex.},
  file = {/Users/cristian/Zotero/storage/KT4ENDNM/Almirall and Gutirrez - 1987 - Auditory and Visual Reaction Time in Adults during.pdf}
}

@article{almirallAuditoryVisualReaction1987,
  title = {Auditory and {{Visual Reaction Time}} in {{Adults}} during {{Long Performance}}},
  author = {Almirall, Helena and Guti{\'e}rrez, Emilia},
  year = {1987},
  month = oct,
  journal = {Perceptual and Motor Skills},
  volume = {65},
  number = {2},
  pages = {543--552},
  publisher = {SAGE Publications Inc},
  issn = {0031-5125},
  doi = {10.2466/pms.1987.65.2.543},
  urldate = {2024-07-29},
  abstract = {Better to understand the relation of auditory to visual reaction time (RT) tasks during a long period (320 min.) as well as the evolution of these performances along time, 24 subjects were tested. RT tasks were delivered at 10-min. intervals for 320 min. Correlations between auditory and visual RTs and cross-correlation functions were calculated. Our results confirmed their association, which was maintained throughout the experiment. There were no significant differences in the mean RT by sex or time of day; however, differences were found for the trend slope between type of stimulus and sex.},
  file = {/Users/cristian/Zotero/storage/AK9C5YTI/Almirall and Gutirrez - 1987 - Auditory and Visual Reaction Time in Adults during.pdf}
}

@article{altarriba-bartesPostcompetitionRecoveryStrategies2020,
  title = {Post-Competition Recovery Strategies in Elite Male Soccer Players. {{Effects}} on Performance: {{A}} Systematic Review and Meta-Analysis},
  author = {{Altarriba-Bartes}, A. and Pe{\~n}a, J. and {Vicens-Bordas}, J. and {Mil{\`a}-Villaroel}, R. and {Calleja-Gonz{\'a}lez}, J.},
  year = {2020},
  journal = {PLoS ONE},
  volume = {15},
  number = {10 October},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0240135}
}

@article{altarriba-bartesPostcompetitionRecoveryStrategies2020,
  title = {Post-Competition Recovery Strategies in Elite Male Soccer Players. {{Effects}} on Performance: {{A}} Systematic Review and Meta-Analysis},
  author = {{Altarriba-Bartes}, A. and Pe{\~n}a, J. and {Vicens-Bordas}, J. and {Mil{\`a}-Villaroel}, R. and {Calleja-Gonz{\'a}lez}, J.},
  year = {2020},
  journal = {PLoS ONE},
  volume = {15},
  number = {10 October},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0240135}
}

@article{altmanBetterReportingRandomised1996,
  title = {Better Reporting of Randomised Controlled Trials: The {{CONSORT}} Statement.},
  author = {Altman, D. G.},
  year = {1996},
  month = sep,
  journal = {BMJ},
  volume = {313},
  number = {7057},
  pages = {570--571},
  address = {England},
  issn = {0959-8138 1468-5833},
  doi = {10.1136/bmj.313.7057.570},
  langid = {english},
  pmcid = {PMC2352018},
  pmid = {8806240},
  keywords = {*Randomized Controlled Trials as Topic,Periodicals as Topic/*standards,Publishing/standards}
}

@article{altmanBetterReportingRandomised1996,
  title = {Better Reporting of Randomised Controlled Trials: {{The CONSORT}} Statement.},
  author = {Altman, D. G.},
  year = {1996},
  month = sep,
  journal = {BMJ (Clinical research ed.)},
  volume = {313},
  number = {7057},
  pages = {570--571},
  address = {England},
  issn = {0959-8138 1468-5833},
  doi = {10.1136/bmj.313.7057.570},
  langid = {english},
  pmcid = {PMC2352018},
  pmid = {8806240},
  keywords = {*Randomized Controlled Trials as Topic,Periodicals as Topic/*standards,Publishing/standards}
}

@article{altmanHowObtainConfidence2011,
  title = {How to Obtain the Confidence Interval from a {{P}} Value},
  author = {Altman, Douglas G. and Bland, J. Martin},
  year = {2011},
  month = aug,
  journal = {BMJ (Clinical research ed.)},
  volume = {343},
  pages = {d2090},
  issn = {1756-1833},
  doi = {10.1136/bmj.d2090},
  langid = {english},
  pmid = {21824904},
  keywords = {Confidence Intervals,Probability},
  file = {/Users/cristian/Zotero/storage/HJMX7KT4/Altman and Bland - 2011 - How to obtain the confidence interval from a P val.pdf}
}

@article{altmanHowObtainConfidence2011,
  title = {How to Obtain the Confidence Interval from a {{P}} Value},
  author = {Altman, Douglas G. and Bland, J. Martin},
  year = {2011},
  month = aug,
  journal = {BMJ (Clinical research ed.)},
  volume = {343},
  pages = {d2090},
  issn = {1756-1833},
  doi = {10.1136/bmj.d2090},
  langid = {english},
  pmid = {21824904},
  keywords = {Confidence Intervals,Probability},
  file = {/Users/cristian/Zotero/storage/M465LEJK/Altman and Bland - 2011 - How to obtain the confidence interval from a P val.pdf}
}

@article{altmanScandalPoorMedical1994,
  title = {The Scandal of Poor Medical Research.},
  author = {Altman, D. G.},
  year = {1994},
  month = jan,
  journal = {BMJ : British Medical Journal},
  volume = {308},
  number = {6924},
  pages = {283--284},
  issn = {0959-8138},
  doi = {10.1136/bmj.308.6924.283},
  urldate = {2025-05-29},
  pmcid = {PMC2539276},
  pmid = {8124111},
  file = {/Users/cristian/Zotero/storage/VS48J24X/Altman - 1994 - The scandal of poor medical research..pdf}
}

@article{altmanScandalPoorMedical1994a,
  title = {The Scandal of Poor Medical Research.},
  author = {Altman, D. G.},
  year = {1994},
  month = jan,
  journal = {British Medical Journal},
  volume = {308},
  number = {6924},
  pages = {283--284},
  issn = {0959-8138},
  doi = {10.1136/bmj.308.6924.283},
  urldate = {2025-05-29},
  pmcid = {PMC2539276},
  pmid = {8124111},
  file = {/Users/cristian/Zotero/storage/VB5948IK/Altman - 1994 - The scandal of poor medical research..pdf}
}

@article{amir-behghadamiPopulationInterventionComparison2020,
  title = {Population, {{Intervention}}, {{Comparison}}, {{Outcomes}} and {{Study}} ({{PICOS}}) Design as a Framework to Formulate Eligibility Criteria in Systematic Reviews},
  author = {{Amir-Behghadami}, Mehrdad and Janati, Ali},
  year = {2020},
  month = jun,
  journal = {Emergency Medicine Journal},
  volume = {37},
  number = {6},
  pages = {387--387},
  publisher = {{BMJ Publishing Group Ltd and the British Association for Accident \& Emergency Medicine}},
  issn = {1472-0205, 1472-0213},
  doi = {10.1136/emermed-2020-209567},
  urldate = {2025-06-18},
  abstract = {Dear editor, We read with great interest the review entitled `Paracetamol vs other analgesia in adult patients with minor musculoskeletal injuries: a systematic review',1 published in the Emergency Medicine Journal . Although the authors have stated that they followed Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines, there is a methodological issue regarding the study discussed in this letter, which {\dots}},
  chapter = {PostScript},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmid = {32253195},
  keywords = {{research, methods},publication}
}

@article{amiriChocolateMilkRecovery2019,
  ids = {amiriChocolateMilkRecovery2019a},
  title = {Chocolate Milk for Recovery from Exercise: A Systematic Review and Meta-Analysis of Controlled Clinical Trials.},
  author = {Amiri, Mojgan and Ghiasvand, Reza and Kaviani, Mojtaba and Forbes, Scott C. and {Salehi-Abargouei}, Amin},
  year = {2019},
  month = jun,
  journal = {European journal of clinical nutrition},
  volume = {73},
  number = {6},
  pages = {835--849},
  publisher = {Nature Publishing Group},
  address = {England},
  issn = {1476-5640 0954-3007},
  doi = {10.1038/s41430-018-0187-x},
  abstract = {BACKGROUND/OBJECTIVES: Chocolate milk (CM) contains carbohydrates, proteins, and fat, as well as water and electrolytes, which may be ideal for post-exercise  recovery. We systematically reviewed the evidence regarding the efficacy of CM  compared to either water or other "sport drinks" on post-exercise recovery  markers. SUBJECTS/METHODS: PubMed, Scopus, and Google scholar were explored up to  April 2017 for controlled trials investigating the effect of CM on markers of  recovery in trained athletes. RESULTS: Twelve studies were included in the  systematic review (2, 9, and 1 with high, fair and low quality, respectively) and  11 had extractable data on at least one performance/recovery marker [7 on ratings  of perceived exertion (RPE), 6 on time to exhaustion (TTE) and heart rate (HR), 4  on serum lactate, and serum creatine kinase (CK)]. The meta-analyses revealed  that CM consumption had no effect on TTE, RPE, HR, serum lactate, and CK  (P\,{$>$}\,0.05) compared to placebo or other sport drinks. Subgroup analysis revealed  that TTE significantly increases after consumption of CM compared to placebo  [mean difference (MD)\,=\,0.78\,min, 95\% confidence interval (CI): 0.27, 1.29,  P\,=\,0.003] and carbohydrate, protein, and fat-containing beverages  (MD\,=\,6.13\,min, 95\% CI: 0.11, 12.15, P\,=\,0.046). Furthermore, a significant  attenuation on serum lactate was observed when CM was compared with placebo  (MD\,=\,-1.2\,mmol/L, 95\% CI: -2.06,-0.34, P\,=\,0.006). CONCLUSION: CM provides  either similar or superior results when compared to placebo or other recovery  drinks. Overall, the evidence is limited and high-quality clinical trials with  more well-controlled methodology and larger sample sizes are warranted.},
  langid = {english},
  pmid = {29921963},
  keywords = {*Chocolate,*Milk,*Recovery of Function,Animals,Beverages,Exercise/*physiology,Humans,Physical Endurance/*physiology,Randomized Controlled Trials as Topic}
}

@article{amiriChocolateMilkRecovery2019,
  ids = {amiriChocolateMilkRecovery2019a},
  title = {Chocolate Milk for Recovery from Exercise: A Systematic Review and Meta-Analysis of Controlled Clinical Trials.},
  author = {Amiri, Mojgan and Ghiasvand, Reza and Kaviani, Mojtaba and Forbes, Scott C. and {Salehi-Abargouei}, Amin},
  year = {2019},
  month = jun,
  journal = {European journal of clinical nutrition},
  volume = {73},
  number = {6},
  pages = {835--849},
  publisher = {Nature Publishing Group},
  address = {England},
  issn = {1476-5640 0954-3007},
  doi = {10.1038/s41430-018-0187-x},
  abstract = {BACKGROUND/OBJECTIVES: Chocolate milk (CM) contains carbohydrates, proteins, and fat, as well as water and electrolytes, which may be ideal for post-exercise recovery. We systematically reviewed the evidence regarding the efficacy of CM compared to either water or other "sport drinks" on post-exercise recovery markers. SUBJECTS/METHODS: PubMed, Scopus, and Google scholar were explored up to April 2017 for controlled trials investigating the effect of CM on markers of recovery in trained athletes. RESULTS: Twelve studies were included in the systematic review (2, 9, and 1 with high, fair and low quality, respectively) and 11 had extractable data on at least one performance/recovery marker [7 on ratings of perceived exertion (RPE), 6 on time to exhaustion (TTE) and heart rate (HR), 4 on serum lactate, and serum creatine kinase (CK)]. The meta-analyses revealed that CM consumption had no effect on TTE, RPE, HR, serum lactate, and CK (P\,{$>$}\,0.05) compared to placebo or other sport drinks. Subgroup analysis revealed that TTE significantly increases after consumption of CM compared to placebo [mean difference (MD)\,=\,0.78\,min, 95\% confidence interval (CI): 0.27, 1.29, P\,=\,0.003] and carbohydrate, protein, and fat-containing beverages (MD\,=\,6.13\,min, 95\% CI: 0.11, 12.15, P\,=\,0.046). Furthermore, a significant attenuation on serum lactate was observed when CM was compared with placebo (MD\,=\,-1.2\,mmol/L, 95\% CI: -2.06,-0.34, P\,=\,0.006). CONCLUSION: CM provides either similar or superior results when compared to placebo or other recovery drinks. Overall, the evidence is limited and high-quality clinical trials with more well-controlled methodology and larger sample sizes are warranted.},
  langid = {english},
  pmid = {29921963},
  keywords = {*Chocolate,*Milk,*Recovery of Function,Animals,Beverages,Exercise/*physiology,Humans,Physical Endurance/*physiology,Randomized Controlled Trials as Topic}
}

@article{anderson_sample_planning,
  title = {Sample-{{Size Planning}} for {{More Accurate Statistical Power}}: {{A Method Adjusting Sample Effect Sizes}} for {{Publication Bias}} and {{Uncertainty}}},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  year = {2017},
  month = nov,
  journal = {Psychological Science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  doi = {10.1177/0956797617723724},
  abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
  langid = {english},
  keywords = {{Data Interpretation, Statistical},\{Data Interpretation\vphantom\},effect size,Humans,methodology,publication bias,Publication Bias,sample size,Sample Size,statistical power,Statistical\vphantom\{\},Uncertainty}
}

@article{andersonAssessingStatisticalResults2019,
  title = {Assessing {{Statistical Results}}: {{Magnitude}}, {{Precision}}, and {{Model Uncertainty}}},
  shorttitle = {Assessing {{Statistical Results}}},
  author = {Anderson, Andrew A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {118--121},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537889},
  urldate = {2023-10-16},
  abstract = {Evaluating the importance and the strength of empirical evidence requires asking three questions: First, what are the practical implications of the findings? Second, how precise are the estimates? Confidence intervals provide an intuitive way to communicate precision. Although nontechnical audiences often misinterpret confidence intervals (CIs), I argue that the result is less dangerous than the misunderstandings that arise from hypothesis tests. Third, is the model correctly specified? The validity of point estimates and CIs depends on the soundness of the underlying model.},
  keywords = {Inference,Robustness,Sampling error},
  file = {/Users/cristian/Zotero/storage/49355DMY/Anderson - 2019 - Assessing Statistical Results Magnitude, Precisio.pdf}
}

@article{andersonAssessingStatisticalResults2019,
  title = {Assessing {{Statistical Results}}: {{Magnitude}}, {{Precision}}, and {{Model Uncertainty}}},
  shorttitle = {Assessing {{Statistical Results}}},
  author = {Anderson, Andrew A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {118--121},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537889},
  urldate = {2023-10-16},
  abstract = {Evaluating the importance and the strength of empirical evidence requires asking three questions: First, what are the practical implications of the findings? Second, how precise are the estimates? Confidence intervals provide an intuitive way to communicate precision. Although nontechnical audiences often misinterpret confidence intervals (CIs), I argue that the result is less dangerous than the misunderstandings that arise from hypothesis tests. Third, is the model correctly specified? The validity of point estimates and CIs depends on the soundness of the underlying model.},
  keywords = {Inference,Robustness,Sampling error},
  file = {/Users/cristian/Zotero/storage/W8QP6JXZ/Anderson - 2019 - Assessing Statistical Results Magnitude, Precisio.pdf}
}

@misc{andersonBUCSSBiasUncertainty2016,
  title = {{{BUCSS}}: {{Bias}} and {{Uncertainty Corrected Sample Size}}},
  shorttitle = {{{BUCSS}}},
  author = {Anderson, Samantha F. and Kelley, Ken},
  year = {2016},
  month = dec,
  pages = {1.2.1},
  urldate = {2024-07-26},
  abstract = {Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and uncertainty when planning sample sizes in a future study from an original study. See Anderson, Kelley, \& Maxwell (2017; Psychological Science, 28, 1547-1562).},
  howpublished = {Comprehensive R Archive Network}
}

@article{andersonPerverseEffectsCompetition2007,
  title = {The {{Perverse Effects}} of {{Competition}} on {{Scientists}}' {{Work}} and {{Relationships}}},
  author = {Anderson, Melissa S. and Ronning, Emily A. and De Vries, Raymond and Martinson, Brian C.},
  year = {2007},
  month = dec,
  journal = {Science and Engineering Ethics},
  volume = {13},
  number = {4},
  pages = {437--461},
  issn = {1471-5546},
  doi = {10.1007/s11948-007-9042-5},
  urldate = {2025-05-31},
  abstract = {Competition among scientists for funding, positions and prestige, among other things, is often seen as a salutary driving force in U.S. science. Its effects on scientists, their work and their relationships are seldom considered. Focus-group discussions with 51 mid- and early-career scientists, on which this study is based, reveal a dark side of competition in science. According to these scientists, competition contributes to strategic game-playing in science, a decline in free and open sharing of information and methods, sabotage of others' ability to use one's work, interference with peer-review processes, deformation of relationships, and careless or questionable research conduct. When competition is pervasive, such effects may jeopardize the progress, efficiency and integrity of science.},
  langid = {english},
  keywords = {{Science, Technology and Society},Competition,Ethics in science,Misconduct,Public Understanding of Science,Research Ethics,Research integrity,Science Communication,Science Ethics,Sociology of Science}
}

@article{andersonPerverseEffectsCompetition2007,
  title = {The {{Perverse Effects}} of {{Competition}} on {{Scientists}}' {{Work}} and {{Relationships}}},
  author = {Anderson, Melissa S. and Ronning, Emily A. and De Vries, Raymond and Martinson, Brian C.},
  year = {2007},
  month = dec,
  journal = {Science and Engineering Ethics},
  volume = {13},
  number = {4},
  pages = {437--461},
  issn = {1471-5546},
  doi = {10.1007/s11948-007-9042-5},
  urldate = {2025-05-31},
  abstract = {Competition among scientists for funding, positions and prestige, among other things, is often seen as a salutary driving force in U.S. science. Its effects on scientists, their work and their relationships are seldom considered. Focus-group discussions with 51 mid- and early-career scientists, on which this study is based, reveal a dark side of competition in science. According to these scientists, competition contributes to strategic game-playing in science, a decline in free and open sharing of information and methods, sabotage of others' ability to use one's work, interference with peer-review processes, deformation of relationships, and careless or questionable research conduct. When competition is pervasive, such effects may jeopardize the progress, efficiency and integrity of science.},
  langid = {english},
  keywords = {\{Science\vphantom\},Competition,Ethics in science,Misconduct,Public Understanding of Science,Research Ethics,Research integrity,Science Communication,Science Ethics,Sociology of Science,Technology and Society\vphantom\{\}}
}

@article{andersonSampleSizePlanningMore2017,
  title = {Sample-{{Size Planning}} for {{More Accurate Statistical Power}}: {{A Method Adjusting Sample Effect Sizes}} for {{Publication Bias}} and {{Uncertainty}}},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  year = {2017},
  month = nov,
  journal = {Psychological Science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  doi = {10.1177/0956797617723724},
  abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
  langid = {english},
  keywords = {{\textbackslash}\{Data Interpretation{\textbackslash}vphantom{\textbackslash}\},\{Data Interpretation\vphantom\},effect size,Humans,methodology,publication bias,Publication Bias,sample size,Sample Size,statistical power,Statistical{\textbackslash}vphantom{\textbackslash}\{{\textbackslash}\},Statistical\vphantom\{\},Uncertainty}
}

@article{andrewsSleepwakeDisturbanceRelated2019,
  title = {Sleep-Wake Disturbance Related to Ocular Disease: {{A}} Systematic Review of Phase-Shifting Pharmaceutical Therapies},
  author = {Andrews, C.D. and Foster, R.G. and Alexander, I. and Vasudevan, S. and Downes, S.M. and Heneghan, C. and Pl{\"u}ddemann, A.},
  year = {2019},
  journal = {Translational Vision Science and Technology},
  volume = {8},
  number = {3},
  publisher = {{Association for Research in Vision and Ophthalmology Inc.}},
  doi = {10.1167/tvst.8.3.49}
}

@article{andrewsSleepwakeDisturbanceRelated2019,
  title = {Sleep-Wake Disturbance Related to Ocular Disease: {{A}} Systematic Review of Phase-Shifting Pharmaceutical Therapies},
  author = {Andrews, C.D. and Foster, R.G. and Alexander, I. and Vasudevan, S. and Downes, S.M. and Heneghan, C. and Pl{\"u}ddemann, A.},
  year = {2019},
  journal = {Translational Vision Science and Technology},
  volume = {8},
  number = {3},
  publisher = {{Association for Research in Vision and Ophthalmology Inc.}},
  doi = {10.1167/tvst.8.3.49}
}

@article{anugrahEffectRoyalJelly2023,
  title = {Effect of {{Royal Jelly}} on {{Performance}} and {{Inflammatory Response}} to {{Muscle Damage}}: {{A Systematic Review}}},
  author = {Anugrah, S.M. and Kusnanik, N.W. and Wahjuni, E.S. and Ayubi, N. and Mulyawan, R.},
  year = {2023},
  journal = {Biointerface Research in Applied Chemistry},
  volume = {13},
  number = {5},
  publisher = {AMG Transcend Association},
  doi = {10.33263/BRIAC135.479}
}

@article{anugrahEffectRoyalJelly2023,
  title = {Effect of {{Royal Jelly}} on {{Performance}} and {{Inflammatory Response}} to {{Muscle Damage}}: {{A Systematic Review}}},
  author = {Anugrah, S.M. and Kusnanik, N.W. and Wahjuni, E.S. and Ayubi, N. and Mulyawan, R.},
  year = {2023},
  journal = {Biointerface Research in Applied Chemistry},
  volume = {13},
  number = {5},
  publisher = {AMG Transcend Association},
  doi = {10.33263/BRIAC135.479}
}

@article{anvari_sesoi,
  title = {Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2021},
  journal = {Journal of Experimental Social Psychology},
  volume = {96},
  pages = {104159},
  doi = {10.1016/j.jesp.2021.104159},
  abstract = {Effect sizes are an important outcome of quantitative research, but few guidelines exist that explain how researchers can determine which effect sizes are meaningful. Psychologists often want to study effects that are large enough to make a difference to people's subjective experience. Thus, subjective experience is one way to gauge the meaningfulness of an effect. We propose and illustrate one method for how to quantify the smallest subjectively experienced difference---the smallest change in an outcome measure that individuals consider to be meaningful enough in their subjective experience such that they are willing to rate themselves as feeling different---using an anchor-based method with a global rating of change question applied to the positive and negative affect scale. We provide a step-by-step guide for the questions that researchers need to consider in deciding whether and how to use the anchor-based method, and we make explicit the assumptions of the method that future research can examine. For researchers interested in people's subjective experiences, this anchor-based method provides one way to specify a smallest effect size of interest, which allows researchers to interpret observed results in terms of their theoretical and practical significance.},
  langid = {english},
  keywords = {Minimum important difference,Negative affect,Positive affect,Practical significance,Smallest effect size of interest,Smallest subjectively experienced difference,Subjectively experienced difference},
  file = {/Users/cristian/Zotero/storage/NFQMX69B/Anvari and Lakens - 2021 - Using anchor-based methods to determine the smalle.pdf}
}

@article{anvariUsingAnchorbasedMethods2021,
  title = {Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2021},
  month = sep,
  journal = {Journal of Experimental Social Psychology},
  volume = {96},
  pages = {104159},
  doi = {10.1016/j.jesp.2021.104159},
  abstract = {Effect sizes are an important outcome of quantitative research, but few guidelines exist that explain how researchers can determine which effect sizes are meaningful. Psychologists often want to study effects that are large enough to make a difference to people's subjective experience. Thus, subjective experience is one way to gauge the meaningfulness of an effect. We propose and illustrate one method for how to quantify the smallest subjectively experienced difference---the smallest change in an outcome measure that individuals consider to be meaningful enough in their subjective experience such that they are willing to rate themselves as feeling different---using an anchor-based method with a global rating of change question applied to the positive and negative affect scale. We provide a step-by-step guide for the questions that researchers need to consider in deciding whether and how to use the anchor-based method, and we make explicit the assumptions of the method that future research can examine. For researchers interested in people's subjective experiences, this anchor-based method provides one way to specify a smallest effect size of interest, which allows researchers to interpret observed results in terms of their theoretical and practical significance.},
  langid = {english},
  keywords = {Minimum important difference,Negative affect,Positive affect,Practical significance,Smallest effect size of interest,Smallest subjectively experienced difference,Subjectively experienced difference},
  file = {/Users/cristian/Zotero/storage/Z4XITN32/Anvari and Lakens - 2021 - Using anchor-based methods to determine the smalle.pdf}
}

@article{aoyagiOsteoporosisOsteoporoticFractures2004,
  title = {Osteoporosis and Osteoporotic Fractures in the Elderly},
  author = {Aoyagi, K.},
  year = {2004},
  journal = {Acta Medica Nagasakiensia},
  volume = {49},
  number = {1-2},
  pages = {7--11}
}

@article{aoyagiOsteoporosisOsteoporoticFractures2004,
  title = {Osteoporosis and Osteoporotic Fractures in the Elderly},
  author = {Aoyagi, K.},
  year = {2004},
  journal = {Acta Medica Nagasakiensia},
  volume = {49},
  number = {1-2},
  pages = {7--11}
}

@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  publisher = {US:},
  issn = {1935-990X},
  doi = {10.1037/amp0000191}
}

@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  publisher = {US:},
  issn = {1935-990X},
  doi = {10.1037/amp0000191}
}

@article{appelbaum_reporting_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  publisher = {US:},
  issn = {1935-990X},
  doi = {10.1037/amp0000191}
}

@article{appelbaumJournalArticleReporting2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3},
  publisher = {US: American Psychological Association},
  issn = {1935-990X},
  doi = {10.1037/amp0000191},
  urldate = {2022-12-07},
  file = {/Users/cristian/Zotero/storage/KEQA9CCC/2018-00750-002.html;/Users/cristian/Zotero/storage/VLV5LFA7/2018-00750-002.html}
}

@article{appelbaumJournalArticleReporting2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3},
  publisher = {US: American Psychological Association},
  issn = {1935-990X},
  doi = {10.1037/amp0000191},
  urldate = {2022-12-07},
  file = {/Users/cristian/Zotero/storage/H7Q66LJE/2018-00750-002.html;/Users/cristian/Zotero/storage/PV5ZHB37/2018-00750-002.html}
}

@misc{AreSmallEffects,
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022) - {{Maximilian A}}. {{Primbs}}, {{Charlotte R}}. {{Pennington}}, {{Dani{\"e}l Lakens}}, {{Miguel Alejandro A}}. {{Silan}}, {{Dwayne S}}. {{N}}. {{Lieck}}, {{Patrick S}}. {{Forscher}}, {{Erin M}}. {{Buchanan}}, {{Samuel J}}. {{Westwood}}, 2023},
  urldate = {2023-10-19},
  howpublished = {https://journals.sagepub.com/doi/10.1177/17456916221100420},
  file = {/Users/cristian/Zotero/storage/HTGS55FX/17456916221100420.html}
}

@misc{AreSmallEffects,
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022) - {{Maximilian A}}. {{Primbs}}, {{Charlotte R}}. {{Pennington}}, {{Dani{\"e}l Lakens}}, {{Miguel Alejandro A}}. {{Silan}}, {{Dwayne S}}. {{N}}. {{Lieck}}, {{Patrick S}}. {{Forscher}}, {{Erin M}}. {{Buchanan}}, {{Samuel J}}. {{Westwood}}, 2023},
  urldate = {2023-10-19},
  file = {/Users/cristian/Zotero/storage/W26QK45M/17456916221100420.html}
}

@misc{AreSmallEffects,
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022) - {{Maximilian A}}. {{Primbs}}, {{Charlotte R}}. {{Pennington}}, {{Dani{\"e}l Lakens}}, {{Miguel Alejandro A}}. {{Silan}}, {{Dwayne S}}. {{N}}. {{Lieck}}, {{Patrick S}}. {{Forscher}}, {{Erin M}}. {{Buchanan}}, {{Samuel J}}. {{Westwood}}, 2023},
  urldate = {2023-10-19},
  file = {/Users/cristian/Zotero/storage/FYDG3R65/17456916221100420.html}
}

@misc{AreSmallEffects,
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022) - {{Maximilian A}}. {{Primbs}}, {{Charlotte R}}. {{Pennington}}, {{Dani{\"e}l Lakens}}, {{Miguel Alejandro A}}. {{Silan}}, {{Dwayne S}}. {{N}}. {{Lieck}}, {{Patrick S}}. {{Forscher}}, {{Erin M}}. {{Buchanan}}, {{Samuel J}}. {{Westwood}}, 2023},
  urldate = {2023-10-19},
  file = {/Users/cristian/Zotero/storage/PJZREQG9/17456916221100420.html}
}

@article{artnerReproducibilityStatisticalResults2021,
  title = {The Reproducibility of Statistical Results in Psychological Research: {{An}} Investigation Using Unpublished Raw Data},
  shorttitle = {The Reproducibility of Statistical Results in Psychological Research},
  author = {Artner, Richard and Verliefde, Thomas and Steegen, Sara and Gomes, Sara and Traets, Frits and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2021},
  month = oct,
  journal = {Psychological Methods},
  volume = {26},
  number = {5},
  pages = {527--546},
  issn = {1939-1463},
  doi = {10.1037/met0000365},
  abstract = {We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three APA journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70\%), 18 of which only by deviating from the article's analytical description. Thirteen (7\%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that APA style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. This article discusses the types of mistakes we could identify and the tediousness of our reproduction efforts in the light of a newly developed taxonomy for reproducibility. We then link our findings with other findings of empirical research on this topic, give practical recommendations on how to achieve reproducibility, and discuss the challenges of large-scale reproducibility checks as well as promising ideas that could considerably increase the reproducibility of psychological research. (PsycInfo Database Record (c) 2021 APA, all rights reserved).},
  langid = {english},
  pmid = {33180514},
  keywords = {Humans,Reproducibility of Results,Research Design}
}

@article{artnerReproducibilityStatisticalResults2021,
  title = {The Reproducibility of Statistical Results in Psychological Research: {{An}} Investigation Using Unpublished Raw Data},
  shorttitle = {The Reproducibility of Statistical Results in Psychological Research},
  author = {Artner, Richard and Verliefde, Thomas and Steegen, Sara and Gomes, Sara and Traets, Frits and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2021},
  month = oct,
  journal = {Psychological Methods},
  volume = {26},
  number = {5},
  pages = {527--546},
  issn = {1939-1463},
  doi = {10.1037/met0000365},
  abstract = {We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three APA journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70\%), 18 of which only by deviating from the article's analytical description. Thirteen (7\%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that APA style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. This article discusses the types of mistakes we could identify and the tediousness of our reproduction efforts in the light of a newly developed taxonomy for reproducibility. We then link our findings with other findings of empirical research on this topic, give practical recommendations on how to achieve reproducibility, and discuss the challenges of large-scale reproducibility checks as well as promising ideas that could considerably increase the reproducibility of psychological research. (PsycInfo Database Record (c) 2021 APA, all rights reserved).},
  langid = {english},
  pmid = {33180514},
  keywords = {Humans,Reproducibility of Results,Research Design}
}

@article{asendorpfRecommendationsIncreasingReplicability2013,
  title = {Recommendations for {{Increasing Replicability}} in {{Psychology}}},
  author = {Asendorpf, Jens B. and Conner, Mark and Fruyt, Filip De and Houwer, Jan De and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and van Aken, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
  year = {2013},
  journal = {European Journal of Personality},
  volume = {27},
  number = {2},
  pages = {108--119},
  issn = {1099-0984},
  doi = {10.1002/per.1919},
  abstract = {Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {confirmation bias,generalizability,publication bias,replicability,research transparency},
  file = {/Users/cristian/Zotero/storage/AA3EBWPV/Asendorpf et al. - 2013 - Recommendations for Increasing Replicability in Ps.pdf;/Users/cristian/Zotero/storage/KHE5LUU5/per.html;/Users/cristian/Zotero/storage/RXKJEY8R/per.html}
}

@article{asendorpfRecommendationsIncreasingReplicability2013,
  title = {Recommendations for {{Increasing Replicability}} in {{Psychology}}},
  author = {Asendorpf, Jens B. and Conner, Mark and Fruyt, Filip De and Houwer, Jan De and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and {van Aken}, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
  year = {2013},
  journal = {European Journal of Personality},
  volume = {27},
  number = {2},
  pages = {108--119},
  issn = {1099-0984},
  doi = {10.1002/per.1919},
  abstract = {Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {confirmation bias,generalizability,publication bias,replicability,research transparency},
  file = {/Users/cristian/Zotero/storage/W2WI9U7P/Asendorpf et al. - 2013 - Recommendations for Increasing Replicability in Ps.pdf;/Users/cristian/Zotero/storage/ERQGN9SX/per.html;/Users/cristian/Zotero/storage/PJUWLM7V/per.html}
}

@misc{AssessingEvidentialValue,
  title = {Assessing the {{Evidential Value}} of {{Mental Fatigue}} and {{Exercise Research}} {\textbar} {{Sports Medicine}}},
  urldate = {2025-03-29},
  howpublished = {https://link.springer.com/article/10.1007/s40279-023-01926-w},
  file = {/Users/cristian/Zotero/storage/YPS6NWR4/s40279-023-01926-w.html}
}

@misc{AssessingEvidentialValue,
  title = {Assessing the {{Evidential Value}} of {{Mental Fatigue}} and {{Exercise Research}} {\textbar} {{Sports Medicine}}},
  urldate = {2025-03-29},
  file = {/Users/cristian/Zotero/storage/3XA9A9FT/s40279-023-01926-w.html}
}

@misc{AssessingEvidentialValue,
  title = {Assessing the {{Evidential Value}} of {{Mental Fatigue}} and {{Exercise Research}} {\textbar} {{Sports Medicine}}},
  urldate = {2025-03-29},
  file = {/Users/cristian/Zotero/storage/NN9AK4P8/s40279-023-01926-w.html}
}

@misc{AssessingEvidentialValue,
  title = {Assessing the {{Evidential Value}} of {{Mental Fatigue}} and {{Exercise Research}} {\textbar} {{Sports Medicine}}},
  urldate = {2025-03-29},
  file = {/Users/cristian/Zotero/storage/CGCCL7VX/s40279-023-01926-w.html}
}

@article{atkinsonSelectedIssuesDesign2001,
  title = {Selected Issues in the Design and Analysis of Sport Performance Research},
  author = {Atkinson, Greg and Nevill, Alan M.},
  year = {2001},
  month = jan,
  journal = {Journal of Sports Sciences},
  volume = {19},
  number = {10},
  pages = {811--827},
  issn = {0264-0414, 1466-447X},
  doi = {10.1080/026404101317015447},
  urldate = {2025-06-03},
  langid = {english},
  pmid = {11561675},
  keywords = {{Models, Statistical},\{Models\vphantom\},Humans,Predictive Value of Tests,Reproducibility of Results,Research Design,Sensitivity and Specificity,Sports,Statistical\vphantom\{\},Statistics as Topic,Task Performance and Analysis,United Kingdom},
  file = {/Users/cristian/Zotero/storage/LLECNBPP/Atkinson and Nevill - 2001 - Selected issues in the design and analysis of spor.pdf;/Users/cristian/Zotero/storage/Q9VTLLYG/Atkinson and Nevill - 2001 - Selected issues in the design and analysis of spor.pdf}
}

@article{atkinsonSelectedIssuesDesign2001,
  title = {Selected Issues in the Design and Analysis of Sport Performance Research},
  author = {Atkinson, Greg and Nevill, Alan M.},
  year = {2001},
  month = jan,
  journal = {Journal of Sports Sciences},
  volume = {19},
  number = {10},
  pages = {811--827},
  issn = {0264-0414, 1466-447X},
  doi = {10.1080/026404101317015447},
  urldate = {2025-06-03},
  langid = {english},
  pmid = {11561675},
  keywords = {{\textbackslash}\{Models{\textbackslash}vphantom{\textbackslash}\},\{Models\vphantom\},Humans,Predictive Value of Tests,Reproducibility of Results,Research Design,Sensitivity and Specificity,Sports,Statistical{\textbackslash}vphantom{\textbackslash}\{{\textbackslash}\},Statistical\vphantom\{\},Statistics as Topic,Task Performance and Analysis,United Kingdom},
  file = {/Users/cristian/Zotero/storage/FMTQIN8V/Atkinson and Nevill - 2001 - Selected issues in the design and analysis of spor.pdf;/Users/cristian/Zotero/storage/X77RML4Y/Atkinson and Nevill - 2001 - Selected issues in the design and analysis of spor.pdf}
}

@article{atkinsonStatisticalMethodsAssessing1998,
  title = {Statistical Methods for Assessing Measurement Error (Reliability) in Variables Relevant to Sports Medicine},
  author = {Atkinson, G. and Nevill, A. M.},
  year = {1998},
  month = oct,
  journal = {Sports Medicine (Auckland, N.Z.)},
  volume = {26},
  number = {4},
  pages = {217--238},
  issn = {0112-1642},
  doi = {10.2165/00007256-199826040-00002},
  abstract = {Minimal measurement error (reliability) during the collection of interval- and ratio-type data is critically important to sports medicine research. The main components of measurement error are systematic bias (e.g. general learning or fatigue effects on the tests) and random error due to biological or mechanical variation. Both error components should be meaningfully quantified for the sports physician to relate the described error to judgements regarding 'analytical goals' (the requirements of the measurement tool for effective practical use) rather than the statistical significance of any reliability indicators. Methods based on correlation coefficients and regression provide an indication of 'relative reliability'. Since these methods are highly influenced by the range of measured values, researchers should be cautious in: (i) concluding acceptable relative reliability even if a correlation is above 0.9; (ii) extrapolating the results of a test-retest correlation to a new sample of individuals involved in an experiment; and (iii) comparing test-retest correlations between different reliability studies. Methods used to describe 'absolute reliability' include the standard error of measurements (SEM), coefficient of variation (CV) and limits of agreement (LOA). These statistics are more appropriate for comparing reliability between different measurement tools in different studies. They can be used in multiple retest studies from ANOVA procedures, help predict the magnitude of a 'real' change in individual athletes and be employed to estimate statistical power for a repeated-measures experiment. These methods vary considerably in the way they are calculated and their use also assumes the presence (CV) or absence (SEM) of heteroscedasticity. Most methods of calculating SEM and CV represent approximately 68\% of the error that is actually present in the repeated measurements for the 'average' individual in the sample. LOA represent the test-retest differences for 95\% of a population. The associated Bland-Altman plot shows the measurement error schematically and helps to identify the presence of heteroscedasticity. If there is evidence of heteroscedasticity or non-normality, one should logarithmically transform the data and quote the bias and random error as ratios. This allows simple comparisons of reliability across different measurement tools. It is recommended that sports clinicians and researchers should cite and interpret a number of statistical methods for assessing reliability. We encourage the inclusion of the LOA method, especially the exploration of heteroscedasticity that is inherent in this analysis. We also stress the importance of relating the results of any reliability statistic to 'analytical goals' in sports medicine.},
  langid = {english},
  pmid = {9820922},
  keywords = {Bias,Data Collection,Humans,Regression Analysis,Reproducibility of Results,Sports Medicine,Statistics as Topic}
}

@article{atkinsonStatisticalMethodsAssessing1998,
  title = {Statistical Methods for Assessing Measurement Error (Reliability) in Variables Relevant to Sports Medicine},
  author = {Atkinson, G. and Nevill, A. M.},
  year = {1998},
  month = oct,
  journal = {Sports Medicine (Auckland, N.Z.)},
  volume = {26},
  number = {4},
  pages = {217--238},
  issn = {0112-1642},
  doi = {10.2165/00007256-199826040-00002},
  abstract = {Minimal measurement error (reliability) during the collection of interval- and ratio-type data is critically important to sports medicine research. The main components of measurement error are systematic bias (e.g. general learning or fatigue effects on the tests) and random error due to biological or mechanical variation. Both error components should be meaningfully quantified for the sports physician to relate the described error to judgements regarding 'analytical goals' (the requirements of the measurement tool for effective practical use) rather than the statistical significance of any reliability indicators. Methods based on correlation coefficients and regression provide an indication of 'relative reliability'. Since these methods are highly influenced by the range of measured values, researchers should be cautious in: (i) concluding acceptable relative reliability even if a correlation is above 0.9; (ii) extrapolating the results of a test-retest correlation to a new sample of individuals involved in an experiment; and (iii) comparing test-retest correlations between different reliability studies. Methods used to describe 'absolute reliability' include the standard error of measurements (SEM), coefficient of variation (CV) and limits of agreement (LOA). These statistics are more appropriate for comparing reliability between different measurement tools in different studies. They can be used in multiple retest studies from ANOVA procedures, help predict the magnitude of a 'real' change in individual athletes and be employed to estimate statistical power for a repeated-measures experiment. These methods vary considerably in the way they are calculated and their use also assumes the presence (CV) or absence (SEM) of heteroscedasticity. Most methods of calculating SEM and CV represent approximately 68\% of the error that is actually present in the repeated measurements for the 'average' individual in the sample. LOA represent the test-retest differences for 95\% of a population. The associated Bland-Altman plot shows the measurement error schematically and helps to identify the presence of heteroscedasticity. If there is evidence of heteroscedasticity or non-normality, one should logarithmically transform the data and quote the bias and random error as ratios. This allows simple comparisons of reliability across different measurement tools. It is recommended that sports clinicians and researchers should cite and interpret a number of statistical methods for assessing reliability. We encourage the inclusion of the LOA method, especially the exploration of heteroscedasticity that is inherent in this analysis. We also stress the importance of relating the results of any reliability statistic to 'analytical goals' in sports medicine.},
  langid = {english},
  pmid = {9820922},
  keywords = {Bias,Data Collection,Humans,Regression Analysis,Reproducibility of Results,Sports Medicine,Statistics as Topic}
}

@article{augusteijnEffectPublicationBias2019,
  title = {The Effect of Publication Bias on the {{Q}} Test and Assessment of Heterogeneity},
  author = {Augusteijn, Hilde E. M. and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {116--134},
  issn = {1939-1463},
  doi = {10.1037/met0000197},
  abstract = {One of the main goals of meta-analysis is to test for and estimate the heterogeneity of effect sizes. We examined the effect of publication bias on the Q test and assessments of heterogeneity as a function of true heterogeneity, publication bias, true effect size, number of studies, and variation of sample sizes. The present study has two main contributions and is relevant to all researchers conducting meta-analysis. First, we show when and how publication bias affects the assessment of heterogeneity. The expected values of heterogeneity measures H{$^2$} and I{$^2$} were analytically derived, and the power and Type I error rate of the Q test were examined in a Monte Carlo simulation study. Our results show that the effect of publication bias on the Q test and assessment of heterogeneity is large, complex, and nonlinear. Publication bias can both dramatically decrease and increase heterogeneity in true effect size, particularly if the number of studies is large and population effect size is small. We therefore conclude that the Q test of homogeneity and heterogeneity measures H{$^2$} and I{$^2$} are generally not valid when publication bias is present. Our second contribution is that we introduce a web application, Q-sense, which can be used to determine the impact of publication bias on the assessment of heterogeneity within a certain meta-analysis and to assess the robustness of the meta-analytic estimate to publication bias. Furthermore, we apply Q-sense to 2 published meta-analyses, showing how publication bias can result in invalid estimates of effect size and heterogeneity. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {30489099},
  keywords = {{Data Interpretation, Statistical},\{Data Interpretation\vphantom\},Humans,Meta-Analysis as Topic,Normal Distribution,Publication Bias,Statistical\vphantom\{\}},
  file = {/Users/cristian/Zotero/storage/78U5PEGL/Augusteijn et al. - 2019 - The effect of publication bias on the Q test and a.pdf}
}

@article{augusteijnEffectPublicationBias2019,
  title = {The Effect of Publication Bias on the {{Q}} Test and Assessment of Heterogeneity},
  author = {Augusteijn, Hilde E. M. and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {116--134},
  issn = {1939-1463},
  doi = {10.1037/met0000197},
  abstract = {One of the main goals of meta-analysis is to test for and estimate the heterogeneity of effect sizes. We examined the effect of publication bias on the Q test and assessments of heterogeneity as a function of true heterogeneity, publication bias, true effect size, number of studies, and variation of sample sizes. The present study has two main contributions and is relevant to all researchers conducting meta-analysis. First, we show when and how publication bias affects the assessment of heterogeneity. The expected values of heterogeneity measures H{$^2$} and I{$^2$} were analytically derived, and the power and Type I error rate of the Q test were examined in a Monte Carlo simulation study. Our results show that the effect of publication bias on the Q test and assessment of heterogeneity is large, complex, and nonlinear. Publication bias can both dramatically decrease and increase heterogeneity in true effect size, particularly if the number of studies is large and population effect size is small. We therefore conclude that the Q test of homogeneity and heterogeneity measures H{$^2$} and I{$^2$} are generally not valid when publication bias is present. Our second contribution is that we introduce a web application, Q-sense, which can be used to determine the impact of publication bias on the assessment of heterogeneity within a certain meta-analysis and to assess the robustness of the meta-analytic estimate to publication bias. Furthermore, we apply Q-sense to 2 published meta-analyses, showing how publication bias can result in invalid estimates of effect size and heterogeneity. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {30489099},
  keywords = {{\textbackslash}\{Data Interpretation{\textbackslash}vphantom{\textbackslash}\},\{Data Interpretation\vphantom\},Humans,Meta-Analysis as Topic,Normal Distribution,Publication Bias,Statistical{\textbackslash}vphantom{\textbackslash}\{{\textbackslash}\},Statistical\vphantom\{\}},
  file = {/Users/cristian/Zotero/storage/PVXI9WCW/Augusteijn et al. - 2019 - The effect of publication bias on the Q test and a.pdf}
}

@article{auspurgHasCredibilitySocial2021,
  title = {Has the {{Credibility}} of the {{Social Sciences Been Credibly Destroyed}}? {{Reanalyzing}} the ``{{Many Analysts}}, {{One Data Set}}'' {{Project}}},
  author = {Auspurg, Katrin and Br{\"u}derl, Josef},
  year = {2021},
  journal = {Socius},
  volume = {7},
  pages = {23780231211024421},
  doi = {10.1177/23780231211024421},
  abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an article in which 29 teams analyzed the same research question with the same data: Are soccer referees more likely to give red cards to players with dark skin tone than light skin tone? The results obtained by the teams differed extensively. Many concluded from this widely noted exercise that the social sciences are not rigorous enough to provide definitive answers. In this article, we investigate why results diverged so much. We argue that the main reason was an unclear research question: Teams differed in their interpretation of the research question and therefore used diverse research designs and model specifications. We show by reanalyzing the data that with a clear research question, a precise definition of the parameter of interest, and theory-guided causal reasoning, results vary only within a narrow range. The broad conclusion of our reanalysis is that social science research needs to be more precise in its ``estimands'' to become credible.}
}

@article{azevedoDifferentCryotherapyModalities2022,
  title = {Different {{Cryotherapy Modalities Demonstrate Similar Effects}} on {{Muscle Performance}}, {{Soreness}}, and {{Damage}} in {{Healthy Individuals}} and {{Athletes}}: {{A Systematic Review}} with {{Metanalysis}}},
  author = {Azevedo, K.P. and Bastos, J.A.I. and {de Sousa Neto}, I.V. and Pastre, C.M. and Durigan, J.L.Q.},
  year = {2022},
  journal = {Journal of Clinical Medicine},
  volume = {11},
  number = {15},
  publisher = {MDPI},
  doi = {10.3390/jcm11154441}
}

@article{bachelezTofacitinibEtanerceptPlacebo2015,
  title = {Tofacitinib versus Etanercept or Placebo in Moderate-to-Severe Chronic Plaque Psoriasis: A Phase 3 Randomised Non-Inferiority Trial},
  shorttitle = {Tofacitinib versus Etanercept or Placebo in Moderate-to-Severe Chronic Plaque Psoriasis},
  author = {Bachelez, Herv{\'e} and {van de Kerkhof}, Peter C. M. and Strohal, Robert and Kubanov, Alexey and Valenzuela, Fernando and Lee, Joo-Heung and Yakusevich, Vladimir and Chimenti, Sergio and Papacharalambous, Jocelyne and Proulx, James and Gupta, Pankaj and Tan, Huaming and Tawadrous, Margaret and Valdez, Hernan and Wolk, Robert and {OPT Compare Investigators}},
  year = {2015},
  month = aug,
  journal = {Lancet (London, England)},
  volume = {386},
  number = {9993},
  pages = {552--561},
  issn = {1474-547X},
  doi = {10.1016/S0140-6736(14)62113-9},
  abstract = {BACKGROUND: New therapeutic options are needed for patients with psoriasis. Tofacitinib, an oral Janus kinase inhibitor, is being investigated as a treatment for moderate-to-severe chronic plaque psoriasis. In this study, we aimed to compare two tofacitinib doses with high-dose etanercept or placebo in this patient population. METHODS: In this phase 3, randomised, multicentre, double-dummy, placebo-controlled, 12-week, non-inferiority trial, adult patients with chronic stable plaque psoriasis (for {$\geq$}12 months) who were candidates for systemic or phototherapy and had a Psoriasis Area and Severity Index (PASI) score of 12 or higher and a Physician's Global Assessment (PGA) of moderate or severe, and had failed to respond to, had a contraindication to, or were intolerant to at least one conventional systemic therapy, were enrolled from 122 investigational dermatology centres worldwide. Eligible patients were randomly assigned in a 3:3:3:1 ratio to receive tofacitinib 5 mg or 10 mg twice daily at about 12 h intervals, etanercept 50 mg subcutaneously twice weekly at about 3-4 day intervals, or placebo. Randomisation was done by a computer-generated randomisation schedule, and all patients and study personnel were masked to treatment assignment. The co-primary endpoints were the proportion of patients at week 12 with at least a 75\% reduction in the PASI score from baseline (PASI75 response) and the proportion of patients achieving a PGA score of "clear" or "almost clear" (PGA response), analysed in the full analysis set (all patients who were randomised and received at least one dose of study drug). This study is registered with ClinicalTrials.gov, number NCT01241591. FINDINGS: Between Nov 29, 2010, and Sept 13, 2012, we enrolled 1106 eligible adult patients with chronic plaque psoriasis and randomly assigned them to the four treatment groups (330 to tofacitinib 5 mg twice daily, 332 to tofacitinib 10 mg twice daily, 336 to etanercept 50 mg twice weekly, and 108 to placebo). Of these patients, 1101 actually received their assigned study medication (329 in the tofactinib 5 mg group, 330 in the tofacitinib 10 mg group, 335 in the etanercept group, and 107 in the placebo group). At week 12, PASI75 responses were recorded in 130 (39{$\cdot$}5\%) of 329 patients in the tofacitinib 5 mg group, 210 (63{$\cdot$}6\%) of 330 in the tofacitinib 10 mg group, 197 (58{$\cdot$}8\%) of 335 in the etanercept group, and six (5{$\cdot$}6\%) of 107 in the placebo group. A PGA response was achieved by 155 (47{$\cdot$}1\%) of 329 patients in the tofacitinib 5 mg group, 225 (68{$\cdot$}2\%) of 330 in the tofacitinib 10 mg group, 222 (66{$\cdot$}3\%) of 335 in the etanercept group, and 16 (15{$\cdot$}0\%) of 107 in the placebo group. The rate of adverse events was similar across the four groups, with serious adverse events occurring in seven (2\%) of 329 patients in the tofacitinib 5 mg group, five (2\%) of 330 in the tofacitinib 10 mg group, seven (2\%) of 335 in the etanercept group, and two (2\%) of 107 in the placebo group. Three (1\%) of 329 patients in the tofacitinib 5 mg group, ten (3\%) of 330 in the tofacitinib 10 mg group, 11 (3\%) of 335 in the etanercept group, and four (4\%) of 107 patients in the placebo group discontinued their assigned treatment because of adverse events. INTERPRETATION: In patients with moderate-to-severe plaque psoriasis, the 10 mg twice daily dose of tofacitinib was non-inferior to etanercept 50 mg twice weekly and was superior to placebo, but the 5 mg twice daily dose did not show non-inferiority to etanercept 50 mg twice weekly. The adverse event rates over 12 weeks were similar for tofacitinib and etanercept. This study indicates that in the future tofacitinib could provide a convenient and well-tolerated therapeutic option for patients with moderate-to-severe plaque psoriasis. FUNDING: Pfizer Inc.},
  langid = {english},
  pmid = {26051365},
  keywords = {{Anti-Inflammatory Agents, Non-Steroidal},{Dose-Response Relationship, Drug},{Receptors, Tumor Necrosis Factor},\{Anti-Inflammatory Agents\vphantom\},\{Dose-Response Relationship\vphantom\},\{Receptors\vphantom\},Adult,Chronic Disease,Double-Blind Method,Drug Administration Schedule,Drug\vphantom\{\},Etanercept,Female,Humans,Immunoglobulin G,Male,Middle Aged,Non-Steroidal\vphantom\{\},Piperidines,Protein Kinase Inhibitors,Psoriasis,Pyrimidines,Pyrroles,Treatment Outcome,Tumor Necrosis Factor\vphantom\{\}}
}

@book{baguleySeriousStatGuide2018,
  title = {Serious {{Stat}}: {{A}} Guide to Advanced Statistics for the Behavioral Sciences},
  shorttitle = {Serious {{Stat}}},
  author = {Baguley, Thomas},
  year = {2018},
  month = jan,
  publisher = {Bloomsbury Publishing},
  abstract = {Ideal for experienced students and researchers in the social sciences who wish to refresh or extend their understanding of statistics, and to apply advanced statistical procedures using SPSS or R. Key theory is reviewed and illustrated with examples of how to apply these concepts using real data.},
  googlebooks = {0ZhGEAAAQBAJ},
  isbn = {978-0-230-36355-7},
  langid = {english},
  keywords = {Psychology / Research \& Methodology}
}

@article{bakerUnderstandingHeterogeneityMetaanalysis2009,
  title = {Understanding Heterogeneity in Meta-Analysis: The Role of Meta-Regression},
  shorttitle = {Understanding Heterogeneity in Meta-Analysis},
  author = {Baker, W. L. and Michael White, C. and Cappelleri, J. C. and Kluger, J. and Coleman, C. I. and {From the Health Outcomes, Policy, and Economics (HOPE) Collaborative Group}},
  year = {2009},
  journal = {International Journal of Clinical Practice},
  volume = {63},
  number = {10},
  pages = {1426--1434},
  issn = {1742-1241},
  doi = {10.1111/j.1742-1241.2009.02168.x},
  urldate = {2025-05-11},
  abstract = {Background: Meta-regression has grown in popularity in recent years, paralleling the increasing numbers of systematic reviews and meta-analysis published in the biomedical literature. However, many clinicians and decision-makers may be unfamiliar with the underlying principles and assumptions made within meta-regression leading to incorrect interpretation of their results. Aims: This paper reviews the appropriate use and interpretation of meta-regression in the medical literature, including cautions and caveats to its use. Materials \& Methods: A literature search of MEDLINE (OVID) from 1966-February 2009 was conducted to identify literature relevant to the topic of heterogeneity and/or meta-regression in systematic reviews and meta-analysis. Results: Meta-analysis, a statistical method of pooling data from studies included in a systematic review, is often compromised by heterogeneity of its results. This could include clinical, methodological or statistical heterogeneity. Meta-regression, said to be a merging of meta-analytic and linear regression principles, is a more sophisticated tool for exploring heterogeneity. It aims to discern whether a linear relationship exists between an outcome measure and on or more covariates. The associations found in a meta-regression should be considered hypothesis generating and not regarded as proof of causality. Conclusions: The current review will enable clinicians and healthcare decision-makers to appropriately interpret the results of meta-regression when used within the constructs of a systematic review, and be able to extend it to their clinical practice.},
  copyright = {{\copyright} 2009 Blackwell Publishing Ltd},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/A36QW9G3/j.1742-1241.2009.02168.html}
}

@article{bakker_quality_poweranalysis,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje L. S. and {van Assen}, Marcel A. L. M. and Crompvoets, Elise A. V. and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David and Wicherts, Jelte M.},
  year = {2020},
  month = dec,
  journal = {PLoS Biology},
  volume = {18},
  number = {12},
  pages = {e3000937},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.3000937},
  urldate = {2021-12-08},
  abstract = {Researchers face many, often seemingly arbitrary, choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of ``researcher degrees of freedom'' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false-positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared 2 formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge Registration (now called ``OSF Preregistration,'' http://osf.io/prereg/). The Prereg Challenge format was a ``structured'' workflow with detailed instructions and an independent review to confirm completeness; the ``Standard'' format was ``unstructured'' with minimal direct guidance to give researchers flexibility for what to prespecify. Results of comparing random samples of 53 preregistrations from each format indicate that the ``structured'' format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the ``unstructured'' format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research., Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. A study of two formats of preregistration available on the OSF reveals that the opportunistic use of researcher degrees of freedom aimed at obtaining statistical significance is restricted by using more extensive preregistration guidelines; however, these guidelines should be further improved.},
  pmcid = {PMC7725296},
  pmid = {33296358},
  file = {/Users/cristian/Zotero/storage/AR27IGZV/Bakker et al. - 2020 - Ensuring the quality and specificity of preregistr.pdf}
}

@article{bakker_rules_game,
  title = {The {{Rules}} of the {{Game Called Psychological Science}}},
  author = {Bakker, Marjan and {van Dijk}, Annette and Wicherts, Jelte M.},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {543--554},
  doi = {10.1177/1745691612459060},
  urldate = {2021-02-01},
  abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96\% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {$<$} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
  langid = {english},
  keywords = {false positives,power,publication bias,replication,sample size},
  file = {/Users/cristian/Zotero/storage/6HBCFWHX/Bakker et al. - 2012 - The Rules of the Game Called Psychological Science.pdf}
}

@article{bakkerMisreportingStatisticalResults2011,
  title = {The (Mis)Reporting of Statistical Results in Psychology Journals},
  author = {Bakker, Marjan and Wicherts, Jelte M.},
  year = {2011},
  journal = {Behavior Research Methods},
  volume = {43},
  number = {3},
  pages = {666--678},
  doi = {10.3758/s13428-011-0089-5},
  abstract = {In order to study the prevalence, nature (direction), and causes of reporting errors in psychology, we checked the consistency of reported test statistics, degrees of freedom, and p values in a random sample of high- and low-impact psychology journals. In a second study, we established the generality of reporting errors in a random sample of recent psychological articles. Our results, on the basis of 281 articles, indicate that around 18\% of statistical results in the psychological literature are incorrectly reported. Inconsistencies were more common in low-impact journals than in high-impact journals. Moreover, around 15\% of the articles contained at least one statistical conclusion that proved, upon recalculation, to be incorrect; that is, recalculation rendered the previously significant result insignificant, or vice versa. These errors were often in line with researchers' expectations. We classified the most common errors and contacted authors to shed light on the origins of the errors.},
  file = {/Users/cristian/Zotero/storage/GLV6K7JK/Bakker and Wicherts - 2011 - The (mis)reporting of statistical results in psych.pdf}
}

@article{bakkerQuestionableOpenResearch2021,
  title = {Questionable and {{Open Research Practices}}: {{Attitudes}} and {{Perceptions}} among {{Quantitative Communication Researchers}}},
  shorttitle = {Questionable and {{Open Research Practices}}},
  author = {Bakker, Bert and Jaidka, Kokil and D{\"o}rr, Timothy and Fasching, Neil and Lelkes, Yphtach},
  year = {2021},
  month = oct,
  journal = {Journal of Communication},
  volume = {71},
  number = {5},
  pages = {715--738},
  issn = {0021-9916},
  doi = {10.1093/joc/jqab031},
  urldate = {2023-01-27},
  abstract = {Recent contributions have questioned the credibility of quantitative communication research. While questionable research practices (QRPs) are believed to be widespread, evidence for this belief is, primarily, derived from other disciplines. Therefore, it is largely unknown to what extent QRPs are used in quantitative communication research and whether researchers embrace open research practices (ORPs). We surveyed first and corresponding authors of publications in the top-20 journals in communication science. Many researchers report using one or more QRPs. We find widespread pluralistic ignorance: QRPs are generally rejected, but researchers believe they are prevalent. At the same time, we find optimism about the use of open science practices. In all, our study has implications for theories in communication that rely upon a cumulative body of empirical work: these theories are negatively affected by QRPs but can gain credibility if based upon ORPs. We outline an agenda to move forward as a discipline.},
  file = {/Users/cristian/Zotero/storage/ITFDNZAL/Bakker et al. - 2021 - Questionable and Open Research Practices Attitude.pdf;/Users/cristian/Zotero/storage/E5GIJB3R/6420674.html;/Users/cristian/Zotero/storage/PKZ9KK4T/6420674.html}
}

@article{bakkerRecommendationsPreregistrationsInternal2020,
  title = {Recommendations in Pre-Registrations and Internal Review Board Proposals Promote Formal Power Analyses but Do Not Increase Sample Size},
  author = {Bakker, Marjan and Veldkamp, Coosje L. S. and Van Den Akker, Olmo R. and Van Assen, Marcel A. L. M. and Crompvoets, Elise and Ong, How Hwee and Wicherts, Jelte M.},
  editor = {Benassi, Mariagrazia},
  year = {2020},
  month = jul,
  journal = {PLOS ONE},
  volume = {15},
  number = {7},
  pages = {e0236079},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236079},
  urldate = {2025-02-21},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/MEKTYRJD/Bakker et al. - 2020 - Recommendations in pre-registrations and internal .pdf}
}

@article{bakkerRecommendationsPreregistrationsInternal2020a,
  title = {Recommendations in Pre-Registrations and Internal Review Board Proposals Promote Formal Power Analyses but Do Not Increase Sample Size},
  author = {Bakker, Marjan and Veldkamp, Coosje L. S. and van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Crompvoets, Elise and Ong, How Hwee and Wicherts, Jelte M.},
  year = {2020},
  month = jul,
  journal = {PLOS ONE},
  volume = {15},
  number = {7},
  pages = {e0236079},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236079},
  urldate = {2025-06-30},
  abstract = {In this preregistered study, we investigated whether the statistical power of a study is higher when researchers are asked to make a formal power analysis before collecting data. We compared the sample size descriptions from two sources: (i) a sample of pre-registrations created according to the guidelines for the Center for Open Science Preregistration Challenge (PCRs) and a sample of institutional review board (IRB) proposals from Tilburg School of Behavior and Social Sciences, which both include a recommendation to do a formal power analysis, and (ii) a sample of pre-registrations created according to the guidelines for Open Science Framework Standard Pre-Data Collection Registrations (SPRs) in which no guidance on sample size planning is given. We found that PCRs and IRBs (72\%) more often included sample size decisions based on power analyses than the SPRs (45\%). However, this did not result in larger planned sample sizes. The determined sample size of the PCRs and IRB proposals (Md = 90.50) was not higher than the determined sample size of the SPRs (Md = 126.00; W = 3389.5, p = 0.936). Typically, power analyses in the registrations were conducted with G*power, assuming a medium effect size, {$\alpha$} = .05 and a power of .80. Only 20\% of the power analyses contained enough information to fully reproduce the results and only 62\% of these power analyses pertained to the main hypothesis test in the pre-registration. Therefore, we see ample room for improvements in the quality of the registrations and we offer several recommendations to do so.},
  langid = {english},
  keywords = {Analysis of variance,Computer software,Linear regression analysis,Metaanalysis,Open science,Psychology,Research ethics,Social sciences},
  file = {/Users/cristian/Zotero/storage/33W6VXNN/Bakker et al. - 2020 - Recommendations in pre-registrations and internal .pdf}
}

@article{bakkerResearchersIntuitionsPower2016,
  title = {Researchers' {{Intuitions About Power}} in {{Psychological Research}}},
  author = {Bakker, Marjan and Hartgerink, Chris H. J. and Wicherts, Jelte M. and {van der Maas}, Han L. J.},
  year = {2016},
  month = aug,
  journal = {Psychological Science},
  volume = {27},
  number = {8},
  pages = {1069--1977},
  doi = {10.1177/0956797616647519},
  abstract = {Many psychology studies are statistically underpowered. In part, this may be because many researchers rely on intuition, rules of thumb, and prior practice (along with practical considerations) to determine the number of subjects to test. In Study 1, we surveyed 291 published research psychologists and found large discrepancies between their reports of their preferred amount of power and the actual power of their studies (calculated from their reported typical cell size, typical effect size, and acceptable alpha). Furthermore, in Study 2, 89\% of the 214 respondents overestimated the power of specific research designs with a small expected effect size, and 95\% underestimated the sample size needed to obtain .80 power for detecting a small effect. Neither researchers' experience nor their knowledge predicted the bias in their self-reported power intuitions. Because many respondents reported that they based their sample sizes on rules of thumb or common practice in the field, we recommend that researchers conduct and report formal power analyses for their studies.},
  langid = {english},
  pmcid = {PMC4976648},
  pmid = {27354203},
  keywords = {effect size,Humans,Intuition,Knowledge,methodology,open data,open materials,power,Psychology,Research,Research Design,Research Personnel,sample size,Sample Size,Self Report,survey,Surveys and Questionnaires},
  file = {/Users/cristian/Zotero/storage/HWSSSYXW/Bakker et al. - 2016 - Researchers' Intuitions About Power in Psychologic.pdf}
}

@article{barboisBenefitRiskIntraoperative2017,
  title = {Benefit--Risk of Intraoperative Liver Biopsy during Bariatric Surgery: Review and Perspectives},
  author = {Barbois, S. and Arvieux, C. and Leroy, V. and Reche, F. and St{\"u}rm, N. and Borel, A.-L.},
  year = {2017},
  journal = {Surgery for Obesity and Related Diseases},
  volume = {13},
  number = {10},
  pages = {1780--1786},
  publisher = {Elsevier Inc.},
  doi = {10.1016/j.soard.2017.07.032}
}

@article{bartelProtectiveRiskFactors2015,
  title = {Protective and Risk Factors for Adolescent Sleep: {{A}} Meta-Analytic Review},
  author = {Bartel, K.A. and Gradisar, M. and Williamson, P.},
  year = {2015},
  journal = {Sleep Medicine Reviews},
  volume = {21},
  pages = {72--85},
  publisher = {W.B. Saunders Ltd},
  doi = {10.1016/j.smrv.2014.08.002}
}

@article{bartlett_jamovi,
  title = {Power to the {{People}}: {{A Beginner}}'s {{Tutorial}} to {{Power Analysis}} Using Jamovi},
  shorttitle = {Power to the {{People}}},
  author = {Bartlett, James and Charles, Sarah},
  year = {2022},
  month = nov,
  journal = {Meta-Psychology},
  volume = {6},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.3078},
  urldate = {2025-06-17},
  abstract = {Authors have highlighted for decades that sample size justification through power analysis is the exception rather than the rule. Even when authors do report a power analysis, there is often no justification for the smallest effect size of interest, or they do not provide enough information for the analysis to be reproducible. We argue one potential reason for these omissions is the lack of a truly accessible introduction to the key concepts and decisions behind power analysis. In this tutorial targeted at complete beginners, we demonstrate a priori and sensitivity power analysis using jamovi for two independent samples and two dependent samples. Respectively, these power analyses allow you to ask the questions: ``How many participants do I need to detect a given effect size?'', and ``What effect sizes can I detect with a given sample size?''. We emphasise how power analysis is most effective as a reflective process during the planning phase of research to balance your inferential goals with your resources. By the end of the tutorial, you will be able to understand the fundamental concepts behind power analysis and extend them to more advanced statistical models.},
  copyright = {Copyright (c) 2022 James E Bartlett, Sarah J Charles},
  langid = {english},
  keywords = {a priori,effect size,jamovi,Power analysis,sensitivity,tutorial},
  file = {/Users/cristian/Zotero/storage/NS5G4F6C/Bartlett and Charles - 2022 - Power to the People A Beginners Tutorial to Powe.pdf}
}

@article{bartos_zcurve_2020,
  title = {Z-Curve.2.0: {{Estimating}} Replication Rates and Discovery Rates},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = {2020},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/urgtn}
}

@article{bartos_zcurve_2020,
  title = {Z-Curve.2.0: {{Estimating}} Replication Rates and Discovery Rates},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = {2020},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/urgtn}
}

@article{bartosAdjustingPublicationBias2022,
  title = {Adjusting for {{Publication Bias}} in {{JASP}} and {{R}}: {{Selection Models}}, {{PET-PEESE}}, and {{Robust Bayesian Meta-Analysis}}},
  shorttitle = {Adjusting for {{Publication Bias}} in {{JASP}} and {{R}}},
  author = {Barto{\v s}, Franti{\v s}ek and Maier, Maximilian and Quintana, Daniel S. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {3},
  pages = {25152459221109259},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459221109259},
  urldate = {2022-09-21},
  abstract = {Meta-analyses are essential for cumulative science, but their validity can be compromised by publication bias. To mitigate the impact of publication bias, one may apply publication-bias-adjustment techniques such as precision-effect test and precision-effect estimate with standard errors (PET-PEESE) and selection models. These methods, implemented in JASP and R, allow researchers without programming experience to conduct state-of-the-art publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how to conduct a publication-bias-adjusted meta-analysis in JASP and R and interpret the results. First, we explain two frequentist bias-correction methods: PET-PEESE and selection models. Second, we introduce robust Bayesian meta-analysis, a Bayesian approach that simultaneously considers both PET-PEESE and selection models. We illustrate the methodology on an example data set, provide an instructional video (https://bit.ly/pubbias) and an R-markdown script (https://osf.io/uhaew/), and discuss the interpretation of the results. Finally, we include concrete guidance on reporting the meta-analytic results in an academic article.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/NEGSBEVG/Barto et al. - 2022 - Adjusting for Publication Bias in JASP and R Sele.pdf}
}

@misc{bartosZcurve2021,
  title = {Zcurve},
  author = {Barto{\v s}, Franti{\v s}ek},
  year = {2021},
  month = oct,
  urldate = {2021-11-10},
  abstract = {zcurve R package for assessing the reliability and trustworthiness of published literature with the z-curve method}
}

@misc{bartosZcurve2021,
  title = {Zcurve},
  author = {Barto{\v s}, Franti{\v s}ek},
  year = {2021},
  month = oct,
  urldate = {2021-11-10},
  abstract = {zcurve R package for assessing the reliability and trustworthiness of published literature with the z-curve method}
}

@article{bartosZcurve20Estimating2022,
  title = {Z-Curve 2.0: {{Estimating Replication Rates}} and {{Discovery Rates}}},
  shorttitle = {Z-Curve 2.0},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = {2022},
  month = sep,
  journal = {Meta-Psychology},
  volume = {6},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.2720},
  urldate = {2022-09-21},
  abstract = {Selection for statistical significance is a well-known factor that distorts the published literature and challenges the cumulative progress in science. Recent replication failures have fueled concerns that many published results are false-positives. Brunner and Schimmack (2020) developed z-curve, a method for estimating the expected replication rate (ERR) -- the predicted success rate of exact replication studies based on the mean power after selection for significance. This article introduces an extension of this method, z-curve 2.0. The main extension is an estimate of the expected discovery rate (EDR) -- the estimate of a proportion that the reported statistically significant results constitute from all conducted statistical tests. This information can be used to detect and quantify the amount of selection bias by comparing the EDR to the observed discovery rate (ODR; observed proportion of statistically significant results). In addition, we examined the performance of bootstrapped confidence intervals in simulation studies. Based on these results, we created robust confidence intervals with good coverage across a wide range of scenarios to provide information about the uncertainty in EDR and ERR estimates. We implemented the method in the zcurve R package (Barto{\v s} \&amp; Schimmack, 2020).},
  langid = {english},
  keywords = {Expected Discovery Rate,Expected Replication Rate,File-Drawer,Mixture Models,Power,Publication Bias,Selection Bias},
  file = {/Users/cristian/Zotero/storage/6NMSE9BC/Barto and Schimmack - 2022 - Z-curve 2.0 Estimating Replication Rates and Disc.pdf;/Users/cristian/Zotero/storage/UX6BWV36/Barto and Schimmack - 2022 - Z-curve 2.0 Estimating Replication Rates and Disc.pdf}
}

@incollection{beckerFailsafeFileDrawerNumber2005,
  title = {Failsafe {{N}} or {{File-Drawer Number}}},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}},
  author = {Becker, Betsy Jane},
  year = {2005},
  pages = {111--125},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470870168.ch7},
  urldate = {2025-05-11},
  abstract = {This chapter contains sections titled: Introduction Definition of the Failsafe N Examples Assumptions of the Failsafe N Variations on the Failsafe N Summary of the Examples Applications of the Failsafe N Conclusions Acknowledgement References},
  chapter = {7},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd},
  isbn = {978-0-470-87016-7},
  langid = {english},
  keywords = {'file-drawer' analysis,`offset publication bias',augmented Fisher test statistic,failsafe N or file-drawer number,file-drawer effect,Raudenbush's teacher expectancy data set,Stouffer tests},
  file = {/Users/cristian/Zotero/storage/4KFELLK8/0470870168.html}
}

@incollection{beckerMultivariateMetaanalysis2000,
  title = {Multivariate Meta-Analysis},
  booktitle = {Handbook of Applied Multivariate Statistics and Mathematical Modeling},
  author = {Becker, Betsy J.},
  year = {2000},
  pages = {499--525},
  publisher = {Academic Press},
  address = {San Diego, CA, US},
  doi = {10.1016/B978-012691360-6/50018-5},
  abstract = {The purpose of this chapter is to review meta-analytic methods for synthesis of multivariate data in a social science context. The approach to multivariate meta-analysis presented here can be applied regardless of the form of the effect of interest (be it an effect size, correlation, proportion, or some other index). In general, the goals of a multivariate meta-analysis are the same as those of univariate syntheses: to estimate magnitudes of effect across studies, and to examine variation in patterns of outcomes. Often the reviewer hopes to explain between-study differences in effect magnitudes using explanatory variables. The chapter begins with a brief description of how multivariate data arise in meta-analysis and the potential benefits and problems associated with several approaches to analyzing such data. I then discuss in detail how to conduct a meta-analysis in which one models the dependence in multivariate meta-analysis data. Effect-size and correlational data are given the most thorough treatment, as these are the two most common forms of multivariate meta-analysis data. Examples are drawn from two existing research syntheses. These methods should lead to more informative reviews, based in full knowledge of the complexity of the accumulated evidence. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-0-12-691360-6},
  keywords = {Effect Size (Statistical),Meta Analysis,Methodology,Multivariate Analysis,Social Sciences,Statistical Correlation},
  file = {/Users/cristian/Zotero/storage/G63I7G52/2001-06613-017.html}
}

@article{beckerSynthesizingStandardizedMeanchange1988,
  title = {Synthesizing Standardized Mean-Change Measures},
  author = {Becker, Betsy J.},
  year = {1988},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {41},
  pages = {257--278},
  publisher = {British Psychological Society},
  address = {United Kingdom},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.1988.tb00901.x},
  abstract = {Presents an approach for the meta-analysis of data from pretest--posttest designs. With this approach, data from studies using different designs may be compared directly and studies without control groups do not need to be omitted. The approach is based on a standardized mean-change measure, computed for each sample within a study, and it involves analysis of the standardized mean changes and differences in the standardized mean changes. Analyses are illustrated using results of studies of the effectiveness of mental practice on motor-skill development (e.g., D. Feltz and D. M. Landers [see PA, Vol 71:325]). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Meta Analysis,Motor Development,Test Standardization},
  file = {/Users/cristian/Zotero/storage/MF3Q7BEH/1989-14191-001.html;/Users/cristian/Zotero/storage/ZFDJKU7M/1989-14191-001.html}
}

@article{bellarEffectsLowdoseCaffeine2012,
  title = {Effects of Low-Dose Caffeine Supplementation on Early Morning Performance in the Standing Shot Put Throw},
  author = {Bellar, David M. and Kamimori, Gary and Judge, Lawrence and Barkley, Jacob E. and Ryan, Edward J. and Muller, Matthew and Glickman, Ellen L.},
  year = {2012},
  month = jan,
  journal = {European Journal of Sport Science},
  volume = {12},
  number = {1},
  pages = {57--61},
  publisher = {Routledge},
  issn = {1746-1391},
  doi = {10.1080/17461391.2010.536585},
  urldate = {2023-07-05},
  abstract = {The purpose of the present investigation was to assess the efficacy of low-dose caffeine use for early morning performance in the shot put event. A double-blind, randomized, crossover design was used to investigate the effects of buccal caffeine supplementation on early morning shot put execution in nine inter-collegiate track and field athletes. In one condition the participants received a piece of caffeinated gum designed to deliver 100 mg of caffeine in a buccal manner, and in a second condition a placebo gum. The gum was chewed for 5 min before being discarded. Participants then completed the first psychomotor vigilance task followed by a series of five warm-up throws, followed by six attempts with a shot put (7.26 kg for males, 4.0 kg for females) measured for distance. The protocol ended with a final psychomotor vigilance task. A repeated-measures analysis of variance (treatment*time) was used to compare performance between the caffeine and placebo treatments over the six measured attempts. A significant difference (treatment{\texttimes}throw) was observed (P=0.030, partial eta-squared = 0.259), indicating that the caffeine treatment produced better performance over the course of the six attempts subsequent to a warm-up. A paired samples t-test (Bonferroni-adjusted for multiple comparisons) revealed that the first attempt in the caffeine treatment (9.62{\textpm}1.71 m) and in the placebo treatment (9.05{\textpm}1.69 m) were significantly different (P = 0.050, effect size = 0.996, 95\%CI 1.02 to 0.13 m). Repeated-measures analysis of covariance revealed a significant (P=0.016, partial eta-squared = 0.650) interaction effect (treatment{\texttimes}mean reaction time), whereby both at the pre and post time points the mean reaction time on the psychomotor vigilance task was reduced under the caffeine treatment (caffeine: pre 0.306{\textpm}0.05 s, post 0.316{\textpm}0.08 s; placebo: pre 0.317{\textpm}0.06 s, post 0.323{\textpm}0.06 s). Based on these results, we suggest that caffeine gum can be beneficial for both performance and alertness if used by shot put athletes during early morning sessions.},
  keywords = {Caffeine,reaction time,shot put,time of day},
  file = {/Users/cristian/Zotero/storage/NY2XFLXZ/Bellar et al. - 2012 - Effects of low-dose caffeine supplementation on ea.pdf}
}

@misc{BenefitsBarriersRisks,
  title = {The {{Benefits}}, {{Barriers}}, and {{Risks}} of {{Big Team Science}} - {{Google Search}}},
  urldate = {2025-06-15},
  howpublished = {https://www.google.com/search?client=safari\&rls=en\&q=The+Benefits\%2C+Barriers\%2C+and+Risks+of+Big+Team+Science\&ie=UTF-8\&oe=UTF-8},
  file = {/Users/cristian/Zotero/storage/K6JA9EXJ/search.html}
}

@article{bennettRandomisedControlledTrial2019,
  title = {A Randomised Controlled Trial of Movement Quality-Focused Exercise versus Traditional Resistance Exercise for Improving Movement Quality and Physical Performance in Trained Adults},
  author = {Bennett, Hunter and Arnold, John and Martin, Max and Norton, Kevin and Davison, Kade},
  year = {2019},
  month = dec,
  journal = {Journal of Sports Sciences},
  volume = {37},
  number = {24},
  pages = {2806--2817},
  publisher = {Routledge},
  issn = {0264-0414},
  doi = {10.1080/02640414.2019.1665234},
  urldate = {2022-10-05},
  abstract = {The aim of this trial was to compare an eight-week individual movement quality versus traditional resistance training intervention on movement quality and physical performance. Forty-six trained adults were randomised to a movement quality-focused training (MQ) or a traditional resistance training (TRAD) group, and performed two individualised training sessions per week, for 8 weeks. Session-RPE (sRPE) was obtained from each session. Measures of movement quality (MovementSCREEN and Functional Movement Screen (FMS)) and physical performance were performed pre- and post-intervention. All measures improved significantly in both groups (3--14.5\%, p = {$<$}0.005). The between-group difference in MovementSCREEN composite score was not statistically significant (0.3, 95\% CI -3.4, 4.1, p = 0.852). However, change in FMS composite was significantly greater in MQ (1.3, 95\% CI 0.8, 1.8, p {$<$} 0.001). There were no significant between-group differences in physical performance (p = 0.060--0.960). The mean sRPE was significantly lower in MQ (5.25, SD 1.2) compared to TRAD (6.6 SD 1.0, p = {$<$}0.001). Thus, although movement quality scores were not distinctly greater in the MQ group, a movement quality specific intervention caused comparable improvements in physical performance compared to traditional resistance training but at lower perceived training intensity.},
  pmid = {31500505},
  keywords = {capacity,Functional,movement,movement system,musculoskeletal,screen}
}

@article{bernardsCurrentResearchStatistical2017,
  title = {Current {{Research}} and {{Statistical Practices}} in {{Sport Science}} and a {{Need}} for {{Change}}},
  author = {Bernards, Jake R. and Sato, Kimitake and Haff, G. Gregory and Bazyler, Caleb D.},
  year = {2017},
  month = dec,
  journal = {Sports},
  volume = {5},
  number = {4},
  pages = {87},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4663},
  doi = {10.3390/sports5040087},
  urldate = {2023-03-08},
  abstract = {Current research ideologies in sport science allow for the possibility of investigators producing statistically significant results to help fit the outcome into a predetermined theory. Additionally, under the current Neyman-Pearson statistical structure, some argue that null hypothesis significant testing (NHST) under the frequentist approach is flawed, regardless. For example, a p-value is unable to measure the probability that the studied hypothesis is true, unable to measure the size of an effect or the importance of a result, and unable to provide a good measure of evidence regarding a model or hypothesis. Many of these downfalls are key questions researchers strive to answer following an investigation. Therefore, a shift towards a magnitude-based inference model, and eventually a fully Bayesian framework, is thought to be a better fit from a statistical standpoint and may be an improved way to address biases within the literature. The goal of this article is to shed light on the current research and statistical shortcomings the field of sport science faces today, and offer potential solutions to help guide future research practices.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian,inference,magnitude,sport science,statistics},
  file = {/Users/cristian/Zotero/storage/FR54PRSY/Bernards et al. - 2017 - Current Research and Statistical Practices in Spor.pdf}
}

@article{birdUnderstandingReplicationCrisis2021,
  title = {Understanding the {{Replication Crisis}} as a {{Base Rate Fallacy}}},
  author = {Bird, Alexander},
  year = {2021},
  journal = {The British Journal for the Philosophy of Science},
  volume = {72},
  number = {4},
  pages = {965--993},
  doi = {10.1093/bjps/axy051},
  abstract = {Abstract The replication (replicability, reproducibility) crisis in social psychology and clinical medicine arises from the fact that many apparently well-confirmed experimental results are subsequently overturned by studies that aim to replicate the original study. The culprit is widely held to be poor science: questionable research practices, failure to publish negative results, bad incentives, and even fraud. In this article I argue that the high rate of failed replications is consistent with high-quality science. We would expect this outcome if the field of science in question produces a high proportion of false hypotheses prior to testing. If most of the hypotheses under test are false, then there will be many false hypotheses that are apparently supported by the outcomes of well conducted experiments and null hypothesis significance tests with a type-I error rate ({$\alpha$}) of 5\%. Failure to recognize this is to commit the fallacy of ignoring the base rate. I argue that this is a plausible diagnosis of the replication crisis and examine what lessons we thereby learn for the future conduct of science. 1.1.~ The replication crisis 1.2.~ The base rate fallacy 2.~ From the Base Rate Fallacy to the Replication Crisis 3.~ Explaining the `Replication Crisis: Low {$\pi$} and Non-negligible {$\alpha$} 3.1.~ Science with low {$\pi$} 3.2.~ The value of {$\alpha$}: Type-I errors 4.~ Other Explanations of the Crisis 4.1.~ Low statistical power 4.2.~ Publication bias 4.3.~ Bias, questionable research practices, and fraud 5.~ What Is to Be Done? 5.1.~ Quietism: This is the nature of science 5.2.~ Higher quality hypotheses 5.3.~ Decrease {$\alpha$} 6.~ Conclusion},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/I5CP4MH6/Bird - 2020 - Understanding the Replication Crisis as a Base Rat.pdf;/Users/cristian/Zotero/storage/3CGE3TSG/axy051.html;/Users/cristian/Zotero/storage/ENJBGAFX/axy051.html}
}

@article{bishopFallibilityScienceResponding2018,
  ids = {bishopFallibilityScienceResponding2018a},
  title = {Fallibility in {{Science}}: {{Responding}} to {{Errors}} in the {{Work}} of {{Oneself}} and {{Others}}},
  shorttitle = {Fallibility in {{Science}}},
  author = {Bishop, D. V. M.},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {432--438},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918776632},
  urldate = {2024-06-11},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/ESB93MQJ/Bishop - 2018 - Fallibility in Science Responding to Errors in th.pdf}
}

@article{bishopPsychologyExperimentalPsychologists2020,
  title = {The Psychology of Experimental Psychologists: {{Overcoming}} Cognitive Constraints to Improve Research: {{The}} 47th {{Sir Frederic Bartlett Lecture}}},
  author = {Bishop, Dorothy V. M.},
  year = {2020},
  month = jan,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {73},
  number = {1},
  pages = {1--19},
  doi = {10.1177/1747021819886519},
  abstract = {Like many other areas of science, experimental psychology is affected by a ``replication crisis'' that is causing concern in many fields of research. Approaches to tackling this crisis include better training in statistical methods, greater transparency and openness, and changes to the incentives created by funding agencies, journals, and institutions. Here, I argue that if proposed solutions are to be effective, we also need to take into account human cognitive constraints that can distort all stages of the research process, including design and execution of experiments, analysis of data, and writing up findings for publication. I focus specifically on cognitive schemata in perception and memory, confirmation bias, systematic misunderstanding of statistics, and asymmetry in moral judgements of errors of commission and omission. Finally, I consider methods that may help mitigate the effect of cognitive constraints: better training, including use of simulations to overcome statistical misunderstanding; specific programmes directed at inoculating against cognitive biases; adoption of Registered Reports to encourage more critical reflection in planning studies; and using methods such as triangulation and ``pre mortem'' evaluation of study design to foster a culture of dialogue and criticism.},
  langid = {english},
  keywords = {citation bias,confirmation bias,incentives,moral judgements,publication bias,Registered Reports,replication,Reproducibility,schemata,simulation,solutions,statistics},
  file = {/Users/cristian/Zotero/storage/7WHW3XEI/Bishop - 2020 - The psychology of experimental psychologists Over.pdf}
}

@article{blackwelderProvingNullHypothesis1982,
  ids = {blackwelderProvingNullHypothesis1982a},
  title = {"{{Proving}} the Null Hypothesis" in Clinical Trials},
  author = {Blackwelder, W. C.},
  year = {1982},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {3},
  number = {4},
  pages = {345--353},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(82)90024-1},
  abstract = {When designing a clinical trial to show whether a new or experimental therapy is as effective as a standard therapy (but not necessarily more effective), the usual null hypothesis of equality is inappropriate and leads to logical difficulties. Since therapies cannot be shown to be literally equivalent, the appropriate null hypothesis is that the standard therapy is more effective than the experimental therapy by at least some specified amount. The problem is presented in terms of a trial in which the outcome of interest is dichotomous; test statistics, confidence intervals, and sample size calculations are discussed. The required sample size may be larger for either null hypothesis formulation than for the other, depending on the specific assumptions made. Reporting results in terms of confidence intervals is especially useful for this type of trial.},
  langid = {english},
  pmid = {7160191},
  keywords = {Clinical Trials as Topic,Decision Theory,Humans,Random Allocation,Statistics as Topic,Therapeutics}
}

@article{blandBestOftForgotten2015,
  title = {Best (but Oft Forgotten) Practices: Testing for Treatment Effects in Randomized Trials by Separate Analyses of Changes from Baseline in Each Group Is a Misleading Approach},
  shorttitle = {Best (but Oft Forgotten) Practices},
  author = {Bland, J. Martin and Altman, Douglas G.},
  year = {2015},
  month = nov,
  journal = {The American Journal of Clinical Nutrition},
  volume = {102},
  number = {5},
  pages = {991--994},
  issn = {1938-3207},
  doi = {10.3945/ajcn.115.119768},
  abstract = {Researchers often analyze randomized trials and other comparative studies by separate analysis of changes from baseline in each parallel group. This may be the only analysis presented or it may be in addition to the direct comparison of allocated groups. We illustrate this by reference to 3 recently published nutritional trials. We show why this method of analysis may be highly misleading and may produce type I errors far greater than the 5\% that we expect. We recommend direct comparison of means between groups with the use of baseline as a covariate if required.},
  langid = {english},
  pmid = {26354536},
  keywords = {Guidelines as Topic,Humans,Nutritional Sciences,Randomized Controlled Trials as Topic,Statistics as Topic,United Kingdom}
}

@article{blandComparisonsBaselineRandomised2011,
  title = {Comparisons against Baseline within Randomised Groups Are Often Used and Can Be Highly Misleading},
  author = {Bland, J Martin and Altman, Douglas G},
  year = {2011},
  month = dec,
  journal = {Trials},
  volume = {12},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-264},
  urldate = {2023-06-30},
  abstract = {Background In randomised trials, rather than comparing randomised groups directly some researchers carry out a significance test comparing a baseline with a final measurement separately in each group. Methods We give several examples where this has been done. We use simulation to demonstrate that the procedure is invalid and also show this algebraically. Results This approach is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three. Conclusions Randomised groups should be compared directly by two-sample methods and separate tests against baseline are highly misleading.},
  pmcid = {PMC3286439},
  pmid = {22192231},
  file = {/Users/cristian/Zotero/storage/RS8JECRF/Bland and Altman - 2011 - Comparisons against baseline within randomised gro.pdf}
}

@article{blumkaitisEffectsExternalPneumatic2022,
  title = {Effects of an External Pneumatic Compression Device vs Static Compression Garment on Peripheral Circulation and Markers of Sports Performance and Recovery},
  author = {Blumkaitis, Julia C. and Moon, Jessica M. and Ratliff, Kayla M. and Stecker, Richard A. and Richmond, Scott R. and Sunderland, Kyle L. and Kerksick, Chad M. and Martin, Jeffrey S. and Mumford, Petey W.},
  year = {2022},
  month = jul,
  journal = {European Journal of Applied Physiology},
  volume = {122},
  number = {7},
  pages = {1709--1722},
  issn = {1439-6327},
  doi = {10.1007/s00421-022-04953-z},
  abstract = {PURPOSE: To identify the effects of a single 30~min partial lower leg external pneumatic compression (EPC) treatment compared to a static compression (SC) garment or a no treatment control (CTL) on markers of recovery and performance following a muscle damaging protocol. METHODS: Thirty healthy, active males~(23\,{\textpm}\,3~years; 180.2\,{\textpm}\,9.0~cm; 81.6\,{\textpm}\,11.3~kg) performed 100 drop jumps from a 0.6~m box followed by a randomized, single 30~min treatment of either a partial lower leg EPC device worn below the knee and above the ankle (110~mmHg), SC garment (20-30~mmHg) covering the foot and calf just below the knee, or no treatment CTL, and then returned 24 and 48~h later. Participants were assessed for measures of muscle soreness, fatigue, hemodynamics, blood lactate, muscle thickness, circumferences, and performance assessments. RESULTS: The drop jump protocol significantly increased muscle soreness (p\,{$<$}\,0.001), fatigue (p\,{$<$}\,0.001), blood flow (p\,{$<$}\,0.001), hemoglobin (p\,{$<$}\,0.001), and muscle oxygen saturation (SMO2; p\,{$<$}\,0.001). Countermovement jump and squat jump testing completed after treatment with either EPC, SC, or CTL revealed no differences for jump height between any condition. However, EPC treatment maintained consistent braking force and propulsive power measures across all timepoints for countermovement jump testing. EPC and SC treatment also led to better maintenance of squat jump performance for average relative propulsive force and power variables at 24 and 48~h compared to CTL. CONCLUSIONS: A single 30~min partial leg EPC treatment may lead to more consistent jump performance following a damaging bout of exercise.},
  langid = {english},
  pmid = {35475921},
  keywords = {{Muscle, Skeletal},\{Muscle\vphantom\},Athletic Performance,Clothing,Damage,Doppler ultrasound,Drop jumps,Exercise,Fatigue,Force,Force plates,Humans,Male,Myalgia,Partial external pneumatic compression,Power,Skeletal\vphantom\{\},Soreness and fatigue,Strength}
}

@article{bonnAdvancingScienceAdvancing2021,
  title = {Advancing Science or Advancing Careers? {{Researchers}}' Opinions on Success Indicators},
  shorttitle = {Advancing Science or Advancing Careers?},
  author = {Bonn, No{\'e}mie Aubert and Pinxten, Wim},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0243664},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0243664},
  urldate = {2022-10-07},
  abstract = {The way in which we assess researchers has been under the radar in the past few years. Critics argue that current research assessments focus on productivity and that they increase unhealthy pressures on scientists. Yet, the precise ways in which assessments should change is still open for debate. We circulated a survey with Flemish researchers to understand how they work, and how they would rate the relevance of specific indicators used in research assessments. We found that most researchers worked far beyond their expected working schedule. We also found that, although they spent most of their time doing research, respondents wished they could dedicate more time to it and spend less time writing grants and performing other activities such as administrative duties and meetings. When looking at success indicators, we found that indicators related to openness, transparency, quality, and innovation were perceived as highly important in advancing science, but as relatively overlooked in career advancement. Conversely, indicators which denoted of prestige and competition were generally rated as important to career advancement, but irrelevant or even detrimental in advancing science. Open comments from respondents further revealed that, although indicators which indicate openness, transparency, and quality (e.g., publishing open access, publishing negative findings, sharing data, etc.) should ultimately be valued more in research assessments, the resources and support currently in place were insufficient to allow researchers to endorse such practices. In other words, current research assessments are inadequate and ignore practices which are essential in contributing to the advancement of science. Yet, before we change the way in which researchers are being assessed, supporting infrastructures must be put in place to ensure that researchers are able to commit to the activities that may benefit the advancement of science.},
  langid = {english},
  keywords = {Careers,Careers in research,Open access publishing,Peer review,Publication ethics,Research assessment,Social media,Surveys},
  file = {/Users/cristian/Zotero/storage/68VYUUTN/Bonn and Pinxten - 2021 - Advancing science or advancing careers Researcher.pdf;/Users/cristian/Zotero/storage/36GTEYPF/article.html;/Users/cristian/Zotero/storage/RJN2TQW3/article.html}
}

@incollection{borensteinEffectSizesBased2009,
  title = {Effect {{Sizes Based}} on {{Means}}},
  booktitle = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry and Rothstein, Hannah},
  year = {2009},
  pages = {21--32},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470743386.ch4},
  urldate = {2023-02-01},
  abstract = {This chapter contains sections titled: Introduction Raw (unstandardized) mean difference D Standardized mean difference, d and g Response ratios Summary points},
  chapter = {4},
  isbn = {978-0-470-74338-6},
  langid = {english},
  keywords = {computing D from studies that use matched groups or pre-post scores,different study designs in the same analysis,log response ratio,raw mean difference,response ratios},
  file = {/Users/cristian/Zotero/storage/ACT4U4DC/9780470743386.html;/Users/cristian/Zotero/storage/LUFVL298/9780470743386.html}
}

@article{borensteinHowUnderstandReport2023,
  title = {How to Understand and Report Heterogeneity in a Meta-Analysis: {{The}} Difference between {{I-squared}} and Prediction Intervals},
  shorttitle = {How to Understand and Report Heterogeneity in a Meta-Analysis},
  author = {Borenstein, Michael},
  year = {2023},
  month = dec,
  journal = {Integrative Medicine Research},
  volume = {12},
  number = {4},
  pages = {101014},
  issn = {2213-4220},
  doi = {10.1016/j.imr.2023.101014},
  urldate = {2025-05-19},
  abstract = {In any meta-analysis it is important to report not only the mean effect size but also how the effect size varies across studies. A treatment that has a moderate clinical impact in all studies is very different than a treatment where the impact is moderate on average, but in some studies is large and in others is trivial (or even harmful). A treatment that has no impact in any studies is very different than a treatment that has no impact on average because it is helpful in some studies but harmful in others. The majority of meta-analyses use the I-squared index to quantify heterogeneity. While this practice is common it is nevertheless incorrect. I-squared does not tell us how much the effect size varies (except when I-squared is zero percent). The statistic that does convey this information is the prediction interval. It allows us to report, for example, that a treatment has a clinically trivial or moderate effect in roughly 10~\% of studies, a large effect in roughly 50~\%, and a very large effect in roughly 40~\%. This is the information that researchers or clinicians have in mind when they ask about heterogeneity. It is the information that researchers believe (incorrectly) is provided by I-squared.},
  keywords = {Confidence interval,Heterogeneity,I,I-squared,Meta-analysis,Prediction interval,Systematic review,Tau-squared},
  file = {/Users/cristian/Zotero/storage/5W9DJBRQ/S2213422023000938.html}
}

@book{borensteinIntroductionMetaAnalysis2021,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P. T. and Rothstein, Hannah R.},
  year = {2021},
  month = apr,
  publisher = {John Wiley \& Sons},
  abstract = {A clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies The first edition of this text was widely acclaimed for the clarity of the presentation, and quickly established itself as the definitive text in this field. The fully updated second edition includes new and expanded content on avoiding common mistakes in meta-analysis, understanding heterogeneity in effects, publication bias, and more. Several brand-new chapters provide a systematic "how to" approach to performing and reporting a meta-analysis from start to finish. Written by four of the world's foremost authorities on all aspects of meta-analysis, the new edition:  Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in effect size across studies Explains how to avoid common mistakes in meta-analysis Discusses controversies in meta-analysis Includes access to a companion website containing videos, spreadsheets, data files, free software for prediction intervals, and step-by-step instructions for performing analyses using Comprehensive Meta-Analysis (CMA)  Download videos, class materials, and worked examples at www.Introduction-to-Meta-Analysis.com "This book offers the reader a unified framework for thinking about meta-analysis, and then discusses all elements of the analysis within that framework. The authors address a series of common mistakes and explain how to avoid them. As the editor-in-chief of the American Psychologist and former editor of Psychological Bulletin, I can say without hesitation that the quality of manuscript submissions reporting meta-analyses would be vastly better if researchers read this book."---Harris Cooper, Hugo L. Blomquist Distinguished Professor Emeritus of Psychology and Neuroscience, Editor-in-chief of the American Psychologist, former editor of Psychological Bulletin "A superb combination of lucid prose and informative graphics, the authors provide a refreshing departure from cookbook approaches with their clear explanations of the what and why of meta-analysis. The book is ideal as a course textbook or for self-study. My students raved about the clarity of the explanations and examples." ---David Rindskopf, Distinguished Professor of Educational Psychology, City University of New York, Graduate School and University Center, \& Editor of the Journal of Educational and Behavioral Statistics "The approach taken by Introduction to Meta-analysis is intended to be primarily conceptual, and it is amazingly successful at achieving that goal. The reader can comfortably skip the formulas and still understand their application and underlying motivation. For the more statistically sophisticated reader, the relevant formulas and worked examples provide a superb practical guide to performing a meta-analysis. The book provides an eclectic mix of examples from education, social science, biomedical studies, and even ecology. For anyone considering leading a course in meta-analysis, or pursuing self-directed study, Introduction to Meta-analysis would be a clear first choice." ---Jesse A. Berlin, ScD},
  googlebooks = {pdQnEAAAQBAJ},
  isbn = {978-1-119-55838-5},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability {\textbackslash}\& Statistics / Stochastic Processes,Mathematics / Probability \& Statistics / Stochastic Processes,Medical / Biostatistics}
}

@article{borensteinMetaAnalysisSubgroups2013,
  title = {Meta-{{Analysis}} and {{Subgroups}}},
  author = {Borenstein, Michael and Higgins, Julian P. T.},
  year = {2013},
  month = apr,
  journal = {Prevention Science},
  volume = {14},
  number = {2},
  pages = {134--143},
  issn = {1573-6695},
  doi = {10.1007/s11121-013-0377-7},
  urldate = {2025-04-29},
  abstract = {Subgroup analysis is the process of comparing a treatment effect for two or more variants of an intervention---to ask, for example, if an intervention's impact is affected by the setting (school versus community), by the delivery agent (outside facilitator versus regular classroom teacher), by the quality of delivery, or if the long-term effect differs from the short-term effect. While large-scale studies often employ subgroup analyses, these analyses cannot generally be performed for small-scale studies, since these typically include a homogeneous population and only one variant of the intervention. This limitation can be bypassed by using meta-analysis. Meta-analysis allows the researcher to compare the treatment effect in different subgroups, even if these subgroups appear in separate studies. We discuss several statistical issues related to this procedure, including the selection of a statistical model and statistical power for the comparison. To illustrate these points, we use the example of a meta-analysis of obesity prevention.},
  langid = {english},
  keywords = {Fixed-effect,Fixed-effects,Meta-analysis,Random-effects,Research synthesis,Subgroup analysis,Subgroups,Systematic review}
}

@article{borgBiasStatisticalSignificance2023,
  ids = {borgBiasStatisticalSignificance2023a},
  title = {The Bias for Statistical Significance in Sport and Exercise Medicine},
  author = {Borg, David N. and Barnett, Adrian G. and Caldwell, Aaron R. and White, Nicole M. and Stewart, Ian B.},
  year = {2023},
  month = mar,
  journal = {Journal of Science and Medicine in Sport},
  volume = {26},
  number = {3},
  pages = {164--168},
  issn = {1878-1861},
  doi = {10.1016/j.jsams.2023.03.002},
  abstract = {OBJECTIVES: We aimed to examine the bias for statistical significance using published confidence intervals in sport and exercise medicine research. DESIGN: Observational study. METHODS: The abstracts of 48,390 articles, published in 18 sports and exercise medicine journals between 2002 and 2022, were searched using a validated text-mining algorithm that identified and extracted ratio confidence intervals (odds, hazard, and risk ratios). The algorithm identified 1744 abstracts that included ratio confidence intervals, from which 4484 intervals were extracted. After excluding ineligible intervals, the analysis used 3819 intervals, reported as 95\,\% confidence intervals, from 1599 articles. The cumulative distributions of lower and upper confidence limits were plotted to identify any abnormal patterns, particularly around a ratio of 1 (the null hypothesis). The distributions were compared to those from unbiased reference data, which was not subjected to p-hacking or publication bias. A bias for statistical significance was further investigated using a histogram plot of z-values calculated from the extracted 95\,\% confidence intervals. RESULTS: There was a marked change in the cumulative distribution of lower and upper bound intervals just over and just under a ratio of 1. The bias for statistical significance was also clear in a stark under-representation of z-values between -1.96 and +1.96, corresponding to p-values above 0.05. CONCLUSIONS: There was an excess of published research with statistically significant results just below the standard significance threshold of 0.05, which is indicative of publication bias. Transparent research practices, including the use of registered reports, are needed to reduce the bias in published research.},
  langid = {english},
  pmid = {36966124},
  keywords = {-hacking,Bias,Confidence interval,Estimation,Exercise,Humans,Misconduct,Odds Ratio,p-hacking,Publication bias,Publication Bias,Selective reporting,Sports},
  file = {/Users/cristian/Zotero/storage/M9YHF69R/Borg et al. - 2023 - The bias for statistical significance in sport and.pdf;/Users/cristian/Zotero/storage/7ZP2IS9U/S1440244023000403.html;/Users/cristian/Zotero/storage/AAVWDDUN/S1440244023000403.html}
}

@article{borgCommentEffectsHeat2022,
  title = {Comment on ``{{Effects}} of {{Heat Acclimation}} and {{Acclimatisation}} on {{Maximal Aerobic Capacity Compared}} to {{Exercise Alone}} in {{Both Thermoneutral}} and {{Hot Environments}}: {{A Meta-Analysis}} and {{Meta-Regression}}''},
  shorttitle = {Comment on ``{{Effects}} of {{Heat Acclimation}} and {{Acclimatisation}} on {{Maximal Aerobic Capacity Compared}} to {{Exercise Alone}} in {{Both Thermoneutral}} and {{Hot Environments}}},
  author = {Borg, David N. and O'Brien, Julia L.},
  year = {2022},
  month = jul,
  journal = {Sports Medicine},
  volume = {52},
  number = {7},
  pages = {1715--1718},
  issn = {1179-2035},
  doi = {10.1007/s40279-021-01611-w},
  urldate = {2023-03-17},
  langid = {english}
}

@article{borgCommentMovingSport2020,
  title = {Comment on: `{{Moving Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}'},
  author = {Borg, David N. and Bon, Joshua J. and Sainani, Kristin L. and Baguley, Brenton J. and Tierney, Nicholas J. and Drovandi, Christopher},
  year = {2020},
  month = aug,
  journal = {Sports Medicine},
  volume = {50},
  number = {8},
  pages = {1551--1553},
  issn = {1179-2035},
  doi = {10.1007/s40279-020-01298-5}
}

@article{borgMetaanalysisPredictionIntervals2024,
  title = {Meta-Analysis Prediction Intervals Are under Reported in Sport and Exercise Medicine},
  author = {Borg, David N. and Impellizzeri, Franco M. and Borg, Samantha J. and Hutchins, Kate P. and Stewart, Ian B. and Jones, Tamara and Baguley, Brenton J. and Orssatto, Lucas B. R. and Bach, Aaron J. E. and Osborne, John O. and McMaster, Benjamin S. and Buhmann, Robert L. and Bon, Joshua J. and Barnett, Adrian G.},
  year = {2024},
  journal = {Scandinavian Journal of Medicine \& Science in Sports},
  volume = {34},
  number = {3},
  pages = {e14603},
  issn = {1600-0838},
  doi = {10.1111/sms.14603},
  urldate = {2025-05-08},
  abstract = {Aim Prediction intervals are a useful measure of uncertainty for meta-analyses that capture the likely effect size of a new (similar) study based on the included studies. In comparison, confidence intervals reflect the uncertainty around the point estimate but provide an incomplete summary of the underlying heterogeneity in the meta-analysis. This study aimed to estimate (i) the proportion of meta-analysis studies that report a prediction interval in sports medicine; and (ii) the proportion of studies with a discrepancy between the reported confidence interval and a calculated prediction interval. Methods We screened, at random, 1500 meta-analysis studies published between 2012 and 2022 in highly ranked sports medicine and medical journals. Articles that used a random effect meta-analysis model were included in the study. We randomly selected one meta-analysis from each article to extract data from, which included the number of estimates, the pooled effect, and the confidence and prediction interval. Results Of the 1500 articles screened, 866 (514 from sports medicine) used a random effect model. The probability of a prediction interval being reported in sports medicine was 1.7\% (95\% CI = 0.9\%, 3.3\%). In medicine the probability was 3.9\% (95\% CI = 2.4\%, 6.6\%). A prediction interval was able to be calculated for 220 sports medicine studies. For 60\% of these studies, there was a discrepancy in study findings between the reported confidence interval and the calculated prediction interval. Prediction intervals were 3.4 times wider than confidence intervals. Conclusion Very few meta-analyses report prediction intervals and hence are prone to missing the impact of between-study heterogeneity on the overall conclusions. The widespread misinterpretation of random effect meta-analyses could mean that potentially harmful treatments, or those lacking a sufficient evidence base, are being used in practice. Authors, reviewers, and editors should be aware of the importance of prediction intervals.},
  copyright = {{\copyright} 2024 The Authors. Scandinavian Journal of Medicine \& Science In Sports published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {confidence interval,forest plot,heterogeneity,random effects model,review},
  file = {/Users/cristian/Zotero/storage/MCE2LGD6/Borg et al. - 2024 - Meta-analysis prediction intervals are under repor.pdf;/Users/cristian/Zotero/storage/KR95GP7W/sms.html}
}

@article{borm_2007,
  title = {A Simple Sample Size Formula for Analysis of Covariance in Randomized Clinical Trials},
  author = {Borm, George F. and Fransen, Jaap and Lemmens, Wim A. J. G.},
  year = {2007},
  month = dec,
  journal = {Journal of Clinical Epidemiology},
  volume = {60},
  number = {12},
  pages = {1234--1238},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2007.02.006},
  abstract = {OBJECTIVE: Randomized clinical trials that compare two treatments on a continuous outcome can be analyzed using analysis of covariance (ANCOVA) or a t-test approach. We present a method for the sample size calculation when ANCOVA is used. STUDY DESIGN AND SETTING: We derived an approximate sample size formula. Simulations were used to verify the accuracy of the formula and to improve the approximation for small trials. The sample size calculations are illustrated in a clinical trial in rheumatoid arthritis. RESULTS: If the correlation between the outcome measured at baseline and at follow-up is rho, ANCOVA comparing groups of (1-rho(2))n subjects has the same power as t-test comparing groups of n subjects. When on the same data, ANCOVA is used instead of t-test, the precision of the treatment estimate is increased, and the length of the confidence interval is reduced by a factor 1-rho(2). CONCLUSION: ANCOVA may considerably reduce the number of patients required for a trial.},
  langid = {english},
  pmid = {17998077},
  keywords = {{Arthritis, Rheumatoid},{Drug Therapy, Combination},Analysis of Variance,Antirheumatic Agents,Humans,Isoxazoles,Leflunomide,Randomized Controlled Trials as Topic,Research Design,Sample Size,Sulfasalazine,Treatment Outcome}
}

@article{borsboomTheoryConstructionMethodology2021,
  title = {Theory {{Construction Methodology}}: {{A Practical Framework}} for {{Building Theories}} in {{Psychology}}},
  shorttitle = {Theory {{Construction Methodology}}},
  author = {Borsboom, Denny and {van der Maas}, Han L. J. and Dalege, Jonas and Kievit, Rogier A. and Haig, Brian D.},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {756--766},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620969647},
  urldate = {2024-07-24},
  abstract = {This article aims to improve theory formation in psychology by developing a practical methodology for constructing explanatory theories: theory construction methodology (TCM). TCM is a sequence of five steps. First, the theorist identifies a domain of empirical phenomena that becomes the target of explanation. Second, the theorist constructs a prototheory, a set of theoretical principles that putatively explain these phenomena. Third, the prototheory is used to construct a formal model, a set of model equations that encode explanatory principles. Fourth, the theorist investigates the explanatory adequacy of the model by formalizing its empirical phenomena and assessing whether it indeed reproduces these phenomena. Fifth, the theorist studies the overall adequacy of the theory by evaluating whether the identified phenomena are indeed reproduced faithfully and whether the explanatory principles are sufficiently parsimonious and substantively plausible. We explain TCM with an example taken from research on intelligence (the mutualism model of intelligence), in which key elements of the method have been successfully implemented. We discuss the place of TCM in the larger scheme of scientific research and propose an outline for a university curriculum that can systematically educate psychologists in the process of theory formation.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/3KKN58SB/Borsboom et al. - 2021 - Theory Construction Methodology A Practical Frame.pdf}
}

@article{bortolatoCognitiveDysfunctionMajor2014,
  title = {Cognitive Dysfunction in Major Depressive Disorder: {{A}} State-of-the-Art Clinical Review},
  author = {Bortolato, B. and Carvalho, A.F. and McIntyre, R.S.},
  year = {2014},
  journal = {CNS and Neurological Disorders - Drug Targets},
  volume = {13},
  number = {10},
  pages = {1804--1818},
  publisher = {Bentham Science Publishers},
  doi = {10.2174/1871527313666141130203823}
}

@article{bossiAddingIntermittentVibration2023,
  title = {Adding {{Intermittent Vibration}} to {{Varied-intensity Work Intervals}}: {{No Extra Benefit}}},
  shorttitle = {Adding {{Intermittent Vibration}} to {{Varied-intensity Work Intervals}}},
  author = {Bossi, Arthur Henrique and Mesquida, Cristian and Hopker, James and R{\o}nnestad, Bent Ronny},
  year = {2023},
  month = feb,
  journal = {International Journal of Sports Medicine},
  volume = {44},
  number = {2},
  pages = {126--132},
  issn = {1439-3964},
  doi = {10.1055/a-1812-7600},
  abstract = {Varied-intensity work intervals have been shown to induce higher fractions of maximal oxygen uptake during high-intensity interval training compared with constant-intensity work intervals. We assessed whether varied-intensity work intervals combined with intermittent vibration could further increase cyclists' fraction of maximal oxygen uptake to potentially optimise adaptive stimulus. Thirteen cyclists ({\.V}O2max: 69.7{\textpm}7.1\,ml{$\cdot$}kg-1{$\cdot$}min-1) underwent a performance assessment and two high-intensity interval training sessions. Both comprised six 5-minute varied-intensity work intervals within which the work rate was alternated between 100\% (3{\texttimes}30-second blocks, with or without vibration) and 77\% of maximal aerobic power (always without vibration). Adding vibration to varied-intensity work intervals did not elicit a longer time above ninety percent of maximal oxygen uptake (415{\textpm}221 versus 399{\textpm}209\,seconds, P=0.69). Heart rate- and perceptual-based training-load metrics were also not affected (all P{$\geq$}0.59). When considering individual work intervals, no between-condition differences were found (fraction of maximal oxygen uptake, P=0.34; total oxygen uptake, P=0.053; mean minute ventilation, P=0.079; mean heart rate, P=0.88; blood lactate concentration, P=0.53; ratings of perceived exertion, P=0.29). Adding intermittent vibration to varied-intensity work intervals does not increase the fraction of maximal oxygen uptake elicited. Whether intermittent exposure to vibration can enhance cyclists' adaptive stimulus triggered by high-intensity interval training remains to be determined.},
  langid = {english},
  pmid = {35354204},
  keywords = {Heart Rate,High-Intensity Interval Training,Humans,Lactic Acid,Oxygen,Oxygen Consumption,Vibration}
}

@article{bossiOptimizingIntervalTraining2020,
  title = {Optimizing {{Interval Training Through Power-Output Variation Within}} the {{Work Intervals}}},
  author = {Bossi, Arthur H. and Mesquida, Cristian and Passfield, Louis and R{\o}nnestad, Bent R. and Hopker, James G.},
  year = {2020},
  month = apr,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {15},
  number = {7},
  pages = {982--989},
  publisher = {Human Kinetics},
  issn = {1555-0273, 1555-0265},
  doi = {10.1123/ijspp.2019-0260},
  urldate = {2025-03-29},
  abstract = {Purpose: Maximal oxygen uptake ( VO2max) is a key determinant of endurance performance. Therefore, devising high-intensity interval training (HIIT) that maximizes stress of the oxygen-transport and -utilization systems may be important to stimulate further adaptation in athletes. The authors compared physiological and perceptual responses elicited by work intervals matched for duration and mean power output but differing in power-output distribution. Methods: Fourteen cyclists ( VO2max 69.2 [6.6] mL{$\cdot$}kg-1{$\cdot$}min-1) completed 3 laboratory visits for a performance assessment and 2 HIIT sessions using either varied-intensity or constant-intensity work intervals. Results: Cyclists spent more time at {$>$}90\%VO2max during HIIT with varied-intensity work intervals (410 [207] vs 286 [162] s, P\,=\,.02), but there were no differences between sessions in heart-rate- or perceptual-based training-load metrics (all P\,{$\geq$}\,.1). When considering individual work intervals, minute ventilation ( VE) was higher in the varied-intensity mode (F\,=\,8.42, P\,=\,.01), but not respiratory frequency, tidal volume, blood lactate concentration [La], ratings of perceived exertion, or cadence (all F\,{$\leq$}\,3.50,\,{$\geq$}\,.08). Absolute changes ({$\Delta$}) between HIIT sessions were calculated per work interval, and {$\Delta$} total oxygen uptake was moderately associated with {$\Delta$}VE (r\,=\,.36, P\,=\,.002). Conclusions: In comparison with an HIIT session with constant-intensity work intervals, well-trained cyclists sustain higher fractions of VO2max when work intervals involved power-output variations. This effect is partially mediated by an increased oxygen cost of hyperpnea and not associated with a higher [La], perceived exertion, or training-load metrics.},
  chapter = {International Journal of Sports Physiology and Performance},
  langid = {american},
  keywords = {elite cycling,exercise hyperpnea,intensity prescription,maximal aerobic power,time at VO2max},
  file = {/Users/cristian/Zotero/storage/FS5UCYZW/Bossi et al. - 2020 - Optimizing Interval Training Through Power-Output .pdf}
}

@article{bougouinNitrogenExcretionBeef2022,
  title = {Nitrogen Excretion from Beef Cattle Fed a Wide Range of Diets Compiled in an Intercontinental Dataset: A Meta-Analysis.},
  author = {Bougouin, Adeline and Hristov, Alexander and Zanetti, Diego and Filho, Sebastiao C. V. and Renn{\'o}, Lucianna N. and Menezes, Ana C. B. and Silva, Jarbas M. and Alhadas, Herlon M. and Mariz, Lays D. S. and Prados, Laura F. and Beauchemin, Karen A. and McAllister, Tim and Yang, WenZhu Z. and Koenig, Karen M. and Goossens, Karen and Yan, Tianhai and Noziere, Pierre and Jonker, Arjan and Kebreab, Ermias},
  year = {2022},
  month = sep,
  journal = {Journal of animal science},
  volume = {100},
  number = {9},
  address = {United States},
  issn = {1525-3163 0021-8812},
  doi = {10.1093/jas/skac150},
  abstract = {Manure N from cattle contributes to nitrate leaching, nitrous oxide, and ammonia emissions. Measurement of manure N outputs on commercial beef cattle operations  is laborious, expensive, and impractical; therefore, models are needed to predict  N excreted in urine and feces. Building robust prediction models requires  extensive data from animals under different management systems worldwide. Thus,  the study objectives were to 1) collate an international dataset of N excretion  in feces and urine based on individual observations from beef cattle; 2)  determine the suitability of key variables for predicting fecal, urinary, and  total manure N excretion; and 3) develop robust and reliable N excretion  prediction models based on individual observation from beef cattle consuming  various diets. A meta-analysis based on individual beef data from different  experiments was carried out from a raw dataset including 1,004 observations from  33 experiments collected from 5 research institutes in Europe (n = 3), North  America (n = 1), and South America (n = 1). A sequential approach was taken in  developing models of increasing complexity by incrementally adding significant  variables that affected fecal, urinary, or total manure N excretion. Nitrogen  excretion was predicted by fitting linear mixed models with experiment as a  random effect. Simple models including dry matter intake (DMI) were better at  predicting fecal N excretion than those using only dietary nutrient composition  or body weight (BW). Simple models based on N intake performed better for urinary  and total manure N excretion than those based on DMI. A model including DMI and  dietary component concentrations led to the most robust prediction of fecal and  urinary N excretion, generating root mean square prediction errors as a  percentage of the observed mean values of 25.0\% for feces and 25.6\% for urine.  Complex total manure N excretion models based on BW and dietary component  concentrations led to the lowest prediction errors of about 14.6\%. In conclusion,  several models to predict N excretion already exist, but the ones developed in  this study are based on individual observations encompassing larger variability  than the previous developed models. In addition, models that include information  on DMI or N intake are required for accurate prediction of fecal, urinary, and  total manure N excretion. In the absence of intake data, equations have poor  performance as compared with equations based on intake and dietary component  concentrations.},
  copyright = {Published by Oxford University Press on behalf of the American Society of Animal Science 2022.},
  langid = {english},
  pmcid = {PMC9486885},
  pmid = {35460418},
  keywords = {*Manure/analysis,*Nitrogen/analysis,Ammonia/analysis,Animal Feed/analysis,Animals,beef cattle,Body Weight,Cattle,Diet/veterinary,Feces/chemistry,Nitrates,nitrogen excretion,Nitrous Oxide/analysis,prediction models}
}

@article{bozkurtCurrentDiagnosticTreatment2016,
  title = {Current {{Diagnostic}} and {{Treatment Strategies}} for {{Specific Dilated Cardiomyopathies}}: {{A Scientific Statement}} from the {{American Heart Association}}},
  author = {Bozkurt, B. and Colvin, M. and Cook, J. and Cooper, L.T. and Deswal, A. and Fonarow, G.C. and Francis, G.S. and Lenihan, D. and Lewis, E.F. and McNamara, D.M. and Pahl, E. and Vasan, R.S. and Ramasubbu, K. and Rasmusson, K. and Towbin, J.A. and Yancy, C.},
  year = {2016},
  journal = {Circulation},
  volume = {134},
  number = {23},
  pages = {e579-e646},
  publisher = {{Lippincott Williams and Wilkins}},
  doi = {10.1161/CIR.0000000000000455}
}

@article{braakhuisCorrectionEffectNew2021,
  ids = {braakhuisCorrectionEffectNew2021a},
  title = {Correction to: {{The}} Effect of {{New Zealand}} Blackcurrant on Sport Performance and Related Biomarkers: A Systematic Review and Meta-Analysis ({{Journal}} of the {{International Society}} of {{Sports Nutrition}}, (2020), 17, 1, (25), 10.1186/S12970-020-00354-9)},
  author = {Braakhuis, A.J. and Somerville, V.X. and Hurst, R.D.},
  year = {2021},
  journal = {Journal of the International Society of Sports Nutrition},
  volume = {18},
  number = {1},
  publisher = {BioMed Central Ltd},
  doi = {10.1186/s12970-020-00398-x}
}

@article{brandPrecisionEffectSize2016,
  title = {The {{Precision}} of {{Effect Size Estimation From Published Psychological Research}}: {{Surveying Confidence Intervals}}},
  author = {Brand, Andrew and Bradley, Michael T.},
  year = {2016},
  month = feb,
  journal = {Psychological Reports},
  volume = {118},
  number = {1},
  pages = {154--170},
  doi = {10.1177/0033294115625265},
  abstract = {Confidence interval (CI) widths were calculated for reported Cohen's d standardized effect sizes and examined in two automated surveys of published psychological literature. The first survey reviewed 1,902 articles from Psychological Science. The second survey reviewed a total of 5,169 articles from across the following four APA journals: Journal of Abnormal Psychology, Journal of Applied Psychology, Journal of Experimental Psychology: Human Perception and Performance, and Developmental Psychology. The median CI width for d was greater than 1 in both surveys. Hence, CI widths were, as Cohen (1994) speculated, embarrassingly large. Additional exploratory analyses revealed that CI widths varied across psychological research areas and that CI widths were not discernably decreasing over time. The theoretical implications of these findings are discussed along with ways of reducing the CI widths and thus improving precision of effect size estimation.},
  langid = {english},
  keywords = {Confidence Interval widths,effect sizes,measurement precision},
  file = {/Users/cristian/Zotero/storage/S7AWIAZ2/Brand and Bradley - 2016 - The Precision of Effect Size Estimation From Publi.pdf}
}

@article{brembsDeepImpactUnintended2013,
  title = {Deep Impact: Unintended Consequences of Journal Rank},
  shorttitle = {Deep Impact},
  author = {Brembs, Bj{\"o}rn and Button, Katherine and Munaf{\`o}, Marcus},
  year = {2013},
  journal = {Frontiers in Human Neuroscience},
  volume = {7},
  pages = {291},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00291},
  urldate = {2021-11-14},
  abstract = {Most researchers acknowledge an intrinsic hierarchy in the scholarly journals (``journal rank'') that they submit their work to, and adjust not only their submission but also their reading strategies accordingly. On the other hand, much has been written about the negative effects of institutionalizing journal rank as an impact measure. So far, contributions to the debate concerning the limitations of journal rank as a scientific impact assessment tool have either lacked data, or relied on only a few studies. In this review, we present the most recent and pertinent data on the consequences of our current scholarly communication system with respect to various measures of scientific quality (such as utility/citations, methodological soundness, expert ratings or retractions). These data corroborate previous hypotheses: using journal rank as an assessment tool is bad scientific practice. Moreover, the data lead us to argue that any journal rank (not only the currently-favored Impact Factor) would have this negative impact. Therefore, we suggest that abandoning journals altogether, in favor of a library-based scholarly communication system, will ultimately be necessary. This new system will use modern information technology to vastly improve the filter, sort and discovery functions of the current journal system.},
  file = {/Users/cristian/Zotero/storage/WF5J3ZSE/Brembs et al. - 2013 - Deep impact unintended consequences of journal ra.pdf}
}

@article{brembsPrestigiousScienceJournals2018,
  title = {Prestigious {{Science Journals Struggle}} to {{Reach Even Average Reliability}}},
  author = {Brembs, Bj{\"o}rn},
  year = {2018},
  journal = {Frontiers in Human Neuroscience},
  volume = {12},
  pages = {37},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2018.00037},
  urldate = {2021-11-14},
  abstract = {In which journal a scientist publishes is considered one of the most crucial factors determining their career. The underlying common assumption is that only the best scientists manage to publish in a highly selective tier of the most prestigious journals. However, data from several lines of evidence suggest that the methodological quality of scientific experiments does not increase with increasing rank of the journal. On the contrary, an accumulating body of evidence suggests the inverse: methodological quality and, consequently, reliability of published research works in several fields may be decreasing with increasing journal rank. The data supporting these conclusions circumvent confounding factors such as increased readership and scrutiny for these journals, focusing instead on quantifiable indicators of methodological soundness in the published literature, relying on, in part, semi-automated data extraction from often thousands of publications at a time. With the accumulating evidence over the last decade grew the realization that the very existence of scholarly journals, due to their inherent hierarchy, constitutes one of the major threats to publicly funded science: hiring, promoting and funding scientists who publish unreliable science eventually erodes public trust in science.},
  keywords = {Deception,Linear regression analysis,Medical humanities,Medicine and health sciences,Open science,Research integrity,Scientific misconduct,Surveys},
  file = {/Users/cristian/Zotero/storage/J9EQCJQM/Brembs - 2018 - Prestigious Science Journals Struggle to Reach Eve.pdf;/Users/cristian/Zotero/storage/X7XHCUB2/Gopalakrishna et al. - 2022 - Prevalence of questionable research practices, res.pdf;/Users/cristian/Zotero/storage/DMC67Z6H/article.html;/Users/cristian/Zotero/storage/IQNCCBRF/article.html}
}

@article{bridgettEffectsCuppingTherapy2018,
  title = {Effects of {{Cupping Therapy}} in {{Amateur}} and {{Professional Athletes}}: {{Systematic Review}} of {{Randomized Controlled Trials}}},
  author = {Bridgett, R. and Klose, P. and Duffield, R. and Mydock, S. and Lauche, R.},
  year = {2018},
  journal = {Journal of Alternative and Complementary Medicine},
  volume = {24},
  number = {3},
  pages = {208--219},
  publisher = {Mary Ann Liebert Inc.},
  doi = {10.1089/acm.2017.0191}
}

@misc{briggsWeUsedLLMs2025,
  title = {We Used {{LLMs}} to {{Track Methodological andSubstantive Publication Patterns}} in {{PoliticalScience}} and {{They Seem}} to Do a {{Pretty Good Job}}},
  author = {Briggs, Ryan and Mellon, Jonathan and {Arel-Bundock}, Vincent and Larson, Tim},
  year = {2025},
  month = feb,
  publisher = {Open Science Framework},
  urldate = {2025-02-11},
  abstract = {How has political science research evolved over the past decade, particularly in its methodological approaches, geographic scope, and research transparency practices? This paper analyses publication patterns in leading political science journals (AJPS and JOP) from 2010-2024. Examining over 2,600 articles, we document several important trends: a substantial rise in comparative politics research, persistent geographic concentration on Western democracies, increasing use of survey experiments, and growing adoption of open science practices. However, we also find concerning evidence of publication bias, with 98.8\% of abstracts reporting non-null results compared to only 16.9\% reporting null findings. To conduct this large-scale analysis, we develop and validate a Large Language Model (LLM) approach for extracting detailed information from academic articles, with accuracy comparable to skilled human coders while being significantly more efficient. By establishing the reliability of this method, we lay the groundwork for expanding our analysis to all political science journals, promising insights into the discipline's evolution. We conclude by outlining plans for this broader investigation and discussing its implications for understanding trends in political science research.},
  archiveprefix = {Open Science Framework},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/GWEWHELP/v7fe8.html}
}

@misc{briggsWeUsedLLMs2025,
  title = {We Used {{LLMs}} to {{Track Methodological andSubstantive Publication Patterns}} in {{PoliticalScience}} and {{They Seem}} to Do a {{Pretty Good Job}}},
  author = {Briggs, Ryan and Mellon, Jonathan and {Arel-Bundock}, Vincent and Larson, Tim},
  year = {2025},
  month = feb,
  publisher = {Open Science Framework},
  urldate = {2025-02-11},
  abstract = {How has political science research evolved over the past decade, particularly in its methodological approaches, geographic scope, and research transparency practices? This paper analyses publication patterns in leading political science journals (AJPS and JOP) from 2010-2024. Examining over 2,600 articles, we document several important trends: a substantial rise in comparative politics research, persistent geographic concentration on Western democracies, increasing use of survey experiments, and growing adoption of open science practices. However, we also find concerning evidence of publication bias, with 98.8\% of abstracts reporting non-null results compared to only 16.9\% reporting null findings. To conduct this large-scale analysis, we develop and validate a Large Language Model (LLM) approach for extracting detailed information from academic articles, with accuracy comparable to skilled human coders while being significantly more efficient. By establishing the reliability of this method, we lay the groundwork for expanding our analysis to all political science journals, promising insights into the discipline's evolution. We conclude by outlining plans for this broader investigation and discussing its implications for understanding trends in political science research.},
  archiveprefix = {Open Science Framework},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/EI8T7PNN/v7fe8.html}
}

@article{britezGlycerolProductivePerformance2021,
  title = {Glycerol on the Productive Performance and Characteristics of the Carcass and Meat of Goat: {{A}} Meta-Analysis},
  shorttitle = {Glicerol En El Comportamiento Productivo y Caracter{\'i}sticas de La Canal y Carne de Cabras: {{Un}} Metaan{\'a}lisis},
  author = {Britez, G.D.V. and {de Vargas Junior}, F.M. and Retore, M. and Leonardo, A.P. and Duarte, N.D.L. and Duarte, J.A.V. and Serafini, J.D.A. and {da Silva Oviedo}, M.O.},
  year = {2021},
  journal = {Revista de Investigaciones Veterinarias del Peru},
  volume = {32},
  number = {3},
  publisher = {Universidad Nacional Mayor de San Marcos},
  doi = {10.15381/RIVEP.V32I3.18357}
}

@article{brown_introduction_LMEM,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2025-09-01},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/876Z3AJ6/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{brownEffectsPriorCognitive2020,
  title = {Effects of {{Prior Cognitive Exertion}} on {{Physical Performance}}: {{A Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Effects of {{Prior Cognitive Exertion}} on {{Physical Performance}}},
  author = {Brown, Denver M. Y. and Graham, Jeffrey D. and Innes, Kira I. and Harris, Sheereen and Flemington, Ashley and Bray, Steven R.},
  year = {2020},
  month = mar,
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {497--529},
  issn = {1179-2035},
  doi = {10.1007/s40279-019-01204-8},
  urldate = {2022-12-21},
  abstract = {An emerging body of the literature in the past two decades has generally shown that prior cognitive exertion is associated with a subsequent decline in physical performance. Two parallel, but overlapping, bodies of literature (i.e., ego depletion, mental fatigue) have examined this question. However, research to date has not merged these separate lines of inquiry to assess the overall magnitude of this effect.},
  langid = {english}
}

@unpublished{brunnerHowReplicablePsychology,
  title = {How Replicable Is Psychology? {{A}} Comparison of Four Methods of Estimating Replicability on the Basis of Test Statistics in Original Studies},
  author = {Brunner, Jerry and Schimmack, Ulrich},
  annotation = {preprint},
  file = {/Users/cristian/Zotero/storage/5F7K7TVU/search.html;/Users/cristian/Zotero/storage/AWBNCKVL/search.html}
}

@article{brunsPCurvePHackingObservational2016,
  title = {P-{{Curve}} and p-{{Hacking}} in {{Observational Research}}},
  author = {Bruns, Stephan B. and Ioannidis, John P. A.},
  year = {2016},
  month = feb,
  journal = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {e0149144},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149144},
  urldate = {2022-03-22},
  abstract = {The p-curve, the distribution of statistically significant p-values of published studies, has been used to make inferences on the proportion of true effects and on the presence of p-hacking in the published literature. We analyze the p-curve for observational research in the presence of p-hacking. We show by means of simulations that even with minimal omitted-variable bias (e.g., unaccounted confounding) p-curves based on true effects and p-curves based on null-effects with p-hacking cannot be reliably distinguished. We also demonstrate this problem using as practical example the evaluation of the effect of malaria prevalence on economic growth between 1960 and 1996. These findings call recent studies into question that use the p-curve to infer that most published research findings are based on true effects in the medical literature and in a wide range of disciplines. p-values in observational research may need to be empirically calibrated to be interpretable with respect to the commonly used significance threshold of 0.05. Violations of randomization in experimental studies may also result in situations where the use of p-curves is similarly unreliable.},
  langid = {english},
  keywords = {Economic growth,Malaria,Medicine and health sciences,Monte Carlo method,Observational studies,Research design,Statistical distributions,Vibration},
  file = {/Users/cristian/Zotero/storage/CSG7TJ98/Bruns and Ioannidis - 2016 - p-Curve and p-Hacking in Observational Research.pdf;/Users/cristian/Zotero/storage/UD5FYJHA/article.html}
}

@article{brydgesEffectSizeGuidelines2019,
  title = {Effect {{Size Guidelines}}, {{Sample Size Calculations}}, and {{Statistical Power}} in {{Gerontology}}},
  author = {Brydges, Christopher R},
  year = {2019},
  month = aug,
  journal = {Innovation in Aging},
  volume = {3},
  number = {4},
  pages = {igz036},
  issn = {2399-5300},
  doi = {10.1093/geroni/igz036},
  urldate = {2023-10-16},
  abstract = {Researchers typically use Cohen's guidelines of Pearson's r = .10, .30, and .50, and Cohen's d = 0.20, 0.50, and 0.80 to interpret observed effect sizes as small, medium, or large, respectively. However, these guidelines were not based on quantitative estimates and are only recommended if field-specific estimates are unknown. This study investigated the distribution of effect sizes in both individual differences research and group differences research in gerontology to provide estimates of effect sizes in the field.Effect sizes (Pearson's r, Cohen's d, and Hedges' g) were extracted from meta-analyses published in 10 top-ranked gerontology journals. The 25th, 50th, and 75th percentile ranks were calculated for Pearson's r (individual differences) and Cohen's d or Hedges' g (group differences) values as indicators of small, medium, and large effects. A priori power analyses were conducted for sample size calculations given the observed effect size estimates.Effect sizes of Pearson's r = .12, .20, and .32 for individual differences research and Hedges' g = 0.16, 0.38, and 0.76 for group differences research were interpreted as small, medium, and large effects in gerontology.Cohen's guidelines appear to overestimate effect sizes in gerontology. Researchers are encouraged to use Pearson's r = .10, .20, and .30, and Cohen's d or Hedges' g = 0.15, 0.40, and 0.75 to interpret small, medium, and large effects in gerontology, and recruit larger samples.},
  file = {/Users/cristian/Zotero/storage/SP24J5X9/Brydges - 2019 - Effect Size Guidelines, Sample Size Calculations, .pdf;/Users/cristian/Zotero/storage/7W4DHNX4/5560156.html}
}

@article{brysbaert_guidelines,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables},
  shorttitle = {How Many Participants Do We Have to Include in Properly Powered Experiments?},
  author = {Brysbaert, Marc},
  year = {2019},
  month = jul,
  journal = {Journal of Cognition},
  volume = {2},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.72},
  urldate = {2024-10-04},
  abstract = {The Journal of Cognition, the official journal of the European Society for Cognitive Psychology, publishes reviews, empirical articles (including registered reports), data reports, stimulus development reports, comments, and methodological notes relevant to all areas of cognitive psychology, including attention, memory, perception, psycholinguistics, and reasoning. We also publish cross-disciplinary research if we judge that it has clear implications for development of cognitive psychological theories. As a signatory of the Center for Open Science's Transparency and Openness Promotion guidelines, we value methodological rigour and transparent scientific practices. We welcome submissions from scholars working anywhere in the world.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/9QIL2DQW/Brysbaert - 2019 - How many participants do we have to include in pro.pdf}
}

@article{brysbaert_how_to_LMEM,
  title = {How to {{Run Linear Mixed Effects Analysis}} for {{Pairwise Comparisons}}? {{A Tutorial}} and a {{Proposal}} for the {{Calculation}} of {{Standardized Effect Sizes}}},
  shorttitle = {How to {{Run Linear Mixed Effects Analysis}} for {{Pairwise Comparisons}}?},
  author = {Brysbaert, Marc and Debeer, Dries},
  year = {2025},
  month = jan,
  journal = {Journal of Cognition},
  volume = {8},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.409},
  urldate = {2025-09-01},
  abstract = {This tutorial provides guidelines for conducting linear mixed effects (LME) analyses for simple designs, aimed at researchers familiar with t-tests, analysis of variance (ANOVA) and linear regression. First, we compare LME analyses with traditional methods when participants are the only source of random variation. We show that LME analysis is more interesting as soon as you have more than one observation per participant per condition. The second section discusses studies where both participants and stimuli are used as sources of random variation, ensuring robust generalization beyond the specific stimuli tested. In our search for standardized effect sizes, we saw that partial eta squared is even less informative for LME than for ANOVA. We present eta squared within as an alternative, to be used in combination with the traditional measure eta squared (also in ANOVA). To facilitate implementation, we analyze toy datasets with R and jamovi. This tutorial gives researchers a good foundation for LME analyses of simple 2 \&times; 2 designs and paves the way for tackling more complicated designs.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/HW98G42G/Brysbaert and Debeer - 2025 - How to Run Linear Mixed Effects Analysis for Pairw.pdf}
}

@article{brysbaert_LMEM,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2025-09-01},
  abstract = {In psychology, attempts to replicate published findings are less successful than expected. For properly powered studies replication rate should be around 80\%, whereas in practice less than 40\% of the studies selected from different areas of psychology can be replicated. Researchers in cognitive psychology are hindered in estimating the power of their studies, because the designs they use present a sample of stimulus materials to a sample of participants, a situation not covered by most power formulas. To remedy the situation, we review the literature related to the topic and introduce recent software packages, which we apply to the data of two masked priming studies with high power. We checked how we could estimate the power of each study and how much they could be reduced to remain powerful enough. On the basis of this analysis, we recommend that a properly powered reaction time experiment with repeated measures has at least 1,600 word observations per condition (e.g., 40 participants, 40 stimuli). This is considerably more than current practice. We also show that researchers must include the number of observations in meta-analyses because the effect sizes currently reported depend on the number of stimuli presented to the participants. Our analyses can easily be applied to new datasets gathered.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/YP42NMSL/Brysbaert and Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf}
}

@article{buccellettiAcuteDecompensatedHeart2006,
  title = {Acute Decompensated Heart Failure: {{Formulating}} an Evidence-Based Approach to Diagnosis and Treatment ({{Part II}})},
  author = {Buccelletti, F. and Hermann, L.},
  year = {2006},
  journal = {Mount Sinai Journal of Medicine},
  volume = {73},
  number = {2},
  pages = {516--527}
}

@article{buchheitNumbersWillLove2016,
  title = {The {{Numbers Will Love You Back}} in {{Return}}---{{I Promise}}},
  author = {Buchheit, Martin},
  year = {2016},
  month = may,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {11},
  number = {4},
  pages = {551--554},
  publisher = {Human Kinetics, Inc.},
  issn = {1555-0273, 1555-0265},
  doi = {10.1123/ijspp.2016-0214},
  urldate = {2023-10-19},
  abstract = {The first sport-science-oriented and comprehensive paper on magnitude-based inferences (MBI) was published 10 y ago in the first issue of this journal. While debate continues, MBI is today well established in sport science and in other fields, particularly clinical medicine, where practical/clinical significance often takes priority over statistical significance. In this commentary, some reasons why both academics and sport scientists should abandon null-hypothesis significance testing and embrace MBI are reviewed. Apparent limitations and future areas of research are also discussed. The following arguments are presented: P values and, in turn, study conclusions are sample-size dependent, irrespective of the size of the effect; significance does not inform on magnitude of effects, yet magnitude is what matters the most; MBI allows authors to be honest with their sample size and better acknowledge trivial effects; the examination of magnitudes per se helps provide better research questions; MBI can be applied to assess changes in individuals; MBI improves data visualization; and MBI is supported by spreadsheets freely available on the Internet. Finally, recommendations to define the smallest important effect and improve the presentation of standardized effects are presented.},
  chapter = {International Journal of Sports Physiology and Performance},
  langid = {english}
}

@misc{bucss_software,
  title = {{{BUCSS}}: {{Bias}} and {{Uncertainty Corrected Sample Size}}},
  shorttitle = {{{BUCSS}}},
  author = {Anderson, Samantha F. and Kelley, Ken},
  year = {2016},
  month = dec,
  urldate = {2024-07-26},
  abstract = {Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and uncertainty when planning sample sizes in a future study from an original study. See Anderson, Kelley, \& Maxwell (2017; Psychological Science, 28, 1547-1562).},
  howpublished = {Comprehensive R Archive Network},
  file = {/Users/cristian/Zotero/storage/R33T3F2M/Samantha F. Anderson , Ken Kelley  - 2016 - BUCSS Bias and Uncertainty Corrected Sample Size.pdf}
}

@article{burakowskaEffectHeattreatedCanola2020,
  title = {Effect of Heat-Treated Canola Meal and Glycerol Inclusion on Performance and Gastrointestinal Development of {{Holstein}} Calves},
  author = {Burakowska, K. and G{\'o}rka, P. and {Kent-Dennis}, C. and Kowalski, Z.M. and Laarveld, B. and Penner, G.B.},
  year = {2020},
  journal = {Journal of Dairy Science},
  volume = {103},
  number = {9},
  pages = {7998--8019},
  publisher = {Elsevier Inc.},
  doi = {10.3168/jds.2019-18133}
}

@article{burkeMethodologiesInvestigatingPerformance2018,
  title = {Methodologies for {{Investigating Performance Changes With Supplement Use}}},
  author = {Burke, Louise M. and Peeling, Peter},
  year = {2018},
  month = mar,
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {28},
  number = {2},
  pages = {159--169},
  issn = {1543-2742},
  doi = {10.1123/ijsnem.2017-0325},
  abstract = {Many expert sporting bodies now support a pragmatic acceptance of the use of performance supplements which have passed a risk:benefit analysis of being safe, effective, and permitted for use, while also being appropriate to the athlete's age and maturation in their sport. However, gaining evidence of the performance benefits of these supplements is a process challenged by the scarcity of research in relation to the number of available products, and the limitations of the poor quality of some studies. While meta-analyses and systematic reviews can help to provide information about the general use of performance supplements, the controlled scientific trial provides the basis on which these reviews are undertaken, as well as an opportunity to address more specific questions about supplement applications. Guidelines for the design of studies include the choice of well-trained athletes who are familiarized with performance tasks that have been chosen on their basis of their known reliability and validity. Supplement protocols should be chosen to maximize the likely benefits, and researchers should also make efforts to control confounding variables, while keeping conditions similar to real-life practices. Performance changes should be interpreted in light of what is meaningful to the outcomes of sporting competition. Issues that have been poorly addressed to date include the use of several supplements in combination and the use of the same supplement over successive events, both within a single, and across multiple competition days. Strategies to isolate and explain the variability of benefits to individuals are also a topic for future investigation.},
  langid = {english},
  pmid = {29468949},
  keywords = {Athletes,Athletic Performance,controlled trials,Dietary Supplements,Humans,Performance-Enhancing Substances,reliability of performance,Sports Nutritional Physiological Phenomena,validity},
  file = {/Users/cristian/Zotero/storage/TKBJXPFX/Burke and Peeling - 2018 - Methodologies for Investigating Performance Change.pdf}
}

@article{burtEffectsExerciseinducedMuscle2023,
  title = {The Effects of Exercise-Induced Muscle Damage on Varying Intensities of Endurance Running Performance: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Les Effets Des L{\'e}sions Musculaires Induites Par l'exercice Sur Les Intensit{\'e}s Variables de La Performance de Course d'endurance : Une Revue Syst{\'e}matique et Une M{\'e}ta-Analyse},
  author = {Burt, D. and Doma, K. and Connor, J.},
  year = {2023},
  journal = {Science and Sports},
  publisher = {Elsevier Masson s.r.l.},
  doi = {10.1016/j.scispo.2022.04.003}
}

@article{butlerGrayZoneQuestionable2017,
  title = {The {{Gray Zone}}: {{Questionable Research Practices}} in the {{Business School}}},
  shorttitle = {The {{Gray Zone}}},
  author = {Butler, Nick and Delaney, Helen and Spoelstra, Sverre},
  year = {2017},
  month = feb,
  journal = {Academy of Management Learning \& Education},
  volume = {16},
  number = {1},
  pages = {94--109},
  publisher = {Academy of Management},
  issn = {1537-260X},
  doi = {10.5465/amle.2015.0201},
  urldate = {2021-03-12},
  abstract = {In recent years, the awareness of academic misconduct has increased due to high-profile scandals involving prominent researchers and a spike in journal retractions. But such examples of fabrication, falsification, and plagiarism (FFP) serve to obscure the less flagrant, more subtle cases of possible misconduct: what some have called ``questionable research practices'' (QRPs). Where FFP is seen as inherently negative, QRPs fall into an ethical ``gray zone'' between permissible and impermissible. We draw on semistructured interviews with business school scholars to explore the occurrence of QRPs. Prevalent QRPs include playing with numbers, playing with models, and playing with hypotheses. Scholars explain the existence of QRPs in three ways: the inadequate training of researchers, the pressures and incentives to publish in certain outlets, and the demands and expectations of journal editors and reviewers. We argue that a paradox is at work here: To live up to the positivist image of ``pure science'' that appears in academic journals, researchers may find themselves---ironically---transgressing this very ideal. Ultimately, this challenges the individualistic account of academic misconduct by drawing attention to the role played by institutional actors, such as academic journals, in encouraging forms of QRPs.},
  file = {/Users/cristian/Zotero/storage/GYHBIFNY/amle.2015.html}
}

@article{buttnerAreQuestionableResearch2020,
  title = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine? {{The}} Proportion of Supported Hypotheses Is Implausibly High},
  author = {B{\"u}ttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
  year = {2020},
  journal = {British journal of sports medicine},
  volume = {54},
  number = {22},
  pages = {1365--1371},
  publisher = {{BMJ Publishing Group Ltd and British Association of Sport and Exercise Medicine}},
  chapter = {Education review},
  isbn = {0306-3674},
  pmid = {32699001},
  keywords = {education,methodological,research,sport,statistics}
}

@article{buttonMinimalClinicallyImportant2015,
  title = {Minimal Clinically Important Difference on the {{Beck Depression Inventory}} - {{II}} According to the Patient's Perspective},
  author = {Button, K. S. and Kounali, D. and Thomas, L. and Wiles, N. J. and Peters, T. J. and Welton, N. J. and Ades, A. E. and Lewis, G.},
  year = {2015},
  month = nov,
  journal = {Psychological Medicine},
  volume = {45},
  number = {15},
  pages = {3269--3279},
  publisher = {Cambridge University Press},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291715001270},
  urldate = {2023-10-18},
  abstract = {BackgroundThe Beck Depression Inventory, 2nd edition (BDI-II) is widely used in research on depression. However, the minimal clinically important difference (MCID) is unknown. MCID can be estimated in several ways. Here we take a patient-centred approach, anchoring the change on the BDI-II to the patient's global report of improvement.MethodWe used data collected (n = 1039) from three randomized controlled trials for the management of depression. Improvement on a `global rating of change' question was compared with changes in BDI-II scores using general linear modelling to explore baseline dependency, assessing whether MCID is best measured in absolute terms (i.e. difference) or as percent reduction in scores from baseline (i.e. ratio), and receiver operator characteristics (ROC) to estimate MCID according to the optimal threshold above which individuals report feeling `better'.ResultsImprovement in BDI-II scores associated with reporting feeling `better' depended on initial depression severity, and statistical modelling indicated that MCID is best measured on a ratio scale as a percentage reduction of score. We estimated a MCID of a 17.5\% reduction in scores from baseline from ROC analyses. The corresponding estimate for individuals with longer duration depression who had not responded to antidepressants was higher at 32\%.ConclusionsMCID on the BDI-II is dependent on baseline severity, is best measured on a ratio scale, and the MCID for treatment-resistant depression is larger than that for more typical depression. This has important implications for clinical trials and practice.},
  langid = {english},
  keywords = {2nd edition (BDI-II),Beck Depression Inventory,depression,minimal clinically important difference,outcome assessment,primary care},
  file = {/Users/cristian/Zotero/storage/XP257R7R/Button et al. - 2015 - Minimal clinically important difference on the Bec.pdf}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/5QQNF2JE/Button et al. - 2013 - Power failure why small sample size undermines th.pdf;/Users/cristian/Zotero/storage/QZXNHZZX/nrn3475.html}
}

@article{caiDiagnosticAccuracyRed2019,
  title = {Diagnostic Accuracy of Red Blood Cell Distribution Width to Platelet Ratio for Predicting Staging Liver Fibrosis in Chronic Liver Disease Patients : {{A}} Systematic Review and Meta-Analysis},
  author = {Cai, Y. and Liu, D. and Cui, J. and Sha, Y. and Zhou, H. and Tang, N. and Wang, N. and Huang, A. and Xia, J.},
  year = {2019},
  journal = {Medicine (United States)},
  volume = {98},
  number = {14},
  publisher = {{Lippincott Williams and Wilkins}},
  doi = {10.1097/MD.0000000000015096}
}

@article{caldwellCaseDefaultEffect2020,
  ids = {caldwellCaseDefaultEffect2020a},
  title = {A Case against Default Effect Sizes in Sport and Exercise Science},
  author = {Caldwell, Aaron and Vigotsky, Andrew D.},
  year = {2020},
  month = nov,
  journal = {PeerJ},
  volume = {8},
  pages = {e10314},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.10314},
  urldate = {2022-12-07},
  abstract = {Recent discussions in the sport and exercise science community have focused on the appropriate use and reporting of effect sizes. Sport and exercise scientists often analyze repeated-measures data, from which mean differences are reported. To aid the interpretation of these data, standardized mean differences (SMD) are commonly reported as a description of effect size. In this manuscript, we hope to alleviate some confusion. First, we provide a philosophical framework for conceptualizing SMDs; that is, by dichotomizing them into two groups: magnitude-based and signal-to-noise SMDs. Second, we describe the statistical properties of SMDs and their implications. Finally, we provide high-level recommendations for how sport and exercise scientists can thoughtfully report raw effect sizes, SMDs, or other effect sizes for their own studies. This conceptual framework provides sport and exercise scientists with the background necessary to make and justify their choice of an SMD.},
  langid = {english},
  pmcid = {PMC7646309},
  pmid = {33194448},
  file = {/Users/cristian/Zotero/storage/2KNAJWKV/Caldwell and Vigotsky - 2020 - A case against default effect sizes in sport and e.pdf;/Users/cristian/Zotero/storage/ENH46UW7/10314.html}
}

@article{caldwellMovingSportExercise2020,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, R{\'e}mi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
  year = {2020},
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {449--459},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\${\textbackslash}chi \$\${$\chi$}iv: https://osf.io/preprints/sportrxiv/fxe7a/.},
  langid = {english}
}

@book{caldwellPowerAnalysisSuperpower2022,
  title = {Power {{Analysis}} with {{Superpower}}},
  author = {Caldwell, Aaron R. and Lakens, Dani{\"e}l and {Parlett-Pelleriti}, Chelsea M. and Prochilo, Guy and Aust, Frederik},
  year = {2022},
  urldate = {2025-07-02},
  abstract = {This is a book describing the capabilities of the Superpower R package.},
  file = {/Users/cristian/Zotero/storage/9PMWI68V/index.html}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  doi = {10.1126/science.aaf0918},
  abstract = {By several metrics, economics experiments do replicate, although not as often as predicted. By several metrics, economics experiments do replicate, although not as often as predicted.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/G9PB7XCF/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/cristian/Zotero/storage/HHBAMU5R/tab-article-info.html}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1--15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516--36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/A3HWH6SJ/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{caoGemcitabineS1Hopeful2015,
  title = {Gemcitabine plus {{S-1}}: {{A}} Hopeful Frontline Treatment for {{Asian}} Patients with Unresectable Advanced Pancreatic Cancer},
  author = {Cao, C. and Kuang, M. and Xu, W. and Zhang, X. and Chen, J. and Tang, C.},
  year = {2015},
  journal = {Japanese Journal of Clinical Oncology},
  volume = {45},
  number = {12},
  pages = {1122--1130},
  publisher = {Oxford University Press},
  doi = {10.1093/jjco/hyv141}
}

@article{carbineQuantifyingPresenceEvidential2019,
  title = {Quantifying the Presence of Evidential Value and Selective Reporting in Food-Related Inhibitory Control Training: A p-Curve Analysis},
  shorttitle = {Quantifying the Presence of Evidential Value and Selective Reporting in Food-Related Inhibitory Control Training},
  author = {Carbine, Kaylie A. and Larson, Michael J.},
  year = {2019},
  month = sep,
  journal = {Health Psychology Review},
  volume = {13},
  number = {3},
  pages = {318--343},
  issn = {1743-7202},
  doi = {10.1080/17437199.2019.1622144},
  abstract = {Meta-analyses suggest inhibitory control training (ICT) may be effective for reducing food intake. However, psychological research has come under scrutiny for lack of reliability. Selective reporting (only reporting significant results) is one factor contributing to the lack of reliability in published research. Therefore, estimates of food-related ICT effects~may be inaccurate. We conducted p-curve analyses to assess the presence of selective reporting, evidential value, average effect size, and power in the food-related ICT literature. Extracted p-values were selected from articles included in food-related ICT meta-analyses and an updated literature search. Four p-curve analyses resulted in 'U'-shaped distributions, suggesting evidence for both a true underlying effect and selective reporting in the food-related ICT literature. Robust analyses suggested the evidence for an underlying effect was primarily driven by the smallest p-value. The average effect size from included studies was small (d\,=\,0.04 to 0.25). Average power to detect this effect was also small (7\% to 18\%). Results suggest no clear support for or against a true effect for food-related ICT. Low average effect size and power across studies suggests estimated effects are likely inflated in published literature. Higher-powered, pre-registered, longitudinal food-related ICT studies are needed to test for the presence and magnitude of ICT effects.},
  langid = {english},
  pmid = {31122177},
  keywords = {-curve,{Inhibition, Psychological},{Outcome Assessment, Health Care},Behavior Therapy,evidential value,Feeding Behavior,food,Health Behavior,Humans,inhibitory control training,Publication Bias,selective reporting,Self-Control}
}

@article{carlsonEffectsTrainingLoad2022,
  title = {The {{Effects}} of {{Training Load During Dietary Intervention Upon Fat Loss}}: {{A Randomized Crossover Trial}}},
  shorttitle = {The {{Effects}} of {{Training Load During Dietary Intervention Upon Fat Loss}}},
  author = {Carlson, Luke and Gschneidner, David and Steele, James and Fisher, James P.},
  year = {2022},
  month = aug,
  journal = {Research Quarterly for Exercise and Sport},
  pages = {1--11},
  issn = {2168-3824},
  doi = {10.1080/02701367.2022.2097625},
  abstract = {Purpose: To date no studies have compared resistance training loading strategies combined with dietary intervention for fat loss. Methods: Thus, we performed a randomised crossover design comparing four weeks of heavier- (HL; {\textasciitilde}80\% 1RM) and lighter-load (LL; {\textasciitilde}60\% 1RM) resistance training, combined with calorie restriction and dietary guidance, including resistance trained participants (n=130; males=49, females=81). Both conditions performed low-volume, (single set of 9 exercises, 2x/week) effort matched (to momentary failure), but non-work-matched protocols. Testing was completed pre- and post-each intervention. Fat mass (kg) was the primary outcome, and a smallest effect size of interest (SESOI) was established at 3.3\% loss of baseline bodyweight. Body fat percentage, lean mass, and strength (7-10RM) for chest press, leg press, and pull-down exercises were also measured. An 8-week washout period of traditional training with normal calorie interspersed each intervention. Results: Both interventions showed small statistically equivalent (within the SESOI) reductions in fat mass (HL: -0.67 kg [95\%CI -0.91 to 0.42]; LL: -0.55 kg [95\%CI -0.80 to -0.31]) which were also equivalent between conditions (HL - LL: -0.113 kg [95\%CI -0.437 kg to 0.212 kg]). Changes in body fat percentage and lean mass were also minimal. Strength increases were small, similar between conditions, and within a previously determined SESOI for the population included (10.1\%). Conclusions: Fat loss reductions are not impacted by resistance training load; both HL and LL produce similar, yet small, changes to body composition over a 4-week intervention. However, the maintenance of both lean mass and strength highlights the value of resistance training during dietary intervention.},
  langid = {english},
  pmid = {35998256},
  keywords = {Body composition,muscle size,resistance training,strength,supervision},
  file = {/Users/cristian/Zotero/storage/399VFDEA/Carlson et al. - 2022 - The Effects of Training Load During Dietary Interv.pdf}
}

@article{carrollRecoveryCentralPeripheral2017,
  title = {Recovery of Central and Peripheral Neuromuscular Fatigue after Exercise},
  author = {Carroll, T. J. and Taylor, J. L. and Gandevia, S. C.},
  year = {2017},
  month = may,
  journal = {Journal of Applied Physiology},
  volume = {122},
  number = {5},
  pages = {1068--1076},
  issn = {8750-7587, 1522-1601},
  doi = {10.1152/japplphysiol.00775.2016},
  urldate = {2023-06-20},
  abstract = {Sustained physical exercise leads to a reduced capacity to produce voluntary force that typically outlasts the exercise bout. This ``fatigue'' can be due both to impaired muscle function, termed ``peripheral fatigue,'' and a reduction in the capacity of the central nervous system to activate muscles, termed ``central fatigue.'' In this review we consider the factors that determine the recovery of voluntary force generating capacity after various types of exercise. After brief, high-intensity exercise there is typically a rapid restitution of force that is due to recovery of central fatigue (typically within 2 min) and aspects of peripheral fatigue associated with excitation-contraction coupling and reperfusion of muscles (typically within 3--5 min). Complete recovery of muscle function may be incomplete for some hours, however, due to prolonged impairment in intracellular Ca               2+               release or sensitivity. After low-intensity exercise of long duration, voluntary force typically shows rapid, partial, recovery within the first few minutes, due largely to recovery of the central, neural component. However, the ability to voluntarily activate muscles may not recover completely within 30 min after exercise. Recovery of peripheral fatigue contributes comparatively little to the fast initial force restitution and is typically incomplete for at least 20--30 min. Work remains to identify what factors underlie the prolonged central fatigue that usually accompanies long-duration single joint and locomotor exercise and to document how the time course of neuromuscular recovery is affected by exercise intensity and duration in locomotor exercise. Such information could be useful to enhance rehabilitation and sports performance.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/K67WZYI2/Carroll et al. - 2017 - Recovery of central and peripheral neuromuscular f.pdf}
}

@article{carsoniiiCardiacSafetyClinical2005,
  title = {Cardiac Safety in Clinical Trials of Phosphodiesterase 5 Inhibitors},
  author = {Carson III, C.C.},
  year = {2005},
  journal = {American Journal of Cardiology},
  volume = {96},
  number = {12 SUPPL. 2},
  pages = {37--41},
  publisher = {Elsevier Inc.},
  doi = {10.1016/j.amjcard.2005.07.010}
}

@article{carterCorrectingBiasPsychology2019,
  title = {Correcting for Bias in Psychology: {{A}} Comparison of Meta-Analytic Methods},
  shorttitle = {Correcting for Bias in Psychology},
  author = {Carter, Evan C. and Sch{\"o}nbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  year = {2019},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  publisher = {Sage Publications},
  address = {US},
  issn = {2515-2467},
  doi = {10.1177/2515245919847196},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses---that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Biased Sampling,Experimental Replication,Experimentation,Experimenter Bias,Open Data,Psychology,Sample Size,Simulation},
  file = {/Users/cristian/Zotero/storage/DGQ6BXGM/Carter et al. - 2019 - Correcting for bias in psychology A comparison of.pdf;/Users/cristian/Zotero/storage/IG9IGMZW/2019-33828-002.html}
}

@article{carterPublicationBiasLimited2014,
  title = {Publication Bias and the Limited Strength Model of Self-Control: Has the Evidence for Ego Depletion Been Overestimated?},
  author = {Carter, Evan C. and McCullough, Michael E.},
  year = {2014},
  journal = {Frontiers in Psychology},
  volume = {5},
  doi = {10.3389/fpsyg.2014.00823},
  abstract = {Few models of self-control have generated as much scientific interest as has the limited strength model. One of the entailments of this model, the depletion effect, is the expectation that acts of self-control will be less effective when they follow prior acts of self-control. Results from a previous meta-analysis concluded that the depletion effect is robust and medium in magnitude (d = 0.62). However, when we applied methods for estimating and correcting for small-study effects (such as publication bias) to the data from this previous meta-analysis effort, we found very strong signals of publication bias, along with an indication that the depletion effect is actually no different from zero. We conclude that until greater certainty about the size of the depletion effect can be established, circumspection about the existence of this phenomenon is warranted, and that rather than elaborating on the model, research efforts should focus on establishing whether the basic effect exists. We argue that the evidence for the depletion effect is a useful case study for illustrating the dangers of small-study effects as well as some of the possible tools for mitigating their influence in psychological science.},
  file = {/Users/cristian/Zotero/storage/A5GTB4HE/Carter and McCullough - 2014 - Publication bias and the limited strength model of.pdf}
}

@article{carvalhoCognitiveDysfunctionDepression2014,
  title = {Cognitive Dysfunction in Depression -- {{Pathophysiology}} and Novel Targets},
  author = {Carvalho, A.F. and Miskowiak, K.K. and Hyphantis, T.N. and K{\"o}hler, C.A. and Alves, G.S. and Bortolato, B. and Sales, P.M.G. and {Machado-Vieira}, R. and Berk, M. and McIntyre, R.S.},
  year = {2014},
  journal = {CNS and Neurological Disorders - Drug Targets},
  volume = {13},
  number = {10},
  pages = {1819--1835},
  publisher = {Bentham Science Publishers},
  doi = {10.2174/1871527313666141130203627}
}

@article{casadevallCausesPersistenceImpact2014,
  title = {Causes for the {{Persistence}} of {{Impact Factor Mania}}},
  author = {Casadevall, Arturo and Fang, Ferric C.},
  year = {2014},
  month = mar,
  journal = {mBio},
  publisher = {American Society for Microbiology},
  doi = {10.1128/mBio.00064-14},
  urldate = {2022-01-11},
  abstract = {Science and scientists are currently afflicted by an epidemic of mania manifested by associating the value of research with the journal where the work is published rather than the content of the work itself. The mania is causing profound distortions in ...},
  copyright = {Copyright {\copyright} 2014 Casadevall and Fang.},
  langid = {english},
  annotation = {1752 N St., N.W., Washington, DC},
  file = {/Users/cristian/Zotero/storage/CNC34TUF/Casadevall and Fang - 2014 - Causes for the Persistence of Impact Factor Mania.pdf;/Users/cristian/Zotero/storage/R9NYCL73/mBio.html}
}

@article{casadevallRigorousScienceHowTo2016,
  title = {Rigorous {{Science}}: A {{How-To Guide}}},
  shorttitle = {Rigorous {{Science}}},
  author = {Casadevall, Arturo and Fang, Ferric C.},
  year = {2016},
  month = dec,
  journal = {mBio},
  volume = {7},
  number = {6},
  publisher = {American Society for Microbiology},
  issn = {2150-7511},
  doi = {10.1128/mBio.01902-16},
  urldate = {2021-03-15},
  abstract = {Proposals to improve the reproducibility of biomedical research have emphasized scientific rigor. Although the word ``rigor'' is widely used, there has been little specific discussion as to what it means and how it can be achieved. We suggest that scientific rigor combines elements of mathematics, logic, philosophy, and ethics. We propose a framework for rigor that includes redundant experimental design, sound statistical analysis, recognition of error, avoidance of logical fallacies, and intellectual honesty. These elements lead to five actionable recommendations for research education.},
  chapter = {Editorial},
  copyright = {Copyright {\copyright} 2016 Casadevall and Fang.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license.},
  langid = {english},
  pmid = {27834205},
  file = {/Users/cristian/Zotero/storage/5ETEATNJ/Casadevall and Fang - 2016 - Rigorous Science a How-To Guide.pdf;/Users/cristian/Zotero/storage/T57EPMBI/e01902-16.html}
}

@article{caseScholarshipSociology1928,
  title = {Scholarship in Sociology},
  author = {Case, Clarence Marsh},
  year = {1928},
  journal = {Sociology and Social Research},
  volume = {12},
  number = {4}
}

@article{chambersPresentFutureRegistered2021,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = {2021},
  month = nov,
  journal = {Nature Human Behaviour},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2022-01-13},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Culture;Publishing\\
Subject\_term\_id: culture;publishing},
  file = {/Users/cristian/Zotero/storage/LXLPGUE4/Chambers and Tzavella - 2021 - The past, present and future of Registered Reports.pdf;/Users/cristian/Zotero/storage/HQ5LDFGB/s41562-021-01193-7.html}
}

@article{champelyPwrBasicFunctions2017,
  title = {Pwr: {{Basic}} Functions for Power Analysis},
  author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and De Rosario, Helios},
  year = {2017}
}

@article{chanEmpiricalEvidenceSelective2004,
  title = {Empirical Evidence for Selective Reporting of Outcomes in Randomized Trials: Comparison of Protocols to Published Articles.},
  author = {Chan, An-Wen and Hr{\'o}bjartsson, Asbj{\o}rn and Haahr, Mette T. and G{\o}tzsche, Peter C. and Altman, Douglas G.},
  year = {2004},
  month = may,
  journal = {JAMA},
  volume = {291},
  number = {20},
  pages = {2457--2465},
  address = {United States},
  issn = {1538-3598 0098-7484},
  doi = {10.1001/jama.291.20.2457},
  abstract = {CONTEXT: Selective reporting of outcomes within published studies based on the nature or direction of their results has been widely suspected, but direct  evidence of such bias is currently limited to case reports. OBJECTIVE: To study  empirically the extent and nature of outcome reporting bias in a cohort of  randomized trials. DESIGN: Cohort study using protocols and published reports of  randomized trials approved by the Scientific-Ethical Committees for Copenhagen  and Frederiksberg, Denmark, in 1994-1995. The number and characteristics of  reported and unreported trial outcomes were recorded from protocols, journal  articles, and a survey of trialists. An outcome was considered incompletely  reported if insufficient data were presented in the published articles for  meta-analysis. Odds ratios relating the completeness of outcome reporting to  statistical significance were calculated for each trial and then pooled to  provide an overall estimate of bias. Protocols and published articles were also  compared to identify discrepancies in primary outcomes. MAIN OUTCOME MEASURES:  Completeness of reporting of efficacy and harm outcomes and of statistically  significant vs nonsignificant outcomes; consistency between primary outcomes  defined in the most recent protocols and those defined in published articles.  RESULTS: One hundred two trials with 122 published journal articles and 3736  outcomes were identified. Overall, 50\% of efficacy and 65\% of harm outcomes per  trial were incompletely reported. Statistically significant outcomes had a higher  odds of being fully reported compared with nonsignificant outcomes for both  efficacy (pooled odds ratio, 2.4; 95\% confidence interval [CI], 1.4-4.0) and harm  (pooled odds ratio, 4.7; 95\% CI, 1.8-12.0) data. In comparing published articles  with protocols, 62\% of trials had at least 1 primary outcome that was changed,  introduced, or omitted. Eighty-six percent of survey responders (42/49) denied  the existence of unreported outcomes despite clear evidence to the contrary.  CONCLUSIONS: The reporting of trial outcomes is not only frequently incomplete  but also biased and inconsistent with protocols. Published articles, as well as  reviews that incorporate them, may therefore be unreliable and overestimate the  benefits of an intervention. To ensure transparency, planned trials should be  registered and protocols should be made publicly available prior to trial  completion.},
  langid = {english},
  pmid = {15161896},
  keywords = {*Publication Bias,*Randomized Controlled Trials as Topic/standards,*Treatment Outcome,Biomedical and Behavioral Research,Clinical Protocols,Empirical Approach,Meta-Analysis as Topic,Registries}
}

@article{chanIdentificationImmuneNeuroendocrineBiomarker2016,
  title = {Identification of an {{Immune-Neuroendocrine Biomarker Panel}} for {{Detection}} of {{Depression}}: {{A Joint Effects Statistical Approach}}},
  author = {Chan, M.K. and Cooper, J.D. and Bot, M. and Steiner, J. and Penninx, B.W.J.H. and Bahn, S.},
  year = {2016},
  journal = {Neuroendocrinology},
  volume = {103},
  number = {6},
  pages = {693--710},
  publisher = {S. Karger AG},
  doi = {10.1159/000442208}
}

@misc{ChapterBetweenStudyHeterogeneity,
  title = {Chapter 5 {{Between-Study Heterogeneity}} {\textbar} {{Doing Meta-Analysis}} in {{R}}},
  urldate = {2024-06-11},
  howpublished = {https://bookdown.org/MathiasHarrer/Doing\_Meta\_Analysis\_in\_R/heterogeneity.html?q=outli\#heterogeneity},
  file = {/Users/cristian/Zotero/storage/MJN8WYZW/heterogeneity.html}
}

@article{chavalariasEvolutionReportingValues2016,
  title = {Evolution of {{Reporting P Values}} in the {{Biomedical Literature}}, 1990-2015},
  author = {Chavalarias, David and Wallach, Joshua David and Li, Alvin Ho Ting and Ioannidis, John P. A.},
  year = {2016},
  journal = {JAMA},
  volume = {315},
  number = {11},
  pages = {1141--1148},
  doi = {10.1001/jama.2016.1952},
  abstract = {The use and misuse of P values has generated extensive debates. OBJECTIVE: To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values. DESIGN: Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text. MAIN OUTCOMES AND MEASURES: P values reported. RESULTS: Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3\% in 1990 to 15.6\% in 2014. In 2014, P values were reported in 33.0\% of abstracts from the 151 core clinical journals (n\,=\,29,725 abstracts), 35.7\% of meta-analyses (n\,=\,5620), 38.9\% of clinical trials (n\,=\,4624), 54.8\% of randomized controlled trials (n\,=\,13,544), and 2.4\% of reviews (n\,=\,71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the "best" (most statistically significant) reported P values were modestly smaller and the "worst" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96\% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7\% (125/796 [95\% CI, 13.2\%-18.4\%]) of abstracts, confidence intervals in 2.3\% (18/796 [95\% CI, 1.3\%-3.6\%]), Bayes factors in 0\% (0/796 [95\% CI, 0\%-0.5\%]), effect sizes in 13.9\% (111/796 [95\% CI, 11.6\%-16.5\%]), other information that could lead to estimation of P values in 12.4\% (99/796 [95\% CI, 10.2\%-14.9\%]), and qualitative statements about significance in 18.1\% (181/1000 [95\% CI, 15.8\%-20.6\%]); only 1.8\% (14/796 [95\% CI, 1.0\%-2.9\%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome. CONCLUSIONS AND RELEVANCE: In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990-2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.},
  langid = {english},
  pmid = {26978209},
  keywords = {{Models, Statistical},Data Mining,MEDLINE,Probability},
  file = {/Users/cristian/Zotero/storage/JWLN5HL5/Chavalarias et al. - 2016 - Evolution of Reporting P Values in the Biomedical .pdf}
}

@article{cheekWhatNumberIssues2006,
  title = {What's in a {{Number}}? {{Issues}} in {{Providing Evidence}} of {{Impact}} and {{Quality}} of {{Research}}(Ers)},
  shorttitle = {What's in a {{Number}}?},
  author = {Cheek, Julianne and Garnham, Bridget and Quan, James},
  year = {2006},
  month = apr,
  journal = {Qualitative health research},
  volume = {16},
  pages = {423--35},
  doi = {10.1177/1049732305285701},
  abstract = {One of the challenges facing qualitative researchers in a climate in which audit culture has permeated many facets of the institutions in which they research is how to establish the impact and quality of their research. When examining track records, granting institutions place significant emphasis on publication performance. Although the quality and impact of publications have traditionally been assessed by peer review, there is currently a global trend toward the development, refinement, and increased use of quantitative metrics, particularly citation analysis and journal impact factor. In this article, the authors share their experience of using the metrics citation analysis and journal impact factor in the preparation of an application for funding. Their aim is twofold: to raise awareness about potential issues in the practical application of these metrics; and to offer critique about and, they hope, "quality" to the writing and rhetoric concerning how to measure publication impact and quality.}
}

@article{chengMiRNAsBiomarkersMyocardial2014,
  title = {{{MiRNAs}} as Biomarkers of Myocardial Infarction: {{A}} Meta-Analysis},
  author = {Cheng, C. and Wang, Q. and You, W. and Chen, M. and Xia, J.},
  year = {2014},
  journal = {PLoS ONE},
  volume = {9},
  number = {2},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0088566}
}

@article{chenMetaanalysisAssociationCKM2017a,
  title = {A Meta-Analysis of the Association of {{CKM}} Gene Rs8111989 Polymorphism with Sport Performance},
  author = {Chen, C. and Sun, Y. and Liang, H. and Yu, D. and Hu, S.},
  year = {2017},
  journal = {Biology of Sport},
  volume = {34},
  number = {4},
  pages = {323--330},
  publisher = {Institute of Sport},
  doi = {10.5114/biolsport.2017.698189}
}

@article{cheungGuideConductingMetaAnalysis2019,
  title = {A {{Guide}} to {{Conducting}} a {{Meta-Analysis}} with {{Non-Independent Effect Sizes}}},
  author = {Cheung, Mike W.-L.},
  year = {2019},
  month = dec,
  journal = {Neuropsychology Review},
  volume = {29},
  number = {4},
  pages = {387--396},
  issn = {1573-6660},
  doi = {10.1007/s11065-019-09415-6},
  abstract = {Conventional meta-analytic procedures assume that effect sizes are independent. When effect sizes are not independent, conclusions based on these conventional procedures can be misleading or even wrong. Traditional approaches, such as averaging the effect sizes and selecting one effect size per study, are usually used to avoid the dependence of the effect sizes. These ad-hoc approaches, however, may lead to missed opportunities to utilize all available data to address the relevant research questions. Both multivariate meta-analysis and three-level meta-analysis have been proposed to handle non-independent effect sizes. This paper gives a brief introduction to these new techniques for applied researchers. The first objective is to highlight the benefits of using these methods to address non-independent effect sizes. The second objective is to illustrate how to apply these techniques with real data in R and Mplus. Researchers may modify the sample R and Mplus code to fit their data.},
  langid = {english},
  pmcid = {PMC6892772},
  pmid = {31446547},
  keywords = {{Models, Statistical},Humans,Meta-analysis,Meta-Analysis as Topic,Multivariate meta-analysis,Neuropsychology,Non-independent effect size,Research Design,Sample Size,Three-level meta-analysis},
  file = {/Users/cristian/Zotero/storage/LGXC3G6E/Cheung - 2019 - A Guide to Conducting a Meta-Analysis with Non-Ind.pdf}
}

@article{cheungModelingDependentEffect2014,
  title = {Modeling Dependent Effect Sizes with Three-Level Meta-Analyses: A Structural Equation Modeling Approach},
  shorttitle = {Modeling Dependent Effect Sizes with Three-Level Meta-Analyses},
  author = {Cheung, Mike W.-L.},
  year = {2014},
  month = jun,
  journal = {Psychological Methods},
  volume = {19},
  number = {2},
  pages = {211--229},
  issn = {1939-1463},
  doi = {10.1037/a0032968},
  abstract = {Meta-analysis is an indispensable tool used to synthesize research findings in the social, educational, medical, management, and behavioral sciences. Most meta-analytic models assume independence among effect sizes. However, effect sizes can be dependent for various reasons. For example, studies might report multiple effect sizes on the same construct, and effect sizes reported by participants from the same cultural group are likely to be more similar than those reported by other cultural groups. This article reviews the problems and common methods to handle dependent effect sizes. The objective of this article is to demonstrate how 3-level meta-analyses can be used to model dependent effect sizes. The advantages of the structural equation modeling approach over the multilevel approach with regard to conducting a 3-level meta-analysis are discussed. This article also seeks to extend the key concepts of Q statistics, I2, and R2 from 2-level meta-analyses to 3-level meta-analyses. The proposed procedures are implemented using the open source metaSEM package for the R statistical environment. Two real data sets are used to illustrate these procedures. New research directions related to 3-level meta-analyses are discussed.},
  langid = {english},
  pmid = {23834422},
  keywords = {{Models, Psychological},{Models, Statistical},Humans,Meta-Analysis as Topic,Multivariate Analysis,Research Design}
}

@article{chinQuestionableResearchPractices2023,
  title = {Questionable {{Research Practices}} and {{Open Science}} in {{Quantitative Criminology}}},
  author = {Chin, Jason M. and Pickett, Justin T. and Vazire, Simine and Holcombe, Alex O.},
  year = {2023},
  month = mar,
  journal = {Journal of Quantitative Criminology},
  volume = {39},
  number = {1},
  pages = {21--51},
  issn = {1573-7799},
  doi = {10.1007/s10940-021-09525-6},
  urldate = {2023-03-02},
  abstract = {Questionable research practices (QRPs) lead to incorrect research results and contribute to irreproducibility in science. Researchers and institutions have proposed open science practices (OSPs) to improve the detectability of QRPs and the credibility of science. We examine the prevalence of QRPs and OSPs in criminology, and researchers' opinions of those practices.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/47TC7RKV/Chin et al. - 2023 - Questionable Research Practices and Open Science i.pdf}
}

@article{chooEffectColdWater2023,
  title = {The Effect of Cold Water Immersion on the Recovery of Physical Performance Revisited: {{A}} Systematic Review with Meta-Analysis.},
  author = {Choo, Hui Cheng and Lee, Marcus and Yeo, Vincent and Poon, Wayne and Ihsan, Mohammed},
  year = {2023},
  month = mar,
  journal = {Journal of sports sciences},
  pages = {1--31},
  address = {England},
  issn = {1466-447X 0264-0414},
  doi = {10.1080/02640414.2023.2178872},
  abstract = {This review evaluated the effect of CWI on the temporal recovery profile of physical performance, accounting for environmental conditions and prior exercise  modality. Sixty-eight studies met the inclusion criteria. Standardised mean  differences were calculated for parameters assessed at {$<$}1, 1-6, 24, 48, 72 and  {$\geq$}96~h post-immersion. CWI improved short-term recovery of endurance performance  (p~=~0.01, 1~h), but impaired sprint (p~=~0.03, 1~h) and jump performance  (p~=~0.04, 6h). CWI improved longer-term recovery of jump performance  (p~{$<~$}0.01-0.02, 24~h and 96~h) and strength (p~{$<~$}0.01, 24~h), which coincided  with decreased creatine kinase (p~{$<~$}0.01-0.04, 24-72~h), improved muscle soreness  (p~{$<~$}0.01-0.02, 1-72~h) and perceived recovery (p~{$<~$}0.01, 72~h). CWI improved the  recovery of endurance performance following exercise in warm (p~{$<~$}0.01) and but  not in temperate conditions (p~=~0.06). CWI improved strength recovery following  endurance exercise performed at cool-to-temperate conditions (p~=~0.04) and  enhanced recovery of sprint performance following resistance exercise (p~=~0.04).  CWI seems to benefit the acute recovery of endurance performance, and longer-term  recovery of muscle strength and power, coinciding with changes in muscle damage  markers. This, however, depends on the nature of the preceding exercise.},
  langid = {english},
  pmid = {36862831},
  keywords = {Cryotherapy,hyperthermia,muscle damage,post-exercise cooling,regeneration}
}

@article{choTwotailedTestingDirectional2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  year = {2013},
  journal = {Journal of Business Research},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2012.02.023},
  abstract = {This paper demonstrates that there is currently a widespread misuse of two-tailed testing for directional research hypotheses tests. One probable reason for this overuse of two-tailed testing is the seemingly valid beliefs that two-tailed testing is more conservative and safer than one-tailed testing. However, the authors examine the legitimacy of this notion and find it to be flawed. A second and more fundamental cause of the current problem is the pervasive oversight in making a clear distinction between the research hypothesis and the statistical hypothesis. Based upon the explicated, sound relationship between the research and statistical hypotheses, the authors propose a new scheme of hypothesis classification to facilitate and clarify the proper use of statistical hypothesis testing in empirical research.},
  keywords = {Hypothesis testing,One-tailed testing,Research hypothesis in existential form,Research hypothesis in non-existential form,Statistical hypothesis,Two-tailed testing}
}

@article{christensonEvidenceBasedApproach1999,
  title = {Evidence Based Approach to Practice Guides and Decision Thresholds for Cardiac Markers},
  author = {Christenson, R.H. and Duh, S.-H.},
  year = {1999},
  journal = {Scandinavian Journal of Clinical and Laboratory Investigation, Supplement},
  volume = {59},
  number = {230},
  pages = {90--102},
  publisher = {Scandinavian University Press},
  doi = {10.3109/00365519909168332}
}

@article{christogiannis_post-hoc,
  title = {The Self-Fulfilling Prophecy of Post-Hoc Power Calculations},
  author = {Christogiannis, Christos and Nikolakopoulos, Stavros and Pandis, Nikolaos and Mavridis, Dimitris},
  year = {2022},
  journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
  volume = {161},
  number = {2},
  pages = {315--317},
  issn = {1097-6752},
  doi = {10.1016/j.ajodo.2021.10.008},
  langid = {english},
  pmid = {35094754},
  keywords = {{Data Interpretation, Statistical},Data Interpretation,Statistical},
  file = {/Users/cristian/Zotero/storage/HE3KKQCB/Christogiannis et al. - 2022 - The self-fulfilling prophecy of post-hoc power cal.pdf}
}

@article{ciria_umbrella_review,
  title = {An Umbrella Review of Randomized Control Trials on the Effects of Physical Exercise on Cognition},
  author = {Ciria, Luis F. and {Rom{\'a}n-Caballero}, Rafael and Vadillo, Miguel A. and Holgado, Darias and {Luque-Casado}, Antonio and Perakakis, Pandelis and Sanabria, Daniel},
  year = {2023},
  month = jun,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {6},
  pages = {928--941},
  issn = {2397-3374},
  doi = {10.1038/s41562-023-01554-4},
  abstract = {Extensive research links regular physical exercise to an overall enhancement of cognitive function across the lifespan. Here we assess the causal evidence supporting this relationship in the healthy population, using an umbrella review of meta-analyses limited to randomized controlled trials (RCTs). Despite most of the 24 reviewed meta-analyses reporting a positive overall effect, our assessment reveals evidence of low statistical power in the primary RCTs, selective inclusion of studies, publication bias and large variation in combinations of pre-processing and analytic decisions. In addition, our meta-analysis of all the primary RCTs included in the revised meta-analyses shows small exercise-related benefits (d\,=\,0.22, 95\% confidence interval 0.16 to 0.28) that became substantially smaller after accounting for key moderators (that is, active control and baseline differences; d\,=\,0.13, 95\% confidence interval 0.07 to 0.20), and negligible after correcting for publication bias (d\,=\,0.05, 95\% confidence interval -0.09 to 0.14). These findings suggest caution in claims and recommendations linking regular physical exercise to cognitive benefits in the healthy human population until more reliable causal evidence accumulates.},
  langid = {english},
  pmid = {36973359},
  keywords = {Cognition,Exercise,Health Status,Humans,Longevity,Meta-Analysis as Topic,Randomized Controlled Trials as Topic}
}

@article{claesenComparingDreamReality2021,
  title = {Comparing Dream to Reality: An Assessment of Adherence of the First Generation of Preregistered Studies},
  shorttitle = {Comparing Dream to Reality},
  author = {Claesen, Aline and Gomes, Sara and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2021},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {10},
  pages = {211037},
  publisher = {Royal Society},
  doi = {10.1098/rsos.211037},
  urldate = {2023-08-10},
  abstract = {Preregistration is a method to increase research transparency by documenting research decisions on a public, third-party repository prior to any influence by data. It is becoming increasingly popular in all subfields of psychology and beyond. Adherence to the preregistration plan may not always be feasible and even is not necessarily desirable, but without disclosure of deviations, readers who do not carefully consult the preregistration plan might get the incorrect impression that the study was exactly conducted and reported as planned. In this paper, we have investigated adherence and disclosure of deviations for all articles published with the Preregistered badge in Psychological Science between February 2015 and November 2017 and shared our findings with the corresponding authors for feedback. Two out of 27 preregistered studies contained no deviations from the preregistration plan. In one study, all deviations were disclosed. Nine studies disclosed none of the deviations. We mainly observed (un)disclosed deviations from the plan regarding the reported sample size, exclusion criteria and statistical analysis. This closer look at preregistrations of the first generation reveals possible hurdles for reporting preregistered studies and provides input for future reporting guidelines. We discuss the results and possible explanations, and provide recommendations for preregistered research.},
  keywords = {open science,preregistration,psychological science,researcher degrees of freedom,transparency},
  file = {/Users/cristian/Zotero/storage/R2L64J2Q/Claesen et al. - 2021 - Comparing dream to reality an assessment of adher.pdf}
}

@article{claussCarbohydrateIngestionProlonged2023,
  title = {Carbohydrate {{Ingestion}} during {{Prolonged Cycling Improves Next-Day Time Trial Performance}} and {{Alters Amino Acid Concentrations}}},
  author = {Clauss, Matthieu and Skattebo, {\O}yvind and D{\ae}hli, Malin Rasen and Valsdottir, Ditta and Bastani, Nasser Ezzatkhah and Johansen, Egil I. and Kolnes, Kristoffer Jensen and Sk{\aa}lhegg, Bj{\o}rn Steen and Jensen, J{\o}rgen},
  year = {2023},
  journal = {Medicine \& Science in Sports \& Exercise},
  pages = {10.1249/MSS.0000000000003264},
  issn = {0195-9131},
  doi = {10.1249/MSS.0000000000003264},
  urldate = {2023-08-10},
  abstract = {Introduction~           Exercise with low carbohydrate availability increases protein degradation, which may reduce subsequent performance considerably. The present study aimed to investigate the effect of carbohydrate ingestion during standardized exercise with and without exhaustion on protein degradation and next-day performance.           Methods~           Seven trained male cyclists (VO2max 66.8 {\textpm} 1.9 mL{$\cdot$}kg-1{$\cdot$}min-1; mean {\textpm} SEM) cycled to exhaustion ({\textasciitilde}2.5 hours) at a power output eliciting 68\% of VO2max (W68\%). This was followed by repeating 1-min work/1-min recovery intervals at 90\% of VO2max (W90\%) until exhaustion. During W68\%, cyclists consumed a placebo water drink (PLA) the first time and a carbohydrate drink (CHO), 1 g carbohydrate{$\cdot$}kg-1{$\cdot$}h-1, the second time. The participants performed the same amount of work under the two conditions, separated by at least one week. A standardized diet was provided to the participants so that the two conditions were isoenergetic. To test the impact of carbohydrates on recovery, participants completed a time trial (TT) the next day.           Results~           Carbohydrate ingestion maintained carbohydrate availability during W68\% and W90\%: total carbohydrate oxidation was significantly higher in CHO (p = 0.022), and plasma glucose concentration was maintained compared to PLA (p = 0.025). Next-day performance during TT was better after CHO ingestion (CHO, 41:49 {\textpm} 1:38 min; PLA, 42:50 {\textpm} 1:46 min; p = 0.020; effect size d = 0.23, small), as was gross efficiency (CHO, 18.6 {\textpm} 0.3 \%; PLA, 17.9 {\textpm} 0.3 \%; p = 0.019). Urinary nitrogen excretion (p = 0.897) and urinary 3-methylhistidine excretion (p = 0.673) did not significantly differ during the study period. Finally, tyrosine and phenylalanine plasma concentrations increased in PLA but not in CHO (p = 0.018).           Conclusions~           Carbohydrate ingestion during exhaustive exercise reduced deterioration in next-day performance through reduced metabolic stress and development of fatigue. In addition, some parameters point toward less protein degradation, which would preserve muscle function.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/NBE3RQTZ/Carbohydrate_Ingestion_during_Prolonged_Cycling.335.html}
}

@article{coburnPublicationBiasFunction2015,
  title = {Publication Bias as a Function of Study Characteristics},
  author = {Coburn, Kathleen M. and Vevea, Jack L.},
  year = {2015},
  month = sep,
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {310--330},
  issn = {1939-1463},
  doi = {10.1037/met0000046},
  abstract = {Researchers frequently conceptualize publication bias as a bias against publishing nonsignificant results. However, other factors beyond significance levels can contribute to publication bias. Some of these factors include study characteristics, such as the source of funding for the research project, whether the project was single center or multicenter, and prevailing theories at the time of publication. This article examines the relationship between publication bias and 2 study characteristics by breaking down 2 meta-analytic data sets into levels of the relevant study characteristic and assessing publication bias in each level with funnel plots, trim and fill (Duval \& Tweedie, 2000a, 2000b), Egger's linear regression (Egger, Smith, Schneider, \& Minder, 1997), cumulative meta-analysis (Borenstein, Hedges, Higgins, \& Rothstein, 2009), and the Vevea and Hedges (1995) weight-function model. Using the Vevea and Hedges model, we conducted likelihood ratio tests to determine whether information was lost if only 1 pattern of selection was estimated. Results indicate that publication bias can differ over levels of study characteristics, and that developing a model to accommodate this relationship could be advantageous.},
  langid = {english},
  pmid = {26348731},
  keywords = {{Data Interpretation, Statistical},Biomedical Research,Forecasting,Humans,Medicine and health sciences,Mental health and psychiatry,Meta-Analysis as Topic,Metaanalysis,Monte Carlo method,Psychology,Publication Bias,Publication ethics,Systematic reviews},
  file = {/Users/cristian/Zotero/storage/GPW63YEY/Aert et al. - 2019 - Publication bias examined in meta-analyses from ps.pdf;/Users/cristian/Zotero/storage/4MHFS9KC/article.html}
}

@misc{coburnWeightrEstimatingWeightFunction2019,
  title = {Weightr: {{Estimating Weight-Function Models}} for {{Publication Bias}}},
  shorttitle = {Weightr},
  author = {Coburn, Kathleen M. and Vevea, Jack L.},
  year = {2019},
  month = jul,
  urldate = {2022-05-21},
  abstract = {Estimates the Vevea and Hedges (1995) weight-function model. By specifying arguments, users can also estimate the modified model described in Vevea and Woods (2005), which may be more practical with small datasets. Users can also specify moderators to estimate a linear model. The package functionality allows users to easily extract the results of these analyses as R objects for other uses. In addition, the package includes a function to launch both models as a Shiny application. Although the Shiny application is also available online, this function allows users to launch it locally if they choose.},
  copyright = {GPL-2 {\textbar} GPL-3},
  keywords = {MetaAnalysis}
}

@article{cohenEarth051994,
  title = {The Earth Is Round (p{\enspace}{$<$}{\enspace}.05)},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  pages = {997--1003},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H{$_0$} is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H{$_0$} one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing},
  file = {/Users/cristian/Zotero/storage/GNLSSE5V/1995-12080-001.html}
}

@article{cohenStatisticalPowerAbnormalsocial1962,
  title = {The Statistical Power of Abnormal-Social Psychological Research: A Review},
  shorttitle = {The Statistical Power of Abnormal-Social Psychological Research},
  author = {Cohen, J.},
  year = {1962},
  month = sep,
  journal = {Journal of Abnormal and Social Psychology},
  volume = {65},
  pages = {145--153},
  doi = {10.1037/h0045186},
  langid = {english},
  keywords = {{Psychology, Social},Biometry,Humans,Psychopathology,PSYCHOPATHOLOGY/statistics,Research},
  annotation = {https://doi.org/10.1037/h0045186}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {Academic Press},
  address = {London},
  langid = {english},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  file = {/Users/cristian/Zotero/storage/DQ7RFJRZ/Cohen - 1988 - Statistical power analysis for the behavioral scie.pdf}
}

@article{cohenStatisticalPowerAnalysis1992,
  title = {Statistical {{Power Analysis}}},
  author = {Cohen, Jacob},
  year = {1992},
  month = jun,
  journal = {Current Directions in Psychological Science},
  volume = {1},
  number = {3},
  pages = {98--101},
  doi = {10.1111/1467-8721.ep10768783},
  langid = {english}
}

@article{colesCostsBenefitsReplication2018,
  title = {The Costs and Benefits of Replication Studies},
  author = {Coles, Nicholas A. and Tiokhin, Leonid and Scheel, Anne M. and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  journal = {The Behavioral and Brain Sciences},
  volume = {41},
  pages = {e124},
  doi = {10.1017/S0140525X18000596},
  abstract = {The debate about whether replication studies should become mainstream is essentially driven by disagreements about their costs and benefits and the best ways to allocate limited resources. Determining when replications are worthwhile requires quantifying their expected utility. We argue that a formalized framework for such evaluations can be useful for both individual decision-making and collective discussions about replication.},
  langid = {english},
  pmid = {31064512},
  keywords = {Cost-Benefit Analysis,Decision Making},
  file = {/Users/cristian/Zotero/storage/2642X6NU/Coles et al. - 2018 - The costs and benefits of replication studies.pdf}
}

@article{collinsUsingUnderstandingPower2021,
  title = {Using and {{Understanding Power}} in {{Psychological Research}}: {{A Survey Study}}},
  shorttitle = {Using and {{Understanding Power}} in {{Psychological Research}}},
  author = {Collins, Elizabeth and Watt, Roger},
  year = {2021},
  month = oct,
  journal = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {28250},
  doi = {10.1525/collabra.28250},
  abstract = {Statistical power is key to planning studies if understood and used correctly. Power is the probability of obtaining a statistically significant p-value, given a set alpha, sample size, and population effect size. The literature suggests that psychology studies are underpowered due to small sample sizes, and that researchers do not hold accurate intuitions about sensible sample sizes and associated levels of power. In this study, we surveyed 214 psychological researchers, and asked them about their experiences of using a priori power analysis, effect size estimation methods, post hoc power, and their understanding of what the term ``power'' actually means. Power analysis use was high, although participants reported difficulties with complex research designs, and effect size estimation. Participants also typically could not accurately define power. If psychological researchers are expected to compute a priori power analyses to plan their research, clearer educational material and guidelines should be made available.},
  file = {/Users/cristian/Zotero/storage/JHNI27GR/Collins and Watt - 2021 - Using and Understanding Power in Psychological Res.pdf;/Users/cristian/Zotero/storage/Z88M9FCT/Using-and-Understanding-Power-in-Psychological.html}
}

@article{colquhounFalsePositiveRisk2019,
  title = {The {{False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  shorttitle = {The {{False Positive Risk}}},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {192--201},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  urldate = {2024-07-22},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value},
  file = {/Users/cristian/Zotero/storage/8EVDM62P/Colquhoun - 2019 - The False Positive Risk A Proposal Concerning Wha.pdf}
}

@article{colquhounInvestigationFalseDiscovery2014,
  title = {An Investigation of the False Discovery Rate and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = {2014},
  month = nov,
  journal = {Royal Society Open Science},
  volume = {1},
  number = {3},
  pages = {140216},
  publisher = {Royal Society},
  doi = {10.1098/rsos.140216},
  urldate = {2024-11-05},
  abstract = {If you use p=0.05 to suggest that you have made a discovery, you will be wrong at least 30\% of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t-tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are over-estimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a three-sigma rule, or to insist on p{$\leq$}0.001. And never use the word `significant'.},
  keywords = {false discovery rate,reproducibility,significance tests,statistics},
  file = {/Users/cristian/Zotero/storage/W3TC5WE9/Colquhoun - 2014 - An investigation of the false discovery rate and t.pdf}
}

@article{cook_mcid_2008,
  title = {Clinimetrics {{Corner}}: {{The Minimal Clinically Important Change Score}} ({{MCID}}): {{A Necessary Pretense}}},
  shorttitle = {Clinimetrics {{Corner}}},
  author = {Cook, Chad E.},
  year = {2008},
  journal = {The Journal of Manual \& Manipulative Therapy},
  volume = {16},
  number = {4},
  pages = {E82-83},
  issn = {1066-9817},
  doi = {10.1179/jmt.2008.16.4.82E},
  abstract = {Minimal clinically important differences (MCID) are patient derived scores that reflect changes in a clinical intervention that are meaningful for the patient. At present, there are a number of different methods to obtain an MCID, as there a number of different factors that can influence the MCID value. This clinimetric corner outlines the hidden challenges associated with identifying a viable MCID and possible suggestions to improve the future development of these single scores.},
  langid = {english},
  pmcid = {PMC2716157},
  pmid = {19771185},
  keywords = {Global rating of change,Minimal clinically important difference,Outcomes measures},
  file = {/Users/cristian/Zotero/storage/K56ER3PL/Cook - 2008 - Clinimetrics Corner The Minimal Clinically Import.pdf}
}

@article{cookAssessingMethodsSpecify2014,
  title = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial: {{DELTA}} ({{Difference ELicitation}} in {{TriAls}}) Review},
  shorttitle = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial},
  author = {Cook, Jonathan A. and Hislop, Jennifer and Adewuyi, Temitope E. and Harrild, Kirsten and Altman, Douglas G. and Ramsay, Craig R. and Fraser, Cynthia and Buckley, Brian and Fayers, Peter and Harvey, Ian and Briggs, Andrew H. and Norrie, John D. and Fergusson, Dean and Ford, Ian and Vale, Luke D.},
  year = {2014},
  month = may,
  journal = {Health Technology Assessment (Winchester, England)},
  volume = {18},
  number = {28},
  pages = {v-vi, 1--175},
  issn = {2046-4924},
  doi = {10.3310/hta18280},
  abstract = {BACKGROUND: The randomised controlled trial (RCT) is widely considered to be the gold standard study for comparing the effectiveness of health interventions. Central to the design and validity of a RCT is a calculation of the number of participants needed (the sample size). The value used to determine the sample size can be considered the 'target difference'. From both a scientific and an ethical standpoint, selecting an appropriate target difference is of crucial importance. Determination of the target difference, as opposed to statistical approaches to calculating the sample size, has been greatly neglected though a variety of approaches have been proposed the current state of the evidence is unclear. OBJECTIVES: The aim was to provide an overview of the current evidence regarding specifying the target difference in a RCT sample size calculation. The specific objectives were to conduct a systematic review of methods for specifying a target difference; to evaluate current practice by surveying triallists; to develop guidance on specifying the target difference in a RCT; and to identify future research needs. DESIGN: The biomedical and social science databases searched were MEDLINE, MEDLINE In-Process \& Other Non-Indexed Citations, EMBASE, Cochrane Central Register of Controlled Trials (CENTRAL), Cochrane Methodology Register, PsycINFO, Science Citation Index, EconLit, Education Resources Information Center (ERIC) and Scopus for in-press publications. All were searched from 1966 or the earliest date of the database coverage and searches were undertaken between November 2010 and January 2011. There were three interlinked components: (1) systematic review of methods for specifying a target difference for RCTs - a comprehensive search strategy involving an electronic literature search of biomedical and some non-biomedical databases and clinical trials textbooks was carried out; (2) identification of current trial practice using two surveys of triallists - members of the Society for Clinical Trials (SCT) were invited to complete an online survey and respondents were asked about their awareness and use of, and willingness to recommend, methods; one individual per triallist group [UK Clinical Research Collaboration (UKCRC)-registered Clinical Trials Units (CTUs), Medical Research Council (MRC) UK Hubs for Trials Methodology Research and National Institute for Health Research (NIHR) UK Research Design Services (RDS)] was invited to complete a survey; (3) production of a structured guidance document to aid the design of future trials - the draft guidance was developed utilising the results of the systematic review and surveys by the project steering and advisory groups. SETTING: Methodological review incorporating electronic searches, review of books and guidelines, two surveys of experts (membership of an international society and UK- and Ireland-based triallists) and development of guidance. PARTICIPANTS: The two surveys were sent out to membership of the SCT and UK- and Ireland-based triallists. INTERVENTIONS: The review focused on methods for specifying the target difference in a RCT. It was not restricted to any type of intervention or condition. MAIN OUTCOME MEASURES: Methods for specifying the target difference for a RCT were considered. RESULTS: The search identified 11,485 potentially relevant studies. In total, 1434 were selected for full-text assessment and 777 were included in the review. Seven methods to specify the target difference for a RCT were identified - anchor, distribution, health economic, opinion-seeking, pilot study, review of evidence base (RoEB) and standardised effect size (SES) - each having important variations in implementation. A total of 216 of the included studies used more than one method. A total of 180 (15\%) responses to the SCT survey were received, representing 13 countries. Awareness of methods ranged from 38\% (n =69) for the health economic method to 90\% (n =162) for the pilot study. Of the 61 surveys sent out to UK triallist groups, 34 (56\%) responses were received. Awareness ranged from 97\% (n =33) for the RoEB and pilot study methods to only 41\% (n =14) for the distribution method. Based on the most recent trial, all bar three groups (91\%, n =30) used a formal method. Guidance was developed on the use of each method and the reporting of the sample size calculation in a trial protocol and results paper. CONCLUSIONS: There is a clear need for greater use of formal methods to determine the target difference and better reporting of its specification. Raising the standard of RCT sample size calculations and the corresponding reporting of them would aid health professionals, patients, researchers and funders in judging the strength of the evidence and ensuring better use of scarce resources. FUNDING: The Medical Research Council UK and the National Institute for Health Research Joint Methodology Research programme.},
  langid = {english},
  pmcid = {PMC4781097},
  pmid = {24806703},
  keywords = {{Societies, Scientific},Attitude of Health Personnel,Bias,Cost-Benefit Analysis,Data Collection,Epidemiologic Research Design,Evidence-Based Medicine,Humans,Internationality,Ireland,Patient Satisfaction,Randomized Controlled Trials as Topic,Research Personnel,Sample Size,Statistics as Topic,United Kingdom},
  file = {/Users/cristian/Zotero/storage/D7XG7I7J/Cook et al. - 2014 - Assessing methods to specify the target difference.pdf}
}

@article{cookMultiplicityConsiderationsDesign1996,
  title = {Multiplicity {{Considerations}} in the {{Design}} and {{Analysis}} of {{Clinical Trials}}},
  author = {Cook, Richard J. and Farewell, Vern T.},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {159},
  number = {1},
  eprint = {2983471},
  eprinttype = {jstor},
  pages = {93--110},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0964-1998},
  doi = {10.2307/2983471},
  urldate = {2024-05-03},
  abstract = {The need for efficient use of available resources in medical research has led to the increased appeal of clinical trial designs based on multiple responses, multiple treatment arms and repeated tests of significance. In recent years there has been considerable methodological work pertaining to these types of multiple comparison, with the common objective typically being the control of the experimental type I error rate. Here we reconsider the appropriateness of these objectives in a variety of contexts and suggest that multiple-comparison procedures are frequently adopted unnecessarily. In particular we argue that, provided that a select number of important well-defined clinical questions are specified at the design, there are situations in which multiple tests of significance can be performed without control of the experimental type I error rate. The primary restriction for this to be reasonable is that test results are interpreted marginally.},
  file = {/Users/cristian/Zotero/storage/YHAJAJ84/Cook and Farewell - 1996 - Multiplicity Considerations in the Design and Anal.pdf}
}

@article{cooperRelativeBenefitsMetaanalysis2009,
  title = {The Relative Benefits of Meta-Analysis Conducted with Individual Participant Data versus Aggregated Data},
  author = {Cooper, Harris and Patall, Erika A.},
  year = {2009},
  month = jun,
  journal = {Psychological Methods},
  volume = {14},
  number = {2},
  pages = {165--176},
  issn = {1082-989X},
  doi = {10.1037/a0015565},
  abstract = {The authors describe the relative benefits of conducting meta-analyses with (a) individual participant data (IPD) gathered from the constituent studies and (b) aggregated data (AD), or the group-level statistics (in particular, effect sizes) that appear in reports of a study's results. Given that both IPD and AD are equally available, meta-analysis of IPD is superior to meta-analysis of AD. IPD meta-analysis permits synthesists to perform subgroup analyses not conducted by the original collectors of the data, to check the data and analyses in the original studies, to add new information to the data sets, and to use different statistical methods. However, the cost of IPD meta-analysis and the lack of available IPD data sets suggest that the best strategy currently available is to use both approaches in a complementary fashion such that the first step in conducting an IPD meta-analysis would be to conduct an AD meta-analysis. Regardless of whether a meta-analysis is conducted with IPD or AD, synthesists must remain vigilant in how they interpret their results. They must avoid ecological fallacies, Simpson's paradox, and interpretation of synthesis-generated evidence as supporting causal inferences.},
  langid = {english},
  pmid = {19485627},
  keywords = {{Data Interpretation, Statistical},{Models, Statistical},Data Collection,Humans,Meta-Analysis as Topic}
}

@book{cooperReportingQuantitativeResearch2020,
  title = {Reporting Quantitative Research in Psychology: {{How}} to Meet {{APA Style Journal Article Reporting Standards}}, 2nd Ed},
  shorttitle = {Reporting Quantitative Research in Psychology},
  author = {Cooper, Harris},
  year = {2020},
  series = {Reporting Quantitative Research in Psychology: {{How}} to Meet {{APA Style Journal Article Reporting Standards}}, 2nd Ed},
  pages = {vii, 217},
  publisher = {American Psychological Association},
  address = {Washington, DC, US},
  doi = {10.1037/0000178-000},
  abstract = {This book offers practical guidance for understanding and implementing the American Psychological Association's Journal Article Reporting Standards for Quantitative Research (JARS-Quant) and Meta-Analysis Reporting Standards (MARS). These standards lay out the essential pieces information researchers need to report, including detailed accounts of the methods they followed, data results and analysis, interpretations of their findings, and implications for future research. The book reflects updates to the original JARS and the MARS that meet researchers' developing needs in the behavioral, social, educational, and medical sciences. It analyzes examples from APA journals, offering readers easy-to-read advice for implementing these revised standards in their own writing while also conforming with the APA Style guidelines laid out in the sixth edition of the Publication Manual. New and expanded chapters offer more detailed guidelines for reporting statistical analyses and unique elements of different types of research, including replication studies, clinical trials, and observational studies. This book is essential reading for experienced and early career researchers alike, as well as undergraduate and graduate students in research methods classes. It presents what JARS recommends for information to include in all reports on new quantitative data collections, and addresses the material that appears first in a research manuscript. It also describes the Method section, presents the JARS standards for reporting basic research designs and covers the general reporting requirements for the statistical results of studies with multiple participants in each condition. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-1-4338-3283-3 978-1-4338-3342-7},
  keywords = {Academic Writing,American Psychological Association,Behavioral Sciences,Experimental Design,Experimental Ethics,Medical Sciences,Meta Analysis,Quantitative Methods,Reporting Standards,Social Sciences,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/HIHIVRJF/2020-01159-000.html}
}

@article{couttsEffectGlycerolHyperhydration2002,
  ids = {couttsEffectGlycerolHyperhydration2002a},
  title = {The Effect of Glycerol Hyperhydration on {{Olympic}} Distance Triathlon Performance in High Ambient Temperatures},
  author = {Coutts, Aaron and Reaburn, Peter and Mummery, Kerry and Holmes, Mark},
  year = {2002},
  journal = {International journal of sport nutrition and exercise metabolism},
  volume = {12},
  number = {1},
  pages = {105--119},
  publisher = {Human Kinetics, Inc.},
  file = {/Users/cristian/Zotero/storage/8KAD5RC7/Coutts et al. - 2002 - The effect of glycerol hyperhydration on Olympic d.pdf;/Users/cristian/Zotero/storage/S73C5E42/article-p105.html}
}

@article{coxProblemsConnectedStatistical1958,
  title = {Some {{Problems Connected}} with {{Statistical Inference}}},
  author = {Cox, D. R.},
  year = {1958},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {29},
  number = {2},
  pages = {357--372},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177706618},
  urldate = {2021-03-09},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/cristian/Zotero/storage/AYVGGKGB/Cox - 1958 - Some Problems Connected with Statistical Inference.pdf;/Users/cristian/Zotero/storage/PBE3KZW6/1177706618.html}
}

@article{cramerHiddenMultiplicityExploratory2016,
  title = {Hidden Multiplicity in Exploratory Multiway {{ANOVA}}: {{Prevalence}} and Remedies},
  shorttitle = {Hidden Multiplicity in Exploratory Multiway {{ANOVA}}},
  author = {Cramer, Ang{\'e}lique O. J. and {van Ravenzwaaij}, Don and Matzke, Dora and Steingroever, Helen and Wetzels, Ruud and Grasman, Raoul P. P. P. and Waldorp, Lourens J. and Wagenmakers, Eric-Jan},
  year = {2016},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {2},
  pages = {640--647},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0913-5},
  urldate = {2023-08-13},
  abstract = {Many psychologists do not realize that exploratory use of the popular multiway analysis of variance harbors a multiple-comparison problem. In the case of two factors, three separate null hypotheses are subject to test (i.e., two main effects and one interaction). Consequently, the probability of at least one Type I error (if all null hypotheses are true) is 14~\% rather than 5~\%, if the three tests are independent. We explain the multiple-comparison problem and demonstrate that researchers almost never correct for it. To mitigate the problem, we describe four remedies: the omnibus F test, control of the familywise error rate, control of the false discovery rate, and preregistration of the hypotheses.},
  langid = {english},
  keywords = {Benjamini--Hochberg procedure,Factorial ANOVA,False discovery rate,Familywise error rate,Multiple comparison problem,Multiway ANOVA,Preregistration,Sequential Bonferroni,Type I error},
  file = {/Users/cristian/Zotero/storage/392BHVGW/Cramer et al. - 2016 - Hidden multiplicity in exploratory multiway ANOVA.pdf}
}

@article{crickReproducibilityResearchSystems2017,
  title = {Reproducibility in {{Research}}: {{Systems}}, {{Infrastructure}}, {{Culture}}},
  shorttitle = {Reproducibility in {{Research}}},
  author = {Crick, Tom and Hall, Benjamin A. and Ishtiaq, Samin},
  year = {2017},
  month = nov,
  journal = {Journal of Open Research Software},
  volume = {5},
  number = {1},
  issn = {2049-9647},
  doi = {10.5334/jors.73},
  urldate = {2025-07-02},
  abstract = {The reproduction and replication of research results has become a major issue for a number of scientific disciplines. In computer science and related computational disciplines such as systems biology, the challenges closely revolve around the ability to implement (and exploit) novel algorithms and models. Taking a new approach from the literature and applying it to a new codebase frequently requires local knowledge missing from the published manuscripts and transient project websites. Alongside this issue, benchmarking, and the lack of open, transparent and fair benchmark sets present another barrier to the verification and validation of claimed results.In this paper, we outline several recommendations to address these issues, driven by specific examples from a range of scientific domains. Based on these recommendations, we propose a high-level prototype open automated platform for scientific software development which effectively abstracts specific dependencies from the individual researcher and their workstation, allowing easy sharing and reproduction of results. This new e-infrastructure for reproducible computational science offers the potential to incentivise a culture change and drive the adoption of new techniques to improve the quality and efficiency -- and thus reproducibility -- of scientific exploration.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/INHLZVJV/Crick et al. - 2017 - Reproducibility in Research Systems, Infrastructu.pdf}
}

@article{crisafulliCreatineelectrolyteSupplementationImproves2018,
  title = {Creatine-Electrolyte Supplementation Improves Repeated Sprint Cycling Performance: {{A}} Double Blind Randomized Control Study},
  shorttitle = {Creatine-Electrolyte Supplementation Improves Repeated Sprint Cycling Performance},
  author = {Crisafulli, Daniel L. and Buddhadev, Harsh H. and Brilla, Lorrie R. and Chalmers, Gordon R. and Suprak, David N. and San Juan, Jun G.},
  year = {2018},
  month = may,
  journal = {Journal of the International Society of Sports Nutrition},
  volume = {15},
  number = {1},
  pages = {21},
  issn = {1550-2783},
  doi = {10.1186/s12970-018-0226-y},
  urldate = {2023-08-21},
  abstract = {Creatine supplementation is recommended as an ergogenic aid to improve repeated sprint cycling performance. Furthermore, creatine uptake is increased in the presence of electrolytes. Prior research examining the effect of a creatine-electrolyte (CE) supplement on repeated sprint cycling performance, however, did not show post-supplementation improvement. The purpose of this double blind randomized control study was to investigate the effect of a six-week CE supplementation intervention on overall and repeated peak and mean power output during repeated cycling sprints with recovery periods of 2 min between sprints.},
  keywords = {Creatine,Ergometer,Recovery interval,Sprint cycling,Sprint duration},
  file = {/Users/cristian/Zotero/storage/9EP3CCG9/Crisafulli et al. - 2018 - Creatine-electrolyte supplementation improves repe.pdf;/Users/cristian/Zotero/storage/8PD4L2M6/s12970-018-0226-y.html}
}

@article{cristeaEffectSizesReported2022,
  title = {Effect {{Sizes Reported}} in {{Highly Cited Emotion Research Compared With Larger Studies}} and {{Meta-Analyses Addressing}} the {{Same Questions}}},
  author = {Cristea, Ioana A. and Georgescu, Raluca and Ioannidis, John P. A.},
  year = {2022},
  month = jul,
  journal = {Clinical Psychological Science},
  volume = {10},
  number = {4},
  pages = {786--800},
  publisher = {SAGE Publications Inc},
  issn = {2167-7026},
  doi = {10.1177/21677026211049366},
  urldate = {2022-10-19},
  abstract = {We assessed whether the most highly cited studies in emotion research reported larger effect sizes compared with meta-analyses and the largest studies on the same question. We screened all reports with at least 1,000 citations and identified matching meta-analyses for 40 highly cited observational studies and 25 highly cited experimental studies. Highly cited observational studies had effects greater on average by 1.42-fold (95\% confidence interval [CI] = [1.09, 1.87]) compared with meta-analyses and 1.99-fold (95\% CI = [1.33, 2.99]) compared with largest studies on the same questions. Highly cited experimental studies had increases of 1.29-fold (95\% CI = [1.01, 1.63]) compared with meta-analyses and 2.02-fold (95\% CI = [1.60, 2.57]) compared with the largest studies. There was substantial between-topics heterogeneity, more prominently for observational studies. Highly cited studies often did not have the largest weight in meta-analyses (12 of 65 topics, 18\%) but were frequently the earliest ones published on the topic (31 of 65 topics, 48\%). Highly cited studies may offer, on average, exaggerated estimates of effects in both observational and experimental designs.},
  langid = {english}
}

@article{cronbachConstructValidityPsychological1955,
  title = {Construct Validity in Psychological Tests},
  author = {Cronbach, Lee J. and Meehl, Paul E.},
  year = {1955},
  journal = {Psychological Bulletin},
  volume = {52},
  pages = {281--302},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0040957},
  abstract = {"Construct validation was introduced in order to specify types of research required in developing tests for which the conventional views on validation are inappropriate. Personality tests, and some tests of ability, are interpreted in terms of attributes for which there is no adequate criterion. This paper indicates what sorts of evidence can substantiate such an interpretation, and how such evidence is to be interpreted." 60 references. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Construct Validity,Personality Measures,Psychometrics,Test Validity},
  file = {/Users/cristian/Zotero/storage/UNSHHMGV/Cronbach and Meehl - 1955 - Construct validity in psychological tests.pdf;/Users/cristian/Zotero/storage/VUCW5GLQ/1956-03730-001.html}
}

@article{cuijpersLackStatisticalPower2021,
  title = {The Lack of Statistical Power of Subgroup Analyses in Meta-Analyses: A Cautionary Note},
  shorttitle = {The Lack of Statistical Power of Subgroup Analyses in Meta-Analyses},
  author = {Cuijpers, Pim and Griffin, Jason W. and Furukawa, Toshi A.},
  year = {2021},
  month = dec,
  journal = {Epidemiology and Psychiatric Sciences},
  volume = {30},
  pages = {e78},
  issn = {2045-7960},
  doi = {10.1017/S2045796021000664},
  urldate = {2025-05-11},
  abstract = {One of the most used methods to examine sources of heterogeneity in meta-analyses is the so-called `subgroup analysis'. In a subgroup analysis, the included studies are divided into two or more subgroups, and it is tested whether the pooled effect sizes found in these subgroups differ significantly from each other. Subgroup analyses can be considered as a core component of most published meta-analyses. One important problem of subgroup analyses is the lack of statistical power to find significant differences between subgroups. In this paper, we explore the power problems of subgroup analyses in more detail, using `metapower', a recently developed statistical package in R to examine power in meta-analyses, including subgroup analyses. We show that subgroup analyses require many more included studies in a meta-analysis than are needed for the main analyses. We work out an example of an `average' meta-analysis, in which a subgroup analysis requires 3--4 times the number of studies that are needed for the main analysis to have sufficient power. This number of studies increases exponentially with decreasing effect sizes and when the studies are not evenly divided over the subgroups. Higher heterogeneity also requires increasing numbers of studies. We conclude that subgroup analyses remain an important method to examine potential sources of heterogeneity in meta-analyses, but that meta-analysts should keep in mind that power is very low for most subgroup analyses. As in any statistical evaluation, researchers should not rely on a test and p-value to interpret results, but should compare the confidence intervals and interpret results carefully.},
  pmcid = {PMC8679832},
  pmid = {34852862},
  file = {/Users/cristian/Zotero/storage/QHH4NIXW/Cuijpers et al. - 2021 - The lack of statistical power of subgroup analyses.pdf}
}

@article{cummingReplicationIntervalsValues2008,
  title = {Replication and p {{Intervals}}: {{P Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  author = {Cumming, Geoff},
  year = {2008},
  journal = {Perspectives on Psychological Science},
  volume = {3},
  number = {4},
  pages = {286--300},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  abstract = {Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is "Surprisingly little." In one simulation of 25 repetitions of a typical experiment, p varied from {$<$}.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p {$<$} .00008, and fully a 10\% chance that p {$>$} .44. Remarkably, the interval--termed a p interval--is this wide however large the sample size, p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and modelfitting techniques and by adopting meta-analytic thinking.}
}

@book{cummingUnderstandingNewStatistics2011,
  title = {Understanding {{The New Statistics}}: {{Effect Sizes}}, {{Confidence Intervals}}, and {{Meta-Analysis}}},
  shorttitle = {Understanding {{The New Statistics}}},
  author = {Cumming, Geoff},
  year = {2011},
  month = jun,
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9780203807002},
  abstract = {This is the first book to introduce the new statistics - effect sizes, confidence intervals, and meta-analysis - in an accessible way. It is chock full of practical examples and tips on how to analyze and report research results using these techniques. The book is invaluable to readers interested in meeting the new APA Publication Manual guidelines by adopting the new statistics - which are more informative than null hypothesis significance testing, and becoming widely used in many disciplines. Accompanying the book is the Exploratory Software for Confidence Intervals (ESCI) package, free software that runs under Excel and is accessible at www.thenewstatistics.com. The book's exercises use ESCI's simulations, which are highly visual and interactive, to engage users and encourage exploration. Working with the simulations strengthens understanding of key statistical ideas. There are also many examples, and detailed guidance to show readers how to analyze their own data using the new statistics, and practical strategies for interpreting the results. A particular strength of the book is its explanation of meta-analysis, using simple diagrams and examples. Understanding meta-analysis is increasingly important, even at undergraduate levels, because medicine, psychology and many other disciplines now use meta-analysis to assemble the evidence needed for evidence-based practice. The book's pedagogical program, built on cognitive science principles, reinforces learning: Boxes provide "evidence-based" advice on the most effective statistical techniques.  Numerous examples reinforce learning, and show that many disciplines are using the new statistics. Graphs are tied in with ESCI to make important concepts vividly clear and memorable. Opening overviews and end of chapter take-home messages summarize key points. Exercises encourage exploration, deep understanding, and practical applications. This highly accessible book is intended as the core text for any course that emphasizes the new statistics, or as a supplementary text for graduate and/or advanced undergraduate courses in statistics and research methods in departments of psychology, education, human development , nursing, and natural, social, and life sciences. Researchers and practitioners interested in understanding the new statistics, and future published research, will also appreciate this book. A basic familiarity with introductory statistics is assumed.},
  isbn = {978-0-203-80700-2}
}

@article{curranSeeminglyQuixoticPursuit2009,
  title = {The Seemingly Quixotic Pursuit of a Cumulative Psychological Science: {{Introduction}} to the Special Issue},
  shorttitle = {The Seemingly Quixotic Pursuit of a Cumulative Psychological Science},
  author = {Curran, Patrick J.},
  year = {2009},
  journal = {Psychological Methods},
  volume = {14},
  number = {2},
  pages = {77--80},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/a0015972},
  abstract = {The goal of any empirical science is to pursue the construction of a cumulative base of knowledge upon which the future of the science may be built. However, there is mixed evidence that the science of psychology can accurately be characterized by such a cumulative progression. Indeed, some argue that the development of a truly cumulative psychological science is not possible with the current paradigms of hypothesis testing in single-study designs. The author explores this controversy as a framework to introduce the 6 articles that make up this special issue on the integration of data and empirical findings across multiple studies. The author proposes that the methods and techniques described in this set of articles can significantly propel researchers forward in their ongoing quest to build a cumulative psychological science. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Meta Analysis,Psychology,Statistical Analysis,Statistical Data},
  file = {/Users/cristian/Zotero/storage/Y85ZRLBW/Curran - 2009 - The seemingly quixotic pursuit of a cumulative psy.pdf;/Users/cristian/Zotero/storage/XAHX7GKE/2009-08072-006.html}
}

@misc{CurtinAltmanDG,
  title = {Curtin {{F}}, {{Altman DG}}, {{Elbourne D}}. {{Meta-analysis}} Combining Parallel and Cross-over Clinical Trials. {{III}}: {{The}} Carry-over Issue. - {{Google Search}}},
  urldate = {2024-06-12},
  howpublished = {https://www.google.com/search?q=Curtin+F\%2C+Altman+DG\%2C+Elbourne+D.+Meta-analysis+combining+parallel+and+cross-over+clinical+trials.+III\%3A+The+carry-over+issue.\&oq=Curtin+F\%2C+Altman+DG\%2C+Elbourne+D.+Meta-analysis+combining+parallel+and+cross-over+clinical+trials.+III\%3A+The+carry-over+issue.\&gs\_lcrp=EgZjaHJvbWUyBggAEEUYOdIBBzUwNWowajeoAgCwAgA\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/cristian/Zotero/storage/PYN6NIMS/search.html}
}

@article{curtinMetaanalysisCombiningParallel2002,
  title = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{I}}: {{Continuous}} Outcomes},
  shorttitle = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{I}}},
  author = {Curtin, Fran{\c c}ois and Altman, Douglas G. and Elbourne, Diana},
  year = {2002},
  month = aug,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {15},
  pages = {2131--2144},
  issn = {0277-6715},
  doi = {10.1002/sim.1205},
  abstract = {Among clinical trials assessing a given treatment, often parallel and cross-over designs are used together. In the first paper of a series of three, we explore two methods to pool continuous outcomes in a meta-analysis combining parallel and cross-over trial designs: the weighted mean difference (WMD) and the standardized weighted mean difference (SWMD). The combined design meta-analytic formulae are based on a weighted average of the two design treatment estimates. A random effects model can be implemented. Both WMD and SWMD can be used, the choice of the method is determined by the type of outcomes obtained in the trials. Compared to the number of included subjects, the relative weight of the cross-over design is large in combined-design meta-analysis. Differences in the weight estimation between WMD and SWMD can also accentuate the relative weight of cross-over trials, which must be considered a case of design-specific bias.},
  langid = {english},
  pmid = {12210629},
  keywords = {Blood Pressure,Clinical Trials as Topic,Cross-Over Studies,Humans,Meta-Analysis as Topic,Potassium,Statistics as Topic}
}

@article{curtinMetaanalysisCombiningParallel2002a,
  title = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{III}}: {{The}} Issue of Carry-Over},
  shorttitle = {Meta-Analysis Combining Parallel and Cross-over Clinical Trials. {{III}}},
  author = {Curtin, Fran{\c c}ois and Elbourne, Diana and Altman, Douglas G.},
  year = {2002},
  month = aug,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {15},
  pages = {2161--2173},
  issn = {0277-6715},
  doi = {10.1002/sim.1207},
  abstract = {In meta-analysis combining results from parallel and cross-over trials, there is a risk of bias originating from the carry-over effect in cross-over trials. When pooling treatment effects estimated from parallel trials and two-period two-treatment cross-over trials, meta-analytic estimators of treatment effect can be obtained from the combination of parallel trial results either with cross-over trial results based on data of the first period only or with cross-over trial results analysed with data from both periods. Taking data from the first cross-over period protects against carry-over but gives less efficient treatment estimators and may lead to selection bias. This study evaluates in terms of variance reduction and mean square error the cost of calculating meta-analysis estimates with data from the first period instead of data from the two cross-over periods. If the information on cross-over sequence is available, we recommend performing two combined design meta-analyses, one using the first cross-over period data and one based on data from both cross-over periods. To investigate simultaneously the statistical significance of these two estimators as well as the carry-over at meta-analysis level, a method based on a multivariate analysis of the meta-analytic treatment effect and carry-over estimates is proposed.},
  langid = {english},
  pmid = {12210631},
  keywords = {{Sodium, Dietary},Blood Pressure,Clinical Trials as Topic,Cross-Over Studies,Female,Humans,Male,Meta-Analysis as Topic,Selection Bias,Statistics as Topic}
}

@article{darmawanDietaryPhytogenicExtracts2022,
  title = {Dietary {{Phytogenic Extracts Favorably Influence Productivity}}, {{Egg Quality}}, {{Blood Constituents}}, {{Antioxidant}} and {{Immunological Parameters}} of {{Laying Hens}}: {{A Meta-Analysis}}},
  author = {Darmawan, A. and Hermana, W. and Suci, D.M. and Mutia, R. and {Sumiati} and Jayanegara, A. and Ozturk, E.},
  year = {2022},
  journal = {Animals},
  volume = {12},
  number = {17},
  publisher = {MDPI},
  doi = {10.3390/ani12172278}
}

@article{daveyBrainMetastasesTreatment2002,
  title = {Brain Metastases: {{Treatment}} Options to Improve Outcomes},
  author = {Davey, P.},
  year = {2002},
  journal = {CNS Drugs},
  volume = {16},
  number = {5},
  pages = {325--338},
  publisher = {Adis International Ltd},
  doi = {10.2165/00023210-200216050-00005}
}

@article{davisMirtazapineReviewIts1996,
  title = {Mirtazapine : {{A Review}} of Its {{Pharmacology}} and {{Therapeutic Potential}} in the {{Management}} of {{Major Depression}}.},
  author = {Davis, R. and Wilde, M. I.},
  year = {1996},
  month = may,
  journal = {CNS drugs},
  volume = {5},
  number = {5},
  pages = {389--402},
  address = {New Zealand},
  issn = {1172-7047},
  doi = {10.2165/00023210-199605050-00007},
  abstract = {SYNOPSIS: Mirtazapine is a tetracyclic antidepressant with a novel mechanism of action; it increases noradrenergic and serotonergic neurotransmission via  blockade of central {$\alpha$}2-adrenergic auto- and heteroreceptors. The increased  release of serotonin (5-hydroxytryptamine; 5-HT) stimulates serotonin 5-HT1  receptors because mirtazapine directly blocks 5-HT2 and 5-HT3 receptors. The  enhancement of both noradrenergic- and 5-HT1 receptor-mediated neurotransmission  is thought to be responsible for the antidepressant activity of mirtazapine. In  short term (5 to 6 weeks) clinical trials in patients with depression.  mirtazapine produces clinical improvements significantly superior to those of  placebo, similar to those of tricyclic antidepressants (TCAs) [amitriptyline,  clomipramine and doxepin] and possibly superior to those of trazodone. Short term  clinical tolerability data suggest that mirtazapine produces fewer  anticholinergic-, adrenergic- and serotonergic-related adverse events than TCAs.  In rare cases, mirtazapine, in common with many antidepressants, was associated  with potentially serious changes in haematological parameters (e.g.  agranulocytosis and neutropenia). The drug appears to be safe in overdose and  possesses a very low propensity for inducing seizures. Comparisons with other  classes of antidepressants are needed to determine the relative position of  mirtazapine in clinical practice. However, preliminary data indicate that  mirtazapine, with its novel mechanism of action, is a promising addition to  currently available options for the treatment of depression. PHARMACODYNAMIC  PROPERTIES: In vitro neurochemical studies have demonstrated that mirtazapine  blocks central {$\alpha$}2-adrenergic auto- and heteroreceptors, but has no effect on  noradrenaline (norepinephrine) reuptake. The affinity of the drug was 10-fold  higher for central presynaptic {$\alpha$}2-adrenoceptors than for central postsynaptic and  peripheral {$\alpha$}2-adrenoceptors, and 30-fold higher for {$\alpha$}2-adrenoceptors than for  {$\alpha$}1-adrenoceptors. Microdialysis and neurophysiological experiments as well as  behavioural studies performed in rats support the {$\alpha$}2-adrenoceptor antagonist  properties of mirtazapine. Receptor binding studies have shown that mirtazapine  has a high affinity for serotonin 5-HT2 and 5-HT3 receptors, central and  peripheral histamine H1 receptors and a low affinity for 5-HT1, dopaminergic and  muscarinic cholinergic receptors. Its activity at serotonin receptor subtypes has  been confirmed in animal behaviour models. Mirtazapine activates 5-HT1  receptor-mediated serotonergic neurotransmission by enhancing the stimulatory  effect of the noradrenergic system on serotonergic cell firing (an  {$\alpha$}1-adrenoceptor-mediated effect) as well as antagonising the inhibitory effect of  the noradrenergic system on serotonin release (an {$\alpha$}2-adrenoceptor-mediated  effect). Electrophysiological experiments have demonstrated that mirtazapine  enhances serotonergic transmission through blockade of presynaptic  {$\alpha$}2-adrenoceptors. The drug does not inhibit serotonin reuptake. PHARMACOKINETIC  PROPERTIES: The bioavailability of mirtazapine is approximately 50\%. Peak plasma  concentrations are reached within 2.2 to 3.1 hours after single oral doses of 15  to 75mg and are dose-dependent. Mirtazapine is extensively metabolised in the  liver; up to 85\% of the drug is eliminated in the urine (up to 4\% as unchanged  drug) and the remaining 15\% is eliminated in the faeces. The mean elimination  half-life of mirtazapine is approximately 22 hours, making it suitable for  once-daily administration. THERAPEUTIC POTENTIAL: In randomised double-blind  comparative trials including patients with major depression, short term (5 to 6  weeks) therapy with mirtazapine was significantly more effective than placebo, as  effective as amitriptyline, clomipramine and doxepin, and at least as effective  as trazodone. Results from a meta-analysis of 5 comparative trials in which 60\%  of patients were hospitalised with severe depression [mean baseline 17-item  Hamilton Depression Rating Scale (HAMD) score {$\geq$}25] revealed no significant  differences between mirtazapine and amitriptyline. The responder rates ({$\geq$}50\%  decrease in HAMD score from baseline) at 6 weeks and study end-point were 70 and  61 \%, respectively, for mirtazapine and 73 and 64\%, respectively, for  amitriptyline. In a comparative trial in older outpatients (mean age 61 to 63  years), reductions in rating scale scores of depression and the percentage of  responders tended to be higher in mirtazapine than in trazodone recipients.  TOLERABILITY: The tolerability profile of mirtazapine is based on results from  short term (5 to 6 weeks) comparisons with placebo and other antidepressants; no  longer term data are available. Drowsiness (23 vs 14\%), excessive sedation (19 vs  5\%), dry mouth (25 vs 16\%), increased appetite (11 vs 2\%) and bodyweight gain (10  vs 1\%) occurred significantly more frequently with mirtazapine in  placebo-controlled trials. Analysis of blood pressure, heart rate and symptoms of  sexual dysfunction indicated no significant differences between mirtazapine and  placebo recipients. In a meta-analysis, mirtazapine appeared to be better  tolerated than amitriptyline, with significantly fewer patients experiencing  anticholinergic (dry mouth, constipation, and abnormal accommodation and vision),  cardiac (palpitations and tachycardia) and neurological (tremor and vertigo)  adverse events. Mirtazapine was at least as well tolerated as clomipramine,  doxepin and trazodone in comparative trials and appeared to be associated with  slightly lower incidences of anticholinergic and neurological adverse events than  these drugs. Clinical trial and postmarketing surveillance data suggest that  mirtazapine has a very low potential for inducing seizures. Excessive but  transient somnolence was the only symptom noted in 10 patients taking an overdose  (up to 315mg) of mirtazapine. Mirtazapine is infrequently associated with  clinically relevant changes in laboratory parameters. Granulocytopenia and  elevated alanine aminotransferase levels have been reported; most were mild in  severity and returned to normal values with continued administration of  mirtazapine. Elevated cholesterol levels (mean 3 to 4\%) have also been reported.  DOSAGE AND ADMINISTRATION: The recommended starting dosage of mirtazapine is 15  mg/day for 4 days, then 30 mg/day for 10 days. If effective, the drug should be  continued unchanged at this dosage or, in patients assessed as insufficiently  improved, the daily dosage may be further increased to 45 mg/day. In patients  with hepatic or renal insufficiency, careful dosage titration as well as regular  and close monitoring for adverse events is recommended. Concomitant use of  mirtazapine and diazepam or alcohol (ethanol) may also impair cognitive and/or  motor performance.},
  langid = {english},
  pmid = {26071050}
}

@article{dcruzPotentialClinicalApplications2022,
  title = {Potential Clinical Applications of {{Ashwagandha}} ({{Withania}} Somnifera) in Medicine and Neuropsychiatry},
  author = {D'Cruz, M. and Andrade, C.},
  year = {2022},
  journal = {Expert Review of Clinical Pharmacology},
  volume = {15},
  number = {9},
  pages = {1067--1080},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/17512433.2022.2121699}
}

@article{deamorimResearchScientificEvolution2018,
  title = {Research on the Scientific Evolution of the Flavonoid Agathisflavone},
  author = {{de Amorim}, V.C.M. and J{\'u}nior, M.S.O. and Bastos, E.M.S. and Da Silva, V.D.A. and Costa, S.L.},
  year = {2018},
  journal = {Journal of Pharmacy and Pharmaceutical Sciences},
  volume = {21},
  number = {1},
  pages = {376--385},
  publisher = {Canadian Society for Pharmaceutical Sciences},
  doi = {10.18433/jpps30103}
}

@misc{debruine_faux,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/zenodo.7852893},
  urldate = {2025-07-18},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  howpublished = {Zenodo},
  file = {/Users/cristian/Zotero/storage/2Y2DFW4A/7852893.html}
}

@article{debruine_mixed_effects,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2025-07-06},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/NKHU7R9A/DeBruine and Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@misc{debruineFauxSimulationFactorial2021,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2021},
  month = sep,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2021-11-09},
  abstract = {Create datasets with factorial designs and single-level or multi-level structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english},
  keywords = {data,R,simulation}
}

@misc{debruinePapercheckCheckScientific2025,
  title = {Papercheck: {{Check Scientific Papers}} for {{Best Practices}}},
  author = {DeBruine, Lisa and Lakens, Dani{\"e}l},
  year = {2025},
  urldate = {2025-03-23},
  abstract = {A modular, extendable system for automatically checking scientific papers for best practices using text search, R code, and/or generative AI queries.},
  file = {/Users/cristian/Zotero/storage/3FAV8HXB/papercheck.html}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2025-09-01},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/DB5FNBWB/DeBruine and Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@misc{debruineWithinsubjectTtestForensics2021,
  title = {Within-Subject t-Test Forensics},
  author = {DeBruine, Lisa},
  year = {2021},
  abstract = {Check the possible p-values and t-values for the given N, means and SDs for correlations between -1 and +1. Given a t-value or p-value, get the possible correlations for one- and two-tailed tests.}
}

@article{decosterOpportunisticBiasesTheir2015,
  title = {Opportunistic Biases: {{Their}} Origins, Effects, and an Integrated Solution},
  shorttitle = {Opportunistic Biases},
  author = {DeCoster, Jamie and Sparks, Erin A. and Sparks, Jordan C. and Sparks, Glenn G. and Sparks, Cheri W.},
  year = {2015},
  month = sep,
  journal = {The American Psychologist},
  volume = {70},
  number = {6},
  pages = {499--514},
  issn = {1935-990X},
  doi = {10.1037/a0039191},
  abstract = {Researchers commonly explore their data in multiple ways before deciding which analyses they will include in the final versions of their papers. While this improves the chances of researchers finding publishable results, it introduces an "opportunistic bias," such that the reported relations are stronger or otherwise more supportive of the researcher's theories than they would be without the exploratory process. The magnitudes of opportunistic biases can often be stronger than those of the effects being investigated, leading to invalid conclusions and a lack of clarity in research results. Authors typically do not report their exploratory procedures, so opportunistic biases are very difficult to detect just by reading the final version of a research report. In this article, we explain how a number of accepted research practices can lead to opportunistic biases, discuss the prevalence of these practices in psychology, consider the different effects that opportunistic biases have on psychological science, evaluate the strategies that methodologists have proposed to prevent or correct for the effects of these biases, and introduce an integrated solution to reduce the prevalence and influence of opportunistic biases. The recent prominence of articles discussing questionable research practices both in scientific journals and in the public media underscores the importance of understanding how opportunistic biases are created and how we might undo their effects.},
  langid = {english},
  pmid = {26348333},
  keywords = {{Data Interpretation, Statistical},Humans,Psychology,Reproducibility of Results,Statistics as Topic}
}

@article{deichmannInteractionStatinsExercise2015,
  title = {The Interaction between Statins and Exercise: {{Mechanisms}} and Strategies to Counter the Musculoskeletal Side Effects of This Combination Therapy},
  author = {Deichmann, R.E. and Lavie, C.J. and Asher, T. and DiNicolantonio, J.J. and O'Keefe, J.H. and Thompson, P.D.},
  year = {2015},
  journal = {Ochsner Journal},
  volume = {15},
  number = {4},
  pages = {429--437},
  publisher = {Ochsner Clinic}
}

@misc{delacreWhyHedgesBased2021,
  title = {Why {{Hedges}}' G*s Based on the Non-Pooled Standard Deviation Should Be Reported with {{Welch}}'s t-Test},
  author = {Delacre, Marie and Lakens, Daniel and Ley, Christophe and Liu, Limin and Leys, Christophe},
  year = {2021},
  month = may,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/tu6mp},
  urldate = {2022-12-07},
  abstract = {Researchers are generally required to report and interpret effect sizes and associated confidence intervals. When comparing two independent groups, the most commonly used estimator of effect size is Cohen's ds where sample mean difference is divided by the pooled standard deviation. However, computing the pooled error term is not valid when both groups do not share common population variances. Furthermore, the assumption of equal population variances is unlikely in many psychological fields. Consequently, researchers shift to the use of Welch's t-test over Student's t-test in the context of hypothesis testing. Meanwhile, the question which effect size to report when equal variances are not assumed remains open. Based on Monte Carlo simulations, we compare Hedges' gs (i.e. Cohen's ds with correction for bias) to Glass's gs, Shieh's gs and Hedges' g\_s{\textasciicircum}*. Comparisons are made under normality as well as under realistic deviations from the assumptions of normality and equal variances. Although it is not directly related with Welch's t-test (unlike Shieh's gs), we recommend the use of Hedges' g\_s{\textasciicircum}* because it shows better properties than all other estimators. Practical recommendations, R package and Shiny App in order to compute effect size estimators and confidence intervals are provided.},
  langid = {american},
  keywords = {Effect size,Monte Carlo Simulations,Parametric Assumptions,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods},
  file = {/Users/cristian/Zotero/storage/JGPUSVC7/Delacre et al. - 2021 - Why Hedges gs based on the non-pooled standard d.pdf}
}

@article{delvecchioIncreaseMuscleForce2019,
  title = {The Increase in Muscle Force after 4 Weeks of Strength Training Is Mediated by Adaptations in Motor Unit Recruitment and Rate Coding},
  author = {Del Vecchio, Alessandro and Casolo, Andrea and Negro, Francesco and Scorcelletti, Matteo and Bazzucchi, Ilenia and Enoka, Roger and Felici, Francesco and Farina, Dario},
  year = {2019},
  journal = {The Journal of Physiology},
  volume = {597},
  number = {7},
  pages = {1873--1887},
  doi = {10.1113/JP277250},
  abstract = {Key points Previous studies have indicated that several weeks of strength training is sufficient to elicit significant adaptations in the neural drive sent to the muscles. There are few data, however, on the changes elicited by strength training in the recruitment and rate coding of motor units during voluntary contractions. We show for the first time that the discharge characteristics of motor units in the tibialis anterior muscle tracked across the intervention are changed by 4 weeks of strength training with isometric voluntary contractions. The specific adaptations included significant increases in motor unit discharge rate, decreases in the recruitment-threshold force of motor units and a similar input--output gain of the motor neurons. The findings suggest that the adaptations in motor unit function may be attributable to changes in synaptic input to the motor neuron pool or to adaptations in intrinsic motor neuron properties. Abstract The strength of a muscle typically begins to increase after only a few sessions of strength training. This increase is usually attributed to changes in the neural drive to muscle as a result of adaptations at the cortical or spinal level. We investigated the change in the discharge characteristics of large populations of longitudinally tracked motor units in tibialis anterior before and after 4 weeks of strength training the ankle-dorsiflexor muscles with isometric contractions. The adaptations exhibited by 14 individuals were compared with 14 control subjects. High-density electromyogram grids with 128 electrodes recorded the myoelectric activity during isometric ramp contractions to the target forces of 35\%, 50\% and 70\% of maximal voluntary force. The motor unit recruitment and derecruitment thresholds, discharge rate, interspike intervals and estimates of synaptic inputs to motor neurons were assessed. The normalized recruitment-threshold forces of the motor units were decreased after strength training (P {$<$} 0.05). Moreover, discharge rate increased by 3.3 {\textpm} 2.5 pps (average across subjects and motor units) during the plateau phase of the submaximal isometric contractions (P {$<$} 0.001). Discharge rates at recruitment and derecruitment were not modified by training (P {$<$} 0.05). The association between force and motor unit discharge rate during the ramp-phase of the contractions was also not altered by training (P {$<$} 0.05). These results demonstrate for the first time that the increase in muscle force after 4 weeks of strength training is the result of an increase in motor neuron output from the spinal cord to the muscle.},
  langid = {english},
  keywords = {Decomposition,EMG,Motor Units,Neural Adaptations,Resistance Training},
  file = {/Users/cristian/Zotero/storage/B248WYCK/Del Vecchio et al. - 2019 - The increase in muscle force after 4 weeks of stre.pdf;/Users/cristian/Zotero/storage/UTFH69PB/JP277250.html}
}

@article{deoliveiraSafetyBetaalanineSupplementation2023,
  title = {Safety of Beta-Alanine Supplementation in Humans: A Narrative Review},
  author = {{de Oliveira}, E.P. and Artioli, G.G. and Burini, R.C.},
  year = {2023},
  journal = {Sport Sciences for Health},
  publisher = {Springer-Verlag Italia s.r.l.},
  doi = {10.1007/s11332-023-01052-0}
}

@article{derondPublishPerishBane2005,
  title = {Publish or {{Perish}}: {{Bane}} or {{Boon}} of {{Academic Life}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {De Rond, Mark and Miller, Alan N.},
  year = {2005},
  month = dec,
  journal = {Journal of Management Inquiry},
  volume = {14},
  number = {4},
  pages = {321--329},
  publisher = {SAGE Publications Inc},
  issn = {1056-4926},
  doi = {10.1177/1056492605276850},
  urldate = {2022-02-23},
  abstract = {There are few more familiar aphorisms in the academic community than ``publish or perish.'' Venerated by many and dreaded by more, this phenomenon is the subject of the authors' essay. Here they consider the publish or perish principle that has come to characterize life at many business schools. They explain when and why it began and suggest reasons for its persistence. This exercise elicits questions that appear as relatively neglected but are integral to our profession, namely, the effect of publish or perish on the creativity, intellectual lives, morale, and psychological and emotional states of faculty.},
  langid = {english},
  keywords = {business schools,publish,research,tenure},
  file = {/Users/cristian/Zotero/storage/KBX82WW9/De Rond and Miller - 2005 - Publish or Perish Bane or Boon of Academic Life.pdf}
}

@article{dessingHumanMovementScience2015,
  title = {Human {{Movement Science}} Adopts {{Registered Reports}} for Hypothesis-Driven Research},
  author = {Dessing, Joost C. and Beek, Peter J.},
  year = {2015},
  month = dec,
  journal = {Human Movement Science},
  volume = {44},
  pages = {A1-2},
  issn = {1872-7646},
  doi = {10.1016/j.humov.2015.09.011},
  langid = {english},
  pmid = {26497619},
  keywords = {Biomedical Research,Humans,Movement,Netherlands,Periodicals as Topic,Publishing,Research Report,Science}
}

@misc{DetectingSelectionBias,
  title = {Detecting {{Selection Bias}} in {{Meta-Analyses}} with {{Multiple Outcomes}}: {{A Simulation Study}}: {{The Journal}} of {{Experimental Education}}: {{Vol}} 89 , {{No}} 1 - {{Get Access}}},
  urldate = {2025-05-14},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/00220973.2019.1582470},
  file = {/Users/cristian/Zotero/storage/NZ4BWIXL/00220973.2019.html}
}

@incollection{DeterminingSampleSize2021,
  title = {Determining the {{Sample Size}}},
  booktitle = {Statistical {{Issues}} in {{Drug Development}}},
  year = {2021},
  pages = {241--264},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119238614.ch13},
  urldate = {2025-08-22},
  abstract = {Clinical trials are expensive, whether the cost is counted in money or in human suffering, but they are capable of providing results which are extremely valuable, whether the value is measured in drug company profits or successful treatment of future patients. This chapter argues that sample size issues are sometimes over-stressed at expense of others in clinical trials. The usual point of view is that the sample size is the determined function of variability, statistical method, power, and difference sought. A very unsatisfactory feature of conventional approaches to sample size calculation is that there is no mention of cost. The chapter discusses that there are various Bayesian suggestions. It considers an alternative frequentist approach to going beyond the P-value, that of severity, an approach championed by Deborah Mayo. P-values are a perfectly reasonable way for scientists to communicate the results of a significance test even if decisions rather than inferences are the object.},
  chapter = {13},
  copyright = {{\copyright} 2021 John Wiley \& Sons Ltd.},
  isbn = {978-1-119-23861-4},
  langid = {english},
  keywords = {Bayesian approach,clinical trials,frequentist approach,P-value,sample size issues},
  file = {/Users/cristian/Zotero/storage/LM6GF7FH/9781119238614.html}
}

@article{devriesUsingSpecificityOverload2023,
  title = {Using the Specificity and Overload Principles to Prevent Sarcopenia, Falls and Fractures with Exercise},
  author = {Devries, M.C. and Giangregorio, L.},
  year = {2023},
  journal = {Bone},
  volume = {166},
  publisher = {Elsevier Inc.},
  doi = {10.1016/j.bone.2022.116573}
}

@article{diamondCaffeineAnalgesicAdjuvant1999,
  title = {Caffeine as an Analgesic Adjuvant in the Treatment of Headache},
  author = {Diamond, S.},
  year = {1999},
  journal = {Headache Quarterly},
  volume = {10},
  number = {2},
  pages = {119--125}
}

@article{diaz-laraEnhancementHighIntensityActions2016,
  title = {Enhancement of {{High-Intensity Actions}} and {{Physical Performance During}} a {{Simulated Brazilian Jiu-Jitsu Competition With}} a {{Moderate Dose}} of {{Caffeine}}},
  author = {{Diaz-Lara}, Francisco Javier and Del Coso, Juan and Portillo, Javier and Areces, Francisco and Garc{\'i}a, Jose Manuel and {Abi{\'a}n-Vic{\'e}n}, Javier},
  year = {2016},
  month = oct,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {11},
  number = {7},
  pages = {861--867},
  issn = {1555-0273},
  doi = {10.1123/ijspp.2015-0686},
  abstract = {CONTEXT: Although caffeine is one of the most commonly used substances in combat sports, information about its ergogenic effects on these disciplines is very limited. PURPOSE: To determine the effectiveness of ingesting a moderate dose of caffeine to enhance overall performance during a simulated Brazilian jiu-jitsu (BJJ) competition. METHODS: Fourteen elite BJJ athletes participated in a double-blind, placebo-controlled experimental design. In a random order, the athletes ingested either 3 mg/kg body mass of caffeine or a placebo (cellulose, 0 mg/kg) and performed 2 simulated BJJ combats (with 20 min rest between them), following official BJJ rules. Specific physical tests such as maximal handgrip dynamometry, maximal height during a countermovement jump, permanence during a maximal static-lift test, peak power in a bench-press exercise, and blood lactate concentration were measured at 3 specific times: before the first combat and immediately after the first and second combats. The combats were video-recorded to analyze fight actions. RESULTS: After the caffeine ingestion, participants spent more time in offensive actions in both combats and revealed higher blood lactate values (P {$<$} .05). Performance in all physical tests carried out before the first combat was enhanced with caffeine (P {$<$} .05), and some improvements remained after the first combat (eg, maximal static-lift test and bench-press exercise; P {$<$} .05). After the second combat, the values in all physical tests were similar between caffeine and placebo. CONCLUSIONS: Caffeine might be an effective ergogenic aid for improving intensity and physical performance during successive elite BJJ combats.},
  langid = {english},
  pmid = {26693858},
  keywords = {Adult,Athletic Performance,Caffeine,Central Nervous System Stimulants,Competitive Behavior,Double-Blind Method,ergogenic aids,exercise,Exercise Test,fight,Hand Strength,Humans,Lactic Acid,Martial Arts,stimulants}
}

@article{dienesObtainingEvidenceNo2021,
  title = {Obtaining {{Evidence}} for {{No Effect}}},
  author = {Dienes, Zoltan},
  year = {2021},
  month = sep,
  journal = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {28202},
  issn = {2474-7394},
  doi = {10.1525/collabra.28202},
  urldate = {2024-02-01},
  abstract = {Obtaining evidence that something does not exist requires knowing how big it would be were it to exist. Testing a theory that predicts an effect thus entails specifying the range of effect sizes consistent with the theory, in order to know when the evidence counts against the theory. Indeed, a theoretically relevant effect size must be specified for power calculations, equivalence testing, and Bayes factors in order that the inferential statistics test the theory. Specifying relevant effect sizes for power, or the equivalence region for equivalence testing, or the scale factor for Bayes factors, is necessary for many journal formats, such as registered reports, and should be necessary for all articles that use hypothesis testing. Yet there is little systematic advice on how to approach this problem. This article offers some principles and practical advice for specifying theoretically relevant effect sizes for hypothesis testing.},
  file = {/Users/cristian/Zotero/storage/CPUXU7MX/Dienes - 2021 - Obtaining Evidence for No Effect.pdf;/Users/cristian/Zotero/storage/UHXRJNSF/Obtaining-Evidence-for-No-Effect.html}
}

@book{dienesUnderstandingPsychologyScience2008,
  title = {Understanding Psychology as a Science: {{An}} Introduction to Scientific and Statistical Inference},
  shorttitle = {Understanding Psychology as a Science},
  author = {Dienes, Zoltan},
  year = {2008},
  publisher = {Macmillan International Higher Education},
  file = {/Users/cristian/Zotero/storage/WFANYXJ9/books.html}
}

@article{difrancisco-donoghueEffectPedalPump2022,
  title = {The {{Effect}} of {{Pedal Pump Lymphatic Technique Versus Passive Recovery Following Maximal Exercise}}: {{A Randomized Cross-Over Trial}}},
  shorttitle = {The {{Effect}} of {{Pedal Pump Lymphatic Technique Versus Passive Recovery Following Maximal Exercise}}},
  author = {{DiFrancisco-Donoghue}, Joanne and Chan, Thomas and Jensen, Alexandra S. and Docherty, James E. B. and Grohman, Rebecca and Yao, Sheldon C.},
  year = {2022},
  month = jan,
  journal = {Sports Medicine - Open},
  volume = {8},
  number = {1},
  pages = {8},
  issn = {2199-1170},
  doi = {10.1186/s40798-021-00402-x},
  abstract = {CONTEXT: Muscle damage and delayed onset muscle soreness (DOMS) can occur following intense exercise. Various modalities have been studied to improve blood lactate accumulation, which is a primary reason for DOMS. It has been well established that active recovery facilitates blood lactate removal more rapidly that passive recovery due to the pumping action of the muscle. The pedal pump is a manual lymphatic technique used in osteopathic manipulative medicine to increase lymphatic drainage throughout the body. Pedal pump has been shown to increase lymphatic flow and improve immunity. This may improve circulation and improve clearance of metabolites post-exercise. OBJECTIVE: This study compared the use of pedal pump lymphatic technique to passive supine recovery following maximal exercise. METHODS: 17 subjects (male n\,=\,10, age 23\,{\textpm}\,3.01; female n\,=\,7, age 24\,{\textpm}\,1.8), performed a maximal volume O2 test (VO2 max) using a Bruce protocol, followed by a recovery protocol using either pedal pump technique or supine passive rest for 10~min, followed by sitting for 10~min. Outcome measures included blood lactate concentration (BL), heart rate (HR), systolic blood pressure (SBP) and VO2. Subjects returned on another day to repeat the VO2 max test to perform the other recovery protocol. All outcomes were measured at rest, within 1- minute post-peak exercise, and at minutes 4, 7, 10 and 20 of the recovery protocols. A 2\,{\texttimes}\,6 repeated measures ANOVA was used to compare outcome measures (p\,{$\leq$}\,0.05). RESULTS: No significant differences were found in VO2, HR, or SBP between any of the recovery protocols. There was no significant difference in BL concentrations for recovery at minutes 4, 7, or 10 (p\,{$>$}\,0.05). However, the pedal pump recovery displayed significantly lower BL concentrations at minute 20 of recovery (p\,=\,0.04). CONCLUSION: The pedal pump significantly decreased blood lactate concentrations following intense exercise at recovery minute 20. The use of manual lymphatic techniques in exercise recovery should be investigated further.},
  langid = {english},
  pmcid = {PMC8761194},
  pmid = {35032224},
  keywords = {OMM,Oxygen uptake,Pedal pump,Recovery},
  file = {/Users/cristian/Zotero/storage/N8YPUPPQ/DiFrancisco-Donoghue et al. - 2022 - The Effect of Pedal Pump Lymphatic Technique Versu.pdf}
}

@article{dimentEffectDailyMixed2012,
  title = {Effect of Daily Mixed Nutritional Supplementation on Immune Indices in Soldiers Undertaking an 8-Week Arduous Training Programme},
  author = {Diment, Bethany C. and Fortes, Matthew B. and Greeves, Julie P. and Casey, Anna and Costa, Ricardo J. S. and Walters, Robert and Walsh, Neil P.},
  year = {2012},
  month = apr,
  journal = {European Journal of Applied Physiology},
  volume = {112},
  number = {4},
  pages = {1411--1418},
  issn = {1439-6327},
  doi = {10.1007/s00421-011-2096-8},
  urldate = {2023-07-20},
  abstract = {The aim was to investigate the influence of a daily mixed nutritional supplement during an 8-week arduous training programme on immune indices and mediators including circulating leucocyte counts; bacterially stimulated neutrophil degranulation; interleukin-6 (IL-6), cortisol and saliva secretory immunoglobulin-A (SIgA). Thirty men (mean (SD): age 25 (3)~years; body mass, 80.9 (7.7)~kg) received a habitual diet (CON, n~=~15) or received a habitual diet plus an additional food supplement (SUP, n~=~15). From weeks 0--6, CON received 14.0~MJ~day-1 and SUP received 19.7~MJ~day-1, and during a final 2-week field exercise in weeks 7 and 8, CON received 17.7~MJ~day-1 and SUP received 21.3~MJ~day-1. Blood and saliva were taken at rest after an overnight fast at weeks 0, 6 and 8. Body mass loss over the 8 weeks was greater in CON (CON, 5.0 (2.3); SUP, 1.6 (1.5)~kg: P~{$<~$}0.001). Training-induced decreases in circulating total leucocytes (CON: weeks 0, 8.0 (2.1); weeks 8, 6.5 (1.6) 109~l-1, P~{$<~$}0.01), lymphocytes (21\%, P~{$<~$}0.01) and monocytes (20\%, P~{$<~$}0.01) were prevented by the nutritional supplement. Saliva SIgA secretion rate increased approximately twofold by week 8 in SUP (P~{$<~$}0.01) and was greater at week 8 compared with CON (P~{$<~$}0.01). Circulating neutrophils, bacterially stimulated neutrophil degranulation, IL-6 and cortisol were similar in CON and SUP at week 8. In conclusion, a daily mixed nutritional supplement prevented the decrease in circulating total leucocytes, lymphocytes and monocytes and increased saliva SIgA output during an 8-week arduous training programme. The increase in saliva SIgA with nutritional supplementation during training may reduce susceptibility to upper respiratory infection.},
  langid = {english},
  keywords = {Cortisol,Exercise,IgA,Immune,Infection,Military,Neutrophil,Nutrition,Saliva,Training},
  file = {/Users/cristian/Zotero/storage/LKMJMFM7/Diment et al. - 2012 - Effect of daily mixed nutritional supplementation .pdf}
}

@article{dimitrovPretestposttestDesignsMeasurement2003,
  title = {Pretest-Posttest Designs and Measurement of Change},
  author = {Dimitrov, Dimiter M. and Rumrill, Phillip D.},
  year = {2003},
  journal = {Work},
  volume = {20},
  number = {2},
  pages = {159--165},
  issn = {1051-9815},
  abstract = {The article examines issues involved in comparing groups and measuring change with pretest and posttest data. Different pretest-posttest designs are presented in a manner that can help rehabilitation professionals to better understand and determine effects resulting from selected interventions. The reliability of gain scores in pretest-posttest measurement is also discussed in the context of rehabilitation research and practice.},
  langid = {english},
  pmid = {12671209},
  keywords = {{Data Interpretation, Statistical},Analysis of Variance,Humans,Rehabilitation,Research Design,Treatment Outcome,United States}
}

@misc{ditroiloExploratoryResearchSport2024,
  title = {Exploratory {{Research}} in {{Sport}} and {{Exercise Science}}},
  author = {Ditroilo, Massimiliano and Mesquida, Cristian and Abt, Grant and Lakens, Dani{\"e}l},
  year = {2024},
  month = sep,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.457},
  urldate = {2025-03-22},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {confirmatory,data analysis,error control,hypothesis testing,questionable research practices,theory driven analysis},
  file = {/Users/cristian/Zotero/storage/YQGHYYX7/Ditroilo et al. - 2024 - Exploratory Research in Sport and Exercise Science.pdf}
}

@article{ditroiloExploratoryResearchSport2025,
  title = {Exploratory Research in Sport and Exercise Science: {{Perceptions}}, Challenges, and Recommendations},
  shorttitle = {Exploratory Research in Sport and Exercise Science},
  author = {Ditroilo, Massimiliano and Mesquida, Cristian and Abt, Grant and Lakens, Dani{\"e}l},
  year = {2025},
  month = apr,
  journal = {Journal of Sports Sciences},
  pages = {1--13},
  issn = {1466-447X},
  doi = {10.1080/02640414.2025.2486871},
  abstract = {Quantitative exploratory research implies a flexible examination of a dataset with the purpose of finding patterns, associations, and interactions between variables to help formulate a hypothesis, which should be severely tested in a future confirmatory study. In many fields, including sport and exercise science, exploratory research is not openly reported, a practice that leads to serious problems. At the same time, exploration is a crucial step in scientific knowledge generation, and a substantial proportion of studies will be exploratory in nature, or include both confirmatory and exploratory analyses. Using a flowchart, we review how data are typically collected and used, and we distinguish exploratory from confirmatory studies by arguing that data-driven analyses, where the Type I and Type II error cannot be controlled, is what characterises exploratory research. We ask which factors increase the quality and value of exploratory analyses, and highlight large sample sizes, uncommon sample compositions, rigorous data collection, widely used measures, observing a logical and coherent pattern across multiple variables, and the potential for generating new research questions as the main factors. Finally, we provide guidelines for carrying out and transparently writing up an exploratory study.},
  langid = {english},
  pmid = {40197233},
  keywords = {confirmatory,data analysis,error control,Hypothesis testing,questionable research practices,theory-driven analysis},
  file = {/Users/cristian/Zotero/storage/2EHKHQJC/Ditroilo et al. - 2025 - Exploratory research in sport and exercise science.pdf}
}

@article{dmitrienkoTraditionalMultiplicityAdjustment2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino Sr, Ralph},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {1097-0258},
  doi = {10.1002/sim.5990},
  urldate = {2023-07-06},
  abstract = {This tutorial discusses important statistical problems arising in clinical trials with multiple clinical objectives based on different clinical variables, evaluation of several doses or regiments of a new treatment, analysis of multiple patient subgroups, etc. Simultaneous assessment of several objectives in a single trial gives rise to multiplicity. If unaddressed, problems of multiplicity can undermine integrity of statistical inferences. The tutorial reviews key concepts in multiple hypothesis testing and introduces main classes of methods for addressing multiplicity in a clinical trial setting. General guidelines for the development of relevant and efficient multiple testing procedures are presented on the basis of application-specific clinical and statistical information. Case studies with common multiplicity problems are used to motivate and illustrate the statistical methods presented in the tutorial, and software implementation of the multiplicity adjustment methods is discussed. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {clinical trials,multiple testing procedures,multiplicity adjustments,multiplicity problems,type I error rate},
  file = {/Users/cristian/Zotero/storage/RV4CJZ7L/Dmitrienko and D'Agostino Sr - 2013 - Traditional multiplicity adjustment methods in cli.pdf;/Users/cristian/Zotero/storage/V96WMWUV/sim.html}
}

@article{doevenPostmatchRecoveryPhysical2018,
  title = {Postmatch Recovery of Physical Performance and Biochemical Markers in Team Ball Sports: {{A}} Systematic Review},
  author = {Doeven, S.H. and Brink, M.S. and Kosse, S.J. and Lemmink, K.A.P.M.},
  year = {2018},
  journal = {BMJ Open Sport and Exercise Medicine},
  volume = {4},
  number = {1},
  publisher = {BMJ Publishing Group},
  doi = {10.1136/bmjsem-2017-000264}
}

@article{draijerDiagnosticAccuracyFibrosis2021,
  title = {Diagnostic Accuracy of Fibrosis Tests in Children with Non-Alcoholic Fatty Liver Disease: {{A}} Systematic Review},
  author = {Draijer, L.G. and {van Oosterhout}, J.P.M. and Vali, Y. and Zwetsloot, S. and {van der Lee}, J.H. and {van Etten-Jamaludin}, F.S. and Chegary, M. and Benninga, M.A. and Koot, B.G.P.},
  year = {2021},
  journal = {Liver International},
  volume = {41},
  number = {9},
  pages = {2087--2100},
  publisher = {{John Wiley and Sons Inc}},
  doi = {10.1111/liv.14908}
}

@article{duarteLowerLimbsWearable2022,
  title = {Lower {{Limbs Wearable Sports Garments}} for {{Muscle Recovery}}: {{An Umbrella Review}}},
  author = {Duarte, J.P. and Fernandes, R.J. and Silva, G. and Sousa, F. and Machado, L. and Pereira, J.R. and {Vilas-Boas}, J.P.},
  year = {2022},
  journal = {Healthcare (Switzerland)},
  volume = {10},
  number = {8},
  publisher = {MDPI},
  doi = {10.3390/healthcare10081552}
}

@book{dubinTheoryBuilding1970,
  title = {Theory Building},
  author = {Dubin, Robert},
  year = {1970},
  volume = {31},
  publisher = {New York, Free Press},
  file = {/Users/cristian/Zotero/storage/2JTSQPQW/DUBTB-2.html;/Users/cristian/Zotero/storage/CZP5AMD3/link.html}
}

@article{dunbarTreatmentXlinkedCreatine2014,
  title = {Treatment of {{X-linked}} Creatine Transporter ({{SLC6A8}}) Deficiency: {{Systematic}} Review of the Literature and Three New Cases},
  author = {Dunbar, M. and Jaggumantri, S. and Sargent, M. and {Stockler-Ipsiroglu}, S. and {van Karnebeek}, C.D.M.},
  year = {2014},
  journal = {Molecular Genetics and Metabolism},
  volume = {112},
  number = {4},
  pages = {259--274},
  publisher = {Academic Press Inc.},
  doi = {10.1016/j.ymgme.2014.05.011}
}

@article{durlakHowSelectCalculate2009,
  title = {How to {{Select}}, {{Calculate}}, and {{Interpret Effect Sizes}}},
  author = {Durlak, J. A.},
  year = {2009},
  month = oct,
  journal = {Journal of Pediatric Psychology},
  volume = {34},
  number = {9},
  pages = {917--928},
  issn = {0146-8693, 1465-735X},
  doi = {10.1093/jpepsy/jsp004},
  urldate = {2022-12-09},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/ZW6KS7AI/Durlak - 2009 - How to Select, Calculate, and Interpret Effect Siz.pdf}
}

@article{earpReplicationFalsificationCrisis2015,
  title = {Replication, Falsification, and the Crisis of Confidence in Social Psychology},
  author = {Earp, Brian D. and Trafimow, David},
  year = {2015},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {6},
  pages = {621},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00621},
  urldate = {2022-12-22},
  abstract = {The (latest) crisis in confidence in social psychology has generated much heated discussion about the importance of replication, including how it should be carried out as well as interpreted by scholars in the field. For example, what does it mean if a replication attempt ``fails''---does it mean that the original results, or the theory that predicted them, have been falsified? And how should ``failed'' replications affect our belief in the validity of the original research? In this paper, we consider the replication debate from a historical and philosophical perspective, and provide a conceptual analysis of both replication and falsification as they pertain to this important discussion. Along the way, we highlight the importance of auxiliary assumptions (for both testing theories and attempting replications), and introduce a Bayesian framework for assessing ``failed'' replications in terms of how they should affect our confidence in original findings.},
  pmcid = {PMC4436798},
  pmid = {26042061},
  file = {/Users/cristian/Zotero/storage/HECYD5AY/Earp and Trafimow - 2015 - Replication, falsification, and the crisis of conf.pdf}
}

@article{easterbrookPublicationBiasClinical1991,
  title = {Publication Bias in Clinical Research},
  author = {Easterbrook, P. J and Gopalan, R and Berlin, J. A and Matthews, D. R},
  year = {1991},
  month = apr,
  journal = {The Lancet},
  series = {Originally Published as {{Volume}} 1, {{Issue}} 8746},
  volume = {337},
  number = {8746},
  pages = {867--872},
  issn = {0140-6736},
  doi = {10.1016/0140-6736(91)90201-Y},
  urldate = {2025-06-07},
  abstract = {In a retrospective survey, 487 research projects approved by the Central Oxford Research Ethics Committee between 1984 and 1987, were studied for evidence of publication bias. As of May, 1990, 285 of the studies had been analysed by the investigators, and 52\% of these had been published. Studies with statistically significant results were more likely to be published than those finding no difference between the study groups (adjusted odds ratio [OR] 2{$\cdot$}32; 95\% confidence interval [Cl] 1{$\cdot$}25-4{$\cdot$}28). Studies with significant results were also more likely to lead to a greater number of publications and presentations and to be published in journals with a high citation impact factor. An increased likelihood of publication was also associated with a high rating by the investigator of the importance of the study results, and with increasing sample size. The tendency towards publication bias was greater with observational and laboratory-based experimental studies (OR=3{$\cdot$}79; 95\% CI=1{$\cdot$}47-9{$\cdot$}76) than with randomised clinical trials (OR=0{$\cdot$}84; 95\% CI=0{$\cdot$}34-2{$\cdot$}09). We have confirmed the presence of publication bias in a cohort of clinical research studies. These findings suggest that conclusions based only on a review of published data should be interpreted cautiously, especially for observational studies. Improved strategies are needed to identify the results of unpublished as well as published studies.},
  file = {/Users/cristian/Zotero/storage/AWZSGV9V/014067369190201Y.html}
}

@article{ebersoleManyLabsEvaluating2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  urldate = {2021-02-16},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences---conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology},
  file = {/Users/cristian/Zotero/storage/9XVJJNV2/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf;/Users/cristian/Zotero/storage/5YBRBSDT/S0022103115300123.html}
}

@article{ebersoleManyLabsTesting2020,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and {Bart-Plange}, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarevi{\'c}, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban{\'i}k, Gabriel and Baskin, Ernest and Belopavlovi{\'c}, Radomir and Bernstein, Michael H. and Bia{\l}ek, Micha{\l} and Bloxsom, Nicholas G. and Bodro{\v z}a, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br{\"u}hlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and {\v C}oli{\'c}, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc{\~a}o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M{\'a}ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha{\l}asa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H{\"u}ffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and {Jim{\'e}nez-Leal}, William and Johannesson, Magnus and {Joy-Gaba}, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko{\l}odziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and {de Lima}, Tiago Jess{\'e} Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, {\L}ukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa{\l} and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orli{\'c}, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedovi{\'c}, Ivana and P{\k e}kala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrovi{\'c}, Boban and Pfeiffer, Thomas and Pie{\'n}kosz, Damian and Preti, Emanuele and Puri{\'c}, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys{\l}aw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and {Schulz-Hardt}, Stefan and Sch{\"u}tz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R{\'u}ben and Sioma, Barbara and Skorb, Lauren and {de Souza}, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilovi{\'c}, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz{\"o}ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and {\v Z}e{\v z}elj, Iris and Zrubka, Mark and Nosek, Brian A.},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  urldate = {2021-02-16},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3--9; median total sample = 1,279.5, range = 276--3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols ({$\Delta$}r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00--.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19--.50).},
  langid = {english},
  keywords = {metascience,open data,peer review,preregistered,Registered Reports,replication,reproducibility},
  file = {/Users/cristian/Zotero/storage/FLS8YEZD/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf}
}

@article{eckertLetterEditorDoublecounting2023,
  title = {Letter to the {{Editor}}: {{Double-counting}} Due to Inadequate Statistics Leads to False-Positive Findings in "{{Effects}} of Creatine Supplementation on Memory in  Healthy Individuals: A Systematic Review and Meta-Analysis of Randomized  Controlled Trials".},
  author = {Eckert, Igor and Pascher, Eric},
  year = {2023},
  month = jan,
  journal = {Nutrition reviews},
  pages = {nuac108},
  address = {United States},
  issn = {1753-4887 0029-6643},
  doi = {10.1093/nutrit/nuac108},
  abstract = {Prokopidis et al have conducted a meta-analysis of randomized, placebo-controlled clinical trials to assess the effects of oral creatine supplementation on memory  performance of healthy individuals. However, concerns were raised regarding the  validity of their statistical analyses, which may have led to misleading  conclusions. In this letter, we describe the statistical issue at hand and its  potential implications.},
  copyright = {{\copyright} The Author(s) 2023. Published by Oxford University Press on behalf of the International Life Sciences Institute. All rights reserved. For permissions,  please e-mail: journals.permissions@oup.com.},
  langid = {english},
  pmid = {36644917},
  keywords = {creatine,dietary supplements,memory,meta-analysis,statistical analysis}
}

@misc{edelsbrunnerImprovingUtilityNonSignificant2020,
  title = {Improving the {{Utility}} of {{Non-Significant Results}} for {{Educational Research}}},
  author = {Edelsbrunner, Peter and Thurn, Christian},
  year = {2020},
  month = apr,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/j93a2},
  urldate = {2022-09-06},
  abstract = {Non-significant results have the potential to further our understanding of what does not work in education, and why. We make three contributions to harness this potential and to improve the usage and interpretation of non-significant results. To evaluate current practices, we conduct a review of misinterpretations of non-significant p-values in recent educational research. The review indicates that over 90\% of non-significant results are erroneously interpreted as indicating the absence of an effect, or a difference compared to a significant effect. Researchers sometimes link these misinterpretations with potentially erroneous conclusions for educational theory, practice, or policy. To improve the status quo and make non-significant results more informative, we provide a detailed framework based on which researchers can design, conduct, and analyze studies that yield reliable evidence regarding the actual absence of an effect. In addition, we provide a competence model that researchers can use to guide their own research and teaching.},
  langid = {american},
  keywords = {competence model,Educational Psychology,equivalence testing,framework,misinterpretations,non-significant results,Quantitative Methods,Social and Behavioral Sciences},
  file = {/Users/cristian/Zotero/storage/F64LFTTZ/Edelsbrunner and Thurn - 2020 - Improving the Utility of Non-Significant Results f.pdf}
}

@article{edelsbrunnerImprovingUtilityNonSignificant2020a,
  title = {Improving the {{Utility}} of {{Non-Significant Results}} for {{Educational Research}}},
  author = {Edelsbrunner, Peter and Thurn, Christian},
  year = {2020},
  month = apr,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/j93a2},
  urldate = {2022-09-06},
  abstract = {Non-significant results have the potential to further our understanding of what does not work in education, and why. We make three contributions to harness this potential and to improve the usage and interpretation of non-significant results. To evaluate current practices, we conduct a review of misinterpretations of non-significant p-values in recent educational research. The review indicates that over 90\% of non-significant results are erroneously interpreted as indicating the absence of an effect, or a difference compared to a significant effect. Researchers sometimes link these misinterpretations with potentially erroneous conclusions for educational theory, practice, or policy. To improve the status quo and make non-significant results more informative, we provide a detailed framework based on which researchers can design, conduct, and analyze studies that yield reliable evidence regarding the actual absence of an effect. In addition, we provide a competence model that researchers can use to guide their own research and teaching.},
  langid = {american},
  keywords = {competence model,Educational Psychology,equivalence testing,framework,misinterpretations,non-significant results,Quantitative Methods,Social and Behavioral Sciences}
}

@article{editorsImpactFactorGame2006,
  title = {The {{Impact Factor Game}}},
  author = {Editors, The PloS Medicine},
  year = {2006},
  month = jun,
  journal = {PLOS Medicine},
  volume = {3},
  number = {6},
  pages = {e291},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0030291},
  urldate = {2021-08-12},
  abstract = {The PLoS Medicine editors argue that we need a better measure than the impact factor for assessing the biomedical literature.},
  langid = {english},
  keywords = {Bibliometrics,Citation analysis,Crops,Games,Health care policy,Medical journals,Scientific publishing,Scientists},
  file = {/Users/cristian/Zotero/storage/KDGR4IB2/Editors - 2006 - The Impact Factor Game.pdf;/Users/cristian/Zotero/storage/8UGEQXKN/article.html}
}

@misc{EffectDailyMixed,
  title = {Effect of Daily Mixed Nutritional Supplementation on Immune Indices in Soldiers Undertaking an 8-Week Arduous Training Programme {\textbar} {{SpringerLink}}},
  urldate = {2023-07-20},
  howpublished = {https://link.springer.com/article/10.1007/s00421-011-2096-8},
  file = {/Users/cristian/Zotero/storage/IH6XTASN/s00421-011-2096-8.html}
}

@incollection{EffectSizesBased2009i,
  title = {Effect {{Sizes Based}} on {{Correlations}}},
  booktitle = {Introduction to {{Meta-Analysis}}},
  author = {, Mich},
  year = {2009},
  pages = {41--43},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470743386.ch6},
  urldate = {2023-02-01},
  abstract = {This chapter contains sections titled: Introduction Computing r Other approaches Summary points},
  chapter = {6},
  isbn = {978-0-470-74338-6},
  langid = {english},
  keywords = {{$\rho$} denoting population parameter,computing r,Fisher's z scale,Hunter and Schmidt - working with correlations,summary effect},
  file = {/Users/cristian/Zotero/storage/LDR3FFIN/9780470743386.html}
}

@article{eggerBiasMetaanalysisDetected1997,
  title = {Bias in Meta-Analysis Detected by a Simple, Graphical Test},
  author = {Egger, M. and Davey Smith, G. and Schneider, M. and Minder, C.},
  year = {1997},
  month = sep,
  journal = {BMJ (Clinical research ed.)},
  volume = {315},
  number = {7109},
  pages = {629--634},
  issn = {0959-8138},
  doi = {10.1136/bmj.315.7109.629},
  abstract = {OBJECTIVE: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. DESIGN: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. MAIN OUTCOME MEASURE: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. RESULTS: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. CONCLUSIONS: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.},
  langid = {english},
  pmcid = {PMC2127453},
  pmid = {9310563},
  keywords = {Bias,Meta-Analysis as Topic,Randomized Controlled Trials as Topic,Regression Analysis,Statistics as Topic,Treatment Outcome},
  file = {/Users/cristian/Zotero/storage/9S4UXGB7/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf}
}

@article{eggerMisleadingMetaanalysis1995,
  title = {Misleading Meta-Analysis},
  author = {Egger, M. and Smith, G. D.},
  year = {1995},
  month = mar,
  journal = {BMJ (Clinical research ed.)},
  volume = {310},
  number = {6982},
  pages = {752--754},
  issn = {0959-8138},
  doi = {10.1136/bmj.310.6982.752},
  langid = {english},
  pmcid = {PMC2549158},
  pmid = {7711568},
  keywords = {Clinical Trials as Topic,Humans,Magnesium,Meta-Analysis as Topic,Myocardial Infarction,Publication Bias,Randomized Controlled Trials as Topic,Retrospective Studies,Streptokinase},
  file = {/Users/cristian/Zotero/storage/42CGE98E/Egger and Smith - 1995 - Misleading meta-analysis.pdf}
}

@article{eggermontAdjuvantTherapyPegylated2020,
  title = {Adjuvant Therapy with Pegylated Interferon-Alfa2b vs Observation in Stage {{II B}}/{{C}} Patients with Ulcerated Primary: {{Results}} of the {{European Organisation}} for {{Research}} and {{Treatment}} of {{Cancer}} 18081 Randomised Trial},
  author = {Eggermont, A.M.M. and Rutkowski, P. and Dutriaux, C. and {Hofman-Wellenhof}, R. and Dziewulski, P. and Marples, M. and Grange, F. and Lok, C. and Pennachioli, E. and Robert, C. and {van Akkooi}, A.C.J. and Bastholt, L. and Minisini, A. and Marshall, E. and Sal{\`e}s, F. and Grob, J.-J. and Bechter, O. and Schadendorf, D. and Marreaud, S. and Kicinski, M. and Suciu, S. and Testori, A.A.E.},
  year = {2020},
  journal = {European Journal of Cancer},
  volume = {133},
  pages = {94--103},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.ejca.2020.04.015}
}

@article{eisenbergAmbiguityStrategyOrganizational1984,
  title = {Ambiguity as Strategy in Organizational Communication},
  author = {Eisenberg, Eric M.},
  year = {1984},
  month = sep,
  journal = {Communication Monographs},
  volume = {51},
  number = {3},
  pages = {227--242},
  publisher = {Routledge},
  issn = {0363-7751},
  doi = {10.1080/03637758409390197},
  urldate = {2022-10-23},
  abstract = {This paper argues that while most teachers, researchers, and practitioners of organizational communication encourage clarity, a critical examination of communication processes in organizations reveals that clarity is both non-normative and not a sensible standard against which to gauge individual or organizational effectiveness. People in organizations confront multiple situational requirements, develop multiple and often conflicting goals, and respond with communicative strategies which do not always minimize ambiguity, but are nonetheless effective. Strategic ambiguity is essential to organizing in that it: (1) promotes unified diversity, (2) facilitates organizational change, and (3) amplifies existing source attributions and preserves privileged positions.}
}

@article{eldibProbioticsTreatmentDepression2021,
  title = {Probiotics for the Treatment of Depression and Anxiety: {{A}} Systematic Review and Meta-Analysis of Randomized Controlled Trials},
  author = {El Dib, R. and Periyasamy, A.G. and {de Barros}, J.L. and Fran{\c c}a, C.G. and Senefonte, F.L. and Vesentini, G. and Alves, M.G.O. and Rodrigues, J.V.D.S. and Gomaa, H. and Gomes J{\'u}nior, J.R. and Costa, L.F. and Von Ancken, T.D.S. and Toneli, C. and Suzumura, E.A. and Kawakami, C.P. and Faustino, E.G. and Jorge, E.C. and Almeida, J.D. and Kapoor, A.},
  year = {2021},
  journal = {Clinical Nutrition ESPEN},
  volume = {45},
  pages = {75--90},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.clnesp.2021.07.027}
}

@article{elliottLivingSystematicReview2017,
  title = {Living Systematic Review: 1. {{Introduction}}---the Why, What, When, and How},
  shorttitle = {Living Systematic Review},
  author = {Elliott, Julian H. and Synnot, Anneliese and Turner, Tari and Simmonds, Mark and Akl, Elie A. and McDonald, Steve and Salanti, Georgia and Meerpohl, Joerg and MacLehose, Harriet and Hilton, John and Tovey, David and Shemilt, Ian and Thomas, James and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and {Kolakowsky-Hayner}, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"u}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie-Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and {Soares-Weiser}, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Mart{\'i}nez Garc{\'i}a, Laura and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and {van Valkenhoef}, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and {Noel-Storr}, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and Yepes Nu{\~n}ez, Juan Jos{\'e} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
  year = {2017},
  month = nov,
  journal = {Journal of Clinical Epidemiology},
  volume = {91},
  pages = {23--30},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2017.08.010},
  urldate = {2025-05-16},
  abstract = {Systematic reviews are difficult to keep up to date, but failure to do so leads to a decay in review currency, accuracy, and utility. We are developing a novel approach to systematic review updating termed ``Living systematic review'' (LSR): systematic reviews that are continually updated, incorporating relevant new evidence as it becomes available. LSRs may be particularly important in fields where research evidence is emerging rapidly, current evidence is uncertain, and new research may change policy or practice decisions. We hypothesize that a continual approach to updating will achieve greater currency and validity, and increase the benefits to end users, with feasible resource requirements over time.},
  keywords = {Evidence synthesis,Guidelines,Living guidelines,Living systematic review,Systematic review},
  file = {/Users/cristian/Zotero/storage/V7QRSSNR/Elliott et al. - 2017 - Living systematic review 1. Introductionthe why,.pdf;/Users/cristian/Zotero/storage/FBZYGS2Q/S0895435617306364.html}
}

@article{emmersonRecommendationsDesigningAnalysing2021,
  ids = {emmersonRecommendationsDesigningAnalysing2021a},
  title = {Recommendations for Designing and Analysing Multi-Arm Non-Inferiority Trials: A Review of Methodology and Current Practice},
  shorttitle = {Recommendations for Designing and Analysing Multi-Arm Non-Inferiority Trials},
  author = {Emmerson, Jake and Todd, Susan and Brown, Julia M.},
  year = {2021},
  month = jun,
  journal = {Trials},
  volume = {22},
  number = {1},
  pages = {417},
  issn = {1745-6215},
  doi = {10.1186/s13063-021-05364-9},
  urldate = {2024-08-26},
  abstract = {Multi-arm non-inferiority (MANI) trials, here defined as non-inferiority trials with multiple experimental treatment arms, can be useful in situations where several viable treatments exist for a disease area or for testing different dose schedules. To maintain the statistical integrity of such trials, issues regarding both design and analysis must be considered, from both the multi-arm and the non-inferiority perspectives. Little guidance currently exists on exactly how these aspects should be addressed and it is the aim of this paper to provide recommendations to aid the design of future MANI trials.},
  langid = {english},
  keywords = {Clinical trials,Family-wise error,Heterogeneous variances,Multi-arm,Multiple testing,Non-inferiority,Power,Simultaneous confidence intervals,Stepwise adjustment},
  file = {/Users/cristian/Zotero/storage/YM26V2QA/Emmerson et al. - 2021 - Recommendations for designing and analysing multi-.pdf}
}

@article{enokaTranslatingFatigueHuman2016,
  title = {Translating {{Fatigue}} to {{Human Performance}}},
  author = {Enoka, Roger M. and Duchateau, Jacques},
  year = {2016},
  month = nov,
  journal = {Medicine and Science in Sports and Exercise},
  volume = {48},
  number = {11},
  pages = {2228--2238},
  issn = {1530-0315},
  doi = {10.1249/MSS.0000000000000929},
  abstract = {Despite flourishing interest in the topic of fatigue-as indicated by the many presentations on fatigue at the 2015 Annual Meeting of the American College of Sports Medicine-surprisingly little is known about its effect on human performance. There are two main reasons for this dilemma: 1) the inability of current terminology to accommodate the scope of the conditions ascribed to fatigue, and 2) a paucity of validated experimental models. In contrast to current practice, a case is made for a unified definition of fatigue to facilitate its management in health and disease. On the basis of the classic two-domain concept of Mosso, fatigue is defined as a disabling symptom in which physical and cognitive function is limited by interactions between performance fatigability and perceived fatigability. As a symptom, fatigue can only be measured by self-report, quantified as either a trait characteristic or a state variable. One consequence of such a definition is that the word fatigue should not be preceded by an adjective (e.g., central, mental, muscle, peripheral, and supraspinal) to suggest the locus of the changes responsible for an observed level of fatigue. Rather, mechanistic studies should be performed with validated experimental models to identify the changes responsible for the reported fatigue. As indicated by three examples (walking endurance in old adults, time trials by endurance athletes, and fatigue in persons with multiple sclerosis) discussed in the review, however, it has proven challenging to develop valid experimental models of fatigue. The proposed framework provides a foundation to address the many gaps in knowledge of how laboratory measures of fatigue and fatigability affect real-world performance.},
  langid = {english},
  pmcid = {PMC5035715},
  pmid = {27015386},
  keywords = {Aging,Fatigue,Humans,Multiple Sclerosis,Muscle Fatigue,Perception,Physical Endurance,Sports,Terminology as Topic,Walking},
  file = {/Users/cristian/Zotero/storage/8TLY8AST/Enoka and Duchateau - 2016 - Translating Fatigue to Human Performance.pdf}
}

@misc{epskampStatcheckExtractStatistics2018,
  title = {Statcheck: {{Extract Statistics}} from {{Articles}} and {{Recompute}} p {{Values}}},
  shorttitle = {Statcheck},
  author = {Epskamp, Sascha and Nuijten, Mich{\`e}le B},
  year = {2018},
  month = may,
  urldate = {2022-10-10},
  abstract = {Extract statistics from articles and recompute p values.},
  copyright = {GPL-2}
}

@article{erringtonChallengesAssessingReplicability2021,
  title = {Challenges for Assessing Replicability in Preclinical Cancer Biology},
  author = {Errington, Timothy M. and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A.},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e67995},
  issn = {2050-084X},
  doi = {10.7554/eLife.67995},
  abstract = {We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68\% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41\% of experiments, they were minimally helpful for 9\% of experiments, and not at all helpful (or did not respond to us) for 32\% of experiments. Third, once experimental work started, 67\% of the peer-reviewed protocols required modifications to complete the research and just 41\% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication - it is hard to assess whether reported findings are credible.},
  langid = {english},
  pmcid = {PMC8651289},
  pmid = {34874008},
  keywords = {Animals,Biomedical Research,cancer biology,human,Humans,mouse,Neoplasms,open data,open science,preregistration,replication,reproducibility,Reproducibility of Results,Reproducibility Project: Cancer Biology,Research Design},
  file = {/Users/cristian/Zotero/storage/QPBTTPRS/Errington et al. - 2021 - Challenges for assessing replicability in preclini.pdf}
}

@article{erringtonInvestigatingReplicabilityPreclinical2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M. and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  doi = {10.7554/eLife.71601},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/Users/cristian/Zotero/storage/3EYQZQ85/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{erringtonOpenInvestigationReproducibility,
  title = {An Open Investigation of the Reproducibility of Cancer Biology Research},
  author = {Errington, Timothy M. and Iorns, Elizabeth and Gunn, William and Tan, Fraser Elisabeth and Lomax, Joelle and Nosek, Brian A},
  journal = {eLife},
  volume = {3},
  issn = {2050-084X},
  doi = {10.7554/eLife.04333},
  urldate = {2021-05-21},
  abstract = {It is widely believed that research that builds upon previously published findings has reproduced the original work. However, it is rare for researchers to perform or publish direct replications of existing results. The Reproducibility Project: Cancer Biology is an open investigation of reproducibility in preclinical cancer biology research. We have identified 50 high impact cancer biology articles published in the period 2010-2012, and plan to replicate a subset of experimental results from each article. A Registered Report detailing the proposed experimental designs and protocols for each subset of experiments will be peer reviewed and published prior to data collection. The results of these experiments will then be published in a Replication Study. The resulting open methodology and dataset will provide evidence about the reproducibility of high-impact results, and an opportunity to identify predictors of reproducibility., DOI: http://dx.doi.org/10.7554/eLife.04333.001},
  pmcid = {PMC4270077},
  pmid = {25490932},
  file = {/Users/cristian/Zotero/storage/DXF9L8Y6/Errington et al. - An open investigation of the reproducibility of ca.pdf}
}

@article{erringtonReproducibilityCancerBiology2017,
  title = {Reproducibility in Cancer Biology: {{The}} Challenges of Replication},
  author = {Errington, Timothy M. and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A.},
  year = {2017},
  month = jan,
  journal = {eLife},
  volume = {10},
  pages = {e67995},
  doi = {10.7554/eLife.23693},
  abstract = {Interpreting the first results from the Reproducibility Project: Cancer Biology requires a highly nuanced approach.},
  keywords = {metascience,methodology,open science,replication,reproducibility,Reproducibility Project: Cancer Biology}
}

@article{eslamiEffectsTwoDifferent2021,
  title = {The Effects of Two Different Intensities of Aerobic Training Protocols on Pain and Serum Neuro-Biomarkers in Women Migraineurs: A Randomized Controlled Trail},
  shorttitle = {The Effects of Two Different Intensities of Aerobic Training Protocols on Pain and Serum Neuro-Biomarkers in Women Migraineurs},
  author = {Eslami, Rasoul and Parnow, Abdolhossein and Pairo, Zahra and Nikolaidis, Pantelis and Knechtle, Beat},
  year = {2021},
  month = feb,
  journal = {European Journal of Applied Physiology},
  volume = {121},
  number = {2},
  pages = {609--620},
  issn = {1439-6327},
  doi = {10.1007/s00421-020-04551-x},
  abstract = {OBJECTIVES: We have a weak understanding of how aerobic training may influence migraine, and the optimal parameters for exercise regimens as migraine therapy are not clear. The objectives of this study were to assess, first, effects of two different intensities of aerobic exercise on migraine headache indices; second, serum neuro-biomarker in women migraineurs. METHODS: A total of 45 non-athlete female migraine patients were selected by a neurologist and randomly divided into three groups: control (CON), moderate-intensity aerobic training (MOD T), and high-intensity aerobic training (HIGH T). Before and after the training protocol, body composition factors, migraine pain indices, VO2max, and serum Adenylate-Cyclase Activating Polypeptide (PACAP) and Substance P (SP) were measured. Exercise training protocol includes two different intensities of aerobic exercise: Moderate (13-15 Borg Scale, 60-80\% HRmax) and High (15-17 Borg Scale, 65-95\% HRmax). RESULTS: Moderate-intensity aerobic training (MOD T) reduced headache intensity, frequency, and duration in women with migraine (p\,{$<$}\,0.001, for all). Also, high-intensity aerobic training (HIGH T) reduced headache intensity, frequency, and duration (p\,{$<$}\,0.001, for all). However, for headache intensity and duration, MOD T was effective rather than HIGH T (p\,{$<$}\,0.001; p\,{$\leq$}\,0.05, respectively). In addition, neither MOD T nor HIGH T could not alter PACAP and SP contents (p\,=\,0.712; p\,=\,0.249, respectively). CONCLUSIONS: Our results demonstrated that either MOD T or HIGH T could modify migraine pain indices but neither MOD T nor HIGH T could not alter the PACAP and SP contents in women with migraine.},
  langid = {english},
  pmid = {33206251},
  keywords = {Adenylate-cyclase activating polypeptide (PACAP),Adult,Aerobic training,Biomarkers,Exercise,Female,Humans,Migraine,Migraine Disorders,Oxygen,Pain,Pituitary Adenylate Cyclase-Activating Polypeptide,Substance P (SP)},
  file = {/Users/cristian/Zotero/storage/65VQGNTB/Eslami et al. - 2021 - The effects of two different intensities of aerobi.pdf}
}

@misc{EstimandsFrameworkPrimer,
  title = {The Estimands Framework: A Primer on the {{ICH E9}}({{R1}}) Addendum {\textbar} {{The BMJ}}},
  urldate = {2025-03-20},
  howpublished = {https://www.bmj.com/content/384/bmj-2023-076316.abstract},
  file = {/Users/cristian/Zotero/storage/4RWRZJW4/bmj-2023-076316.html}
}

@article{etxebarriaRunningYourBest2021,
  title = {Running {{Your Best Triathlon Race}}},
  author = {Etxebarria, Naroa and Wright, Jackson and Jeacocke, Hamish and Mesquida, Cristian and Pyne, David B.},
  year = {2021},
  month = feb,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {16},
  number = {5},
  pages = {744--747},
  publisher = {Human Kinetics},
  issn = {1555-0273, 1555-0265},
  doi = {10.1123/ijspp.2020-0838},
  urldate = {2025-03-29},
  abstract = {Negative or evenly paced racing strategies often lead to more favorable performance outcomes for endurance athletes. However, casual inspection of race split times and observational studies both indicate that elite triathletes competing in Olympic-distance triathlon typically implement a positive pacing strategy during the last of the 3 disciplines, the 10-km run. To address this apparent contradiction, the authors examined data from 14 International Triathlon Union elite races over 3 consecutive years involving a total of 725 male athletes. Analyses of race results confirm that triathletes typically implement a positive running pace strategy, running the first lap of the standard 4-lap circuit substantially faster than laps 2 ({$\sim$}7\%), 3 ({$\sim$}9\%), and 4 ({$\sim$}12\%). Interestingly, mean running pace in lap 1 had a substantially lower correlation with 10-km run time (r\,=\,.82) than both laps 2 and 3. Overall triathlon race performance (ranking) was best associated with run performance (r\,=\,.82) compared with the swim and cycle sections. Lower variability in race pace during the 10-km run was also reflective of more successful run times. Given that overall race outcome is mainly explained by the 10-km run performance, with top run performances associated with a more evenly paced strategy, triathletes (and their coaches) should reevaluate their pacing strategy during the run section.},
  chapter = {International Journal of Sports Physiology and Performance},
  langid = {english},
  keywords = {endurance performance,individualized racing,Olympic distance,pacing strategy},
  file = {/Users/cristian/Zotero/storage/47U8T757/Etxebarria et al. - 2021 - Running Your Best Triathlon Race.pdf}
}

@techreport{europeanmedicalagencyGuidelineMultiplicityIssues2017,
  title = {Guideline on Multiplicity Issues in Clinical Trials},
  author = {European Medical Agency},
  year = {2017}
}

@article{falagasComparisonSCImagoJournal2008,
  title = {Comparison of {{SCImago}} Journal Rank Indicator with Journal Impact Factor},
  author = {Falagas, Matthew E. and Kouranos, Vasilios D. and {Arencibia-Jorge}, Ricardo and Karageorgopoulos, Drosos E.},
  year = {2008},
  journal = {The FASEB Journal},
  volume = {22},
  number = {8},
  pages = {2623--2628},
  issn = {1530-6860},
  doi = {10.1096/fj.08-107938},
  urldate = {2021-04-21},
  abstract = {The application of currently available sophisticated algorithms of citation analysis allows for the incorporation of the ``quality'' of citations in the evaluation of scientific journals. We sought to compare the newly introduced SCImago journal rank (SJR) indicator with the journal impact factor (IF). We retrieved relevant information from the official Web sites hosting the above indices and their source databases. The SJR indicator is an open-access resource, while the journal IF requires paid subscription. The SJR indicator (based on Scopus data) lists considerably more journal titles published in a wider variety of countries and languages, than the journal IF (based on Web of Science data). Both indices divide citations to a journal by articles of the journal, during a specific time period. However, contrary to the journal IF, the SJR indicator attributes different weight to citations depending on the ``prestige'' of the citing journal without the influence of journal self-citations; prestige is estimated with the application of the PageRank algorithm in the network of journals. In addition, the SJR indicator includes the total number of documents of a journal in the denominator of the relevant calculation, whereas the journal IF includes only ``citable'' articles (mainly original articles and reviews). A 3-yr period is analyzed in both indices but with the use of different approaches. Regarding the top 100 journals in the 2006 journal IF ranking order, the median absolute change in their ranking position with the use of the SJR indicator is 32 (1st quartile: 12; 3rd quartile: 75). Although further validation is warranted, the novel SJR indicator poses as a serious alternative to the well-established journal IF, mainly due to its openaccess nature, larger source database, and assessment of the quality of citations.---Falagas, M. E., Kouranos, V. D., Arencibia-Jorge, R., Karageorgopoulos, D. E. Comparison of SCImago journal rank indicator with journal impact factor. FASEB J. 22, 2623--2628 (2008)},
  copyright = {{\copyright} FASEB},
  langid = {english},
  keywords = {bibliographic databases,bibliometric analysis,mathematical computing,quality of publications,scientometrics},
  file = {/Users/cristian/Zotero/storage/CBFRCHLU/Falagas et al. - 2008 - Comparison of SCImago journal rank indicator with .pdf;/Users/cristian/Zotero/storage/M36HTU27/fj.html}
}

@article{fanelliHowManyScientists2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  journal = {PLoS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc{\dots} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86--4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once --a serious form of misconduct by any standard-- and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91--19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  langid = {english},
  keywords = {Deception,Medical journals,Medicine and health sciences,Metaanalysis,Scientific misconduct,Scientists,Social research,Surveys},
  file = {/Users/cristian/Zotero/storage/EAMMNDXU/Fanelli - 2009 - How Many Scientists Fabricate and Falsify Research.pdf;/Users/cristian/Zotero/storage/HD4BWUTE/article.html}
}

@article{fanelliNegativeResultsAre2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  author = {Fanelli, Daniele},
  year = {2012},
  month = mar,
  journal = {Scientometrics},
  volume = {90},
  number = {3},
  pages = {891--904},
  issn = {1588-2861},
  doi = {10.1007/s11192-011-0494-7},
  urldate = {2021-04-06},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ``tested'' a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/9585GGQH/Fanelli - 2012 - Negative results are disappearing from most discip.pdf}
}

@article{fanelliPositiveResultsIncrease2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PloS One},
  volume = {5},
  number = {4},
  pages = {e10068},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research---i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors---is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  langid = {english},
  keywords = {Forecasting,Mental health and psychiatry,Physical sciences,Scientists,Social psychology,Social research,Social sciences,Sociology},
  annotation = {https://doi.org/10.1371\\
/journal.pone.0010068},
  file = {/Users/cristian/Zotero/storage/957T3F3R/Fanelli - 2010 - Positive Results Increase Down the Hierarchy of .pdf;/Users/cristian/Zotero/storage/3TEU8EHY/article.html}
}

@article{fanelliResearchersIndividualPublication2016,
  title = {Researchers' {{Individual Publication Rate Has Not Increased}} in a {{Century}}},
  author = {Fanelli, Daniele and Larivi{\`e}re, Vincent},
  year = {2016},
  month = mar,
  journal = {PLOS ONE},
  volume = {11},
  number = {3},
  pages = {e0149504},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149504},
  urldate = {2022-03-16},
  abstract = {Debates over the pros and cons of a ``publish or perish'' philosophy have inflamed academia for at least half a century. Growing concerns, in particular, are expressed for policies that reward ``quantity'' at the expense of ``quality,'' because these might prompt scientists to unduly multiply their publications by fractioning (``salami slicing''), duplicating, rushing, simplifying, or even fabricating their results. To assess the reasonableness of these concerns, we analyzed publication patterns of over 40,000 researchers that, between the years 1900 and 2013, have published two or more papers within 15 years, in any of the disciplines covered by the Web of Science. The total number of papers published by researchers during their early career period (first fifteen years) has increased in recent decades, but so has their average number of co-authors. If we take the latter factor into account, by measuring productivity fractionally or by only counting papers published as first author, we observe no increase in productivity throughout the century. Even after the 1980s, adjusted productivity has not increased for most disciplines and countries. These results are robust to methodological choices and are actually conservative with respect to the hypothesis that publication rates are growing. Therefore, the widespread belief that pressures to publish are causing the scientific literature to be flooded with salami-sliced, trivial, incomplete, duplicated, plagiarized and false results is likely to be incorrect or at least exaggerated.},
  langid = {english},
  keywords = {Bibliometrics,Careers,Citation analysis,Generalized linear model,Science policy,Scientific publishing,Scientists,Social sciences},
  file = {/Users/cristian/Zotero/storage/RXQFV9IJ/Fanelli and Larivire - 2016 - Researchers Individual Publication Rate Has Not I.pdf;/Users/cristian/Zotero/storage/7IR8SEQ7/article.html}
}

@article{fanelliUSStudiesMay2013,
  title = {{{US}} Studies May Overestimate Effect Sizes in Softer Research},
  author = {Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2013},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {37},
  pages = {15031--15036},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1302997110},
  urldate = {2022-10-11},
  file = {/Users/cristian/Zotero/storage/8X22KIJL/Fanelli and Ioannidis - 2013 - US studies may overestimate effect sizes in softer.pdf}
}

@article{fangEfficacyXuefuZhuyu2016,
  title = {Efficacy of {{Xuefu Zhuyu}} Decoction Compared with Nitrates in Treating Angina Pectoris: {{A}} Meta-Analysis of Randomized Controlled Trials},
  author = {Fang, Z. and Wang, J.S. and Guo, P.P. and Jing, L. and Kong, L.Y.},
  year = {2016},
  journal = {International Journal of Clinical and Experimental Medicine},
  volume = {9},
  number = {3},
  pages = {5279--5290},
  publisher = {E-Century Publishing Corporation}
}

@article{fanGlobalSignificanceSubstrates2023,
  title = {Global {{Significance}} of {{Substrates}} for {{Nitrate Removal}} in {{Denitrifying Bioreactors Revealed}} by {{Meta-Analysis}}},
  author = {Fan, Y. and Zhuang, J. and Essington, M. and Jagadamma, S. and Schwartz, J. and Lee, J.},
  year = {2023},
  journal = {Engineering},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.eng.2022.08.017}
}

@article{Farewell01092004,
  title = {The Impact of Dichotomization on the Efficiency of Testing for an Interaction Effect in Exponential Family Models},
  author = {Farewell, Vern T and Tom, Brian D. M and Royston, Patrick},
  year = {2004},
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  eprint = {https://doi.org/10.1198/016214504000001169},
  pages = {822--831},
  publisher = {ASA Website},
  doi = {10.1198/016214504000001169}
}

@misc{FarewellBonferroniProblems,
  title = {Farewell to {{Bonferroni}}: The Problems of Low Statistical Power and Publication Bias {\textbar} {{Behavioral Ecology}} {\textbar} {{Oxford Academic}}},
  urldate = {2024-08-23},
  howpublished = {https://academic.oup.com/beheco/article/15/6/1044/206216},
  file = {/Users/cristian/Zotero/storage/XVMH4X53/206216.html}
}

@article{farrarTriallingMetaResearchComparative2020,
  title = {Trialling {{Meta-Research}} in {{Comparative Cognition}}: {{Claims}} and {{Statistical Inference}} in {{Animal Physical Cognition}}},
  shorttitle = {Trialling {{Meta-Research}} in {{Comparative Cognition}}},
  author = {Farrar, Benjamin G. and Altschul, Drew M. and Fischer, Julia and Van Der Mescht, Jolene and Placi, Sarah and Troisi, Camille A. and Vernouillet, Alizee and Clayton, Nicola S. and Ostojic, Ljerka},
  year = {2020},
  month = aug,
  journal = {Animal Behavior and Cognition},
  volume = {7},
  number = {3},
  pages = {419--444},
  issn = {23725052, 23724323},
  doi = {10.26451/abc.07.03.09.2020},
  urldate = {2023-08-09},
  abstract = {Scientific disciplines face concerns about replicability and statistical inference, and these concerns are also relevant in animal cognition research. This paper presents a first attempt to assess how researchers make and publish claims about animal physical cognition, and the statistical inferences they use to support them. We surveyed 116 published experiments from 63 papers on physical cognition, covering 43 different species. The most common tasks in our sample were trap-tube tasks (14 papers), other tool use tasks (13 papers), means-end understanding and string-pulling tasks (11 papers), object choice and object permanence tasks (9 papers) and access tasks (5 papers). This sample is not representative of the full scope of physical cognition research; however, it does provide data on the types of statistical design and publication decisions researchers have adopted. Across the 116 experiments, the median sample size was 7. Depending on the definitions we used, we estimated that between 44\% and 59\% of our sample of papers made positive claims about animals' physical cognitive abilities, between 24\% and 46\% made inconclusive claims, and between 10\% and 17\% made negative claims. Several failures of animals to pass physical cognition tasks were reported. Although our measures had low inter-observer reliability, these findings show that negative results can and have been published in the field. However, publication bias is still present, and consistent with this, we observed a drop in the frequency of p-values above .05. This suggests that some non-significant results have not been published. More promisingly, we found that researchers are likely making many correct statistical inferences at the individual-level. The strength of evidence of statistical effects at the group-level was weaker, and its p-value distribution was consistent with some effect sizes being overestimated. Studies such as ours can form part of a wider investigation into statistical reliability in comparative cognition. However, future work should focus on developing the validity and reliability of the measurements they use, and we offer some starting points.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/E8JACBQP/Farrar et al. - 2020 - Trialling Meta-Research in Comparative Cognition .pdf}
}

@article{faulStatisticalPowerAnalyses2009,
  title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
  author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  year = {2009},
  journal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1149--1160},
  doi = {10.3758/BRM.41.4.1149},
  abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/ZD2U96CD/Faul et al. - 2009 - Statistical power analyses using GPower 3.1 Test.pdf}
}

@article{fernandez-castillaDetectingSelectionBias2021,
  title = {Detecting {{Selection Bias}} in {{Meta-Analyses}} with {{Multiple Outcomes}}: {{A Simulation Study}}},
  shorttitle = {Detecting {{Selection Bias}} in {{Meta-Analyses}} with {{Multiple Outcomes}}},
  author = {{Fern{\'a}ndez-Castilla}, Bel{\'e}n and , Lies, Declercq and , Laleh, Jamshidi and , S. Natasha, Beretvas and , Patrick, Onghena and {and Van den Noortgate}, Wim},
  year = {2021},
  month = jan,
  journal = {The Journal of Experimental Education},
  volume = {89},
  number = {1},
  pages = {125--144},
  publisher = {Routledge},
  issn = {0022-0973},
  doi = {10.1080/00220973.2019.1582470},
  urldate = {2025-05-14},
  abstract = {This study explores the performance of classical methods for detecting publication bias---namely, Egger's regression test, Funnel Plot test, Begg's Rank Correlation and Trim and Fill method---in meta-analysis of studies that report multiple effects. Publication bias, outcome reporting bias, and a combination of these were generated. Egger's regression test and the Funnel Plot test were extended to three-level models, and possible cutoffs for the estimator of the Trim and Fill method were explored. Furthermore, we checked whether the combination of results of several methods yielded a better control of Type I error rates. Results show that no method works well across all conditions and that performance depends mainly on the population effect size value and the total variance.},
  keywords = {Meta-analysis,multiple effect sizes,publication bias,selective outcome reporting bias,simulation study},
  file = {/Users/cristian/Zotero/storage/A3NGI7MM/Fernndez-Castilla et al. - 2021 - Detecting Selection Bias in Meta-Analyses with Mul.pdf}
}

@article{fernandez-landaEffectCombinationCreatine2019,
  title = {Effect of the Combination of Creatine Monohydrate plus Hmb Supplementation on Sports Performance, Body Composition, Markers of Muscle Damage and Hormone Status: {{A}} Systematic Review},
  author = {{Fern{\'a}ndez-Landa}, J. and {Calleja-Gonz{\'a}lez}, J. and {Le{\'o}n-Guere{\~n}o}, P. and {Caballero-Garc{\'i}a}, A. and C{\'o}rdova, A. and {Mielgo-Ayuso}, J.},
  year = {2019},
  journal = {Nutrients},
  volume = {11},
  number = {10},
  publisher = {MDPI AG},
  doi = {10.3390/nu11102528}
}

@article{fernandez-lazaroModulationExerciseinducedMuscle2020,
  title = {Modulation of Exercise-Induced Muscle Damage, Inflammation, and Oxidative Markers by Curcumin Supplementation in a Physically Active Population: {{A}} Systematic Review},
  author = {{Fern{\'a}ndez-L{\'a}zaro}, D. and {Mielgo-Ayuso}, J. and Calvo, J.S. and Mart{\'i}nez, A.C. and Garc{\'i}a, A.C. and {Fernandez-Lazaro}, C.I.},
  year = {2020},
  journal = {Nutrients},
  volume = {12},
  number = {2},
  publisher = {MDPI AG},
  doi = {10.3390/nu12020501}
}

@article{fidlerEpistemicImportanceEstablishing2018,
  title = {The {{Epistemic Importance}} of {{Establishing}} the {{Absence}} of an {{Effect}}},
  author = {Fidler, Fiona and Singleton Thorn, Felix and Barnett, Ashley and Kambouris, Steven and Kruger, Ariel},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {237--244},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918770407},
  urldate = {2022-09-08},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/989VHV4J/Fidler et al. - 2018 - The Epistemic Importance of Establishing the Absen.pdf}
}

@article{fiedlerQuestionableResearchPractices2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550615612150},
  urldate = {2023-03-02},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english}
}

@article{fieldWhenWhyReplicate2019,
  title = {When and {{Why}} to {{Replicate}}: {{As Easy}} as 1, 2, 3?},
  author = {Field, Sarahanne M. and Hoekstra, Rink and Bringmann, Laura and {van Ravenzwaaij}, Don},
  editor = {Savalei, Victoria and Savalei, Victoria},
  year = {2019},
  month = sep,
  journal = {Collabra: Psychology},
  volume = {5},
  number = {1},
  pages = {46},
  doi = {10.1525/collabra.218},
  abstract = {The crisis of confidence in psychology has prompted vigorous and persistent debate in the scientific community concerning the veracity of the findings of psychological experiments. This discussion has led to changes in psychology's approach to research, and several new initiatives have been developed, many with the aim of improving our findings. One key advancement is the marked increase in the number of replication studies conducted. We argue that while it is important to conduct replications as part of regular research protocol, it is neither efficient nor useful to replicate results at random. We recommend adopting a methodical approach toward the selection of replication targets to maximize the impact of the outcomes of those replications, and minimize waste of scarce resources. In the current study, we demonstrate how a Bayesian re--analysis of existing research findings followed by a simple qualitative assessment process can drive the selection of the best candidate article for replication.},
  file = {/Users/cristian/Zotero/storage/I6FAEQML/Field et al. - 2019 - When and Why to Replicate As Easy as 1, 2, 3.pdf;/Users/cristian/Zotero/storage/SFFYEMDT/When-and-Why-to-Replicate-As-Easy-as-1-2-3.html}
}

@article{fisherApplicationsStudentDistribution1925,
  title = {Applications of "{{Student}}'s" Distribution.},
  author = {Fisher, Ronald A.},
  year = {1925},
  journal = {Metron},
  volume = {5},
  pages = {90--104}
}

@article{fisherArrangementFieldExperiments1926,
  title = {The Arrangement of Field Experiments},
  author = {Fisher, R. A.},
  year = {1926},
  journal = {Journal of the Ministry of Agriculture},
  volume = {33},
  pages = {503--515},
  doi = {10.23637/rothamsted.8v61q},
  abstract = {Author's note. From about 1923 onwards the Statistical Department at Rothamsted had been much concerned with the precision of field experiments in agriculture, and with modifications in their design, having the dual aim of increasing the precision and of providing a valid estimate of error. These two desiderata had been somewhat confused in the minds of the experimenters, and the present paper was the author's first attempt at setting out the rational principles on which he might proceed. The paper is a precursor to the book on the Design of Experiments published nine years later.},
  langid = {english}
}

@article{florezIncidencePrimaryEnd2023,
  title = {Incidence of {{Primary End Point Changes Among Active Cancer Phase}} 3 {{Randomized Clinical Trials}}},
  author = {Florez, Marcus A. and Jaoude, Joseph Abi and Patel, Roshal R. and Kouzy, Ramez and Lin, Timothy A. and De, Brian and Beck, Esther J. and Taniguchi, Cullen M. and Minsky, Bruce D. and Fuller, Clifton D. and Lee, J. Jack and Kupferman, Michael and Raghav, Kanwal P. and Overman, Michael J. and Thomas, Jr, Charles R. and Ludmir, Ethan B.},
  year = {2023},
  month = may,
  journal = {JAMA Network Open},
  volume = {6},
  number = {5},
  pages = {e2313819-e2313819},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2023.13819},
  urldate = {2023-06-19},
  abstract = {Primary end point (PEP) changes to an active clinical trial raise questions regarding trial quality and the risk of outcome reporting bias. It is unknown how the frequency and transparency of the reported changes depend on reporting method and whether the PEP changes are associated with trial positivity (ie, the trial met the prespecified statistical threshold for PEP positivity).To assess the frequency of reported PEP changes in oncology randomized clinical trials (RCTs) and whether these changes are associated with trial positivity.This cross-sectional study used publicly available data for complete oncology phase 3 RCTs registered in ClinicalTrials.gov from inception through February 2020.The main outcome was change between the initial PEP and the final reported PEP, assessed using 3 methods: (1) history of tracked changes on ClinicalTrials.gov, (2) self-reported changes noted in the article, and (3) changes reported within the protocol, including all available protocol documents. Logistic regression analyses were performed to evaluate whether PEP changes were associated with US Food and Drug Administration approval or trial positivity.Of 755 included trials, 145 (19.2\%) had PEP changes found by at least 1 of the 3 detection methods. Of the 145 trials with PEP changes, 102 (70.3\%) did not have PEP changes disclosed within the manuscript. There was significant variability in rates of PEP detection by each method ({$\chi$}2\,=\,72.1; P\,\&lt;\,.001). Across all methods, PEP changes were detected at higher rates when multiple versions of the protocol (47 of 148 [31.8\%]) were available compared with 1 version (22 of 134 [16.4\%]) or no protocol (76 of 473 [16.1\%]) ({$\chi$}2\,=\,18.7; P\,\&lt;\,.001). Multivariable analysis demonstrated that PEP changes were associated with trial positivity (odds ratio, 1.86; 95\% CI, 1.25-2.82; P\,=\,.003).This cross-sectional study revealed substantial rates of PEP changes among active RCTs; PEP changes were markedly underreported in published articles and mostly occurred after reported study completion dates. Significant discrepancies in the rate of detected PEP changes call into question the role of increased protocol transparency and completeness in identifying key changes occurring in active trials.}
}

@techreport{fooddrugadministrationMultipleEndpointsClinical2022,
  title = {Multiple {{Endpoints}} in {{Clinical Trials}} - {{Guidance}} for {{Industry}}},
  author = {Food Drug Administration},
  year = {2022},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/2CWHXZMX/Multiple Endpoints in Clinical Trials - Guidance f.pdf}
}

@article{forscherBenefitsBarriersRisks2023,
  title = {The {{Benefits}}, {{Barriers}}, and {{Risks}} of {{Big-Team Science}}},
  author = {Forscher, Patrick S. and Wagenmakers, Eric-Jan and Coles, Nicholas A. and Silan, Miguel Alejandro and Dutra, Nat{\'a}lia and {Basnight-Brown}, Dana and IJzerman, Hans},
  year = {2023},
  month = may,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {18},
  number = {3},
  pages = {607--623},
  issn = {1745-6924},
  doi = {10.1177/17456916221082970},
  abstract = {Progress in psychology has been frustrated by challenges concerning replicability, generalizability, strategy selection, inferential reproducibility, and computational reproducibility. Although often discussed separately, these five challenges may share a common cause: insufficient investment of intellectual and nonintellectual resources into the typical psychology study. We suggest that the emerging emphasis on big-team science can help address these challenges by allowing researchers to pool their resources together to increase the amount available for a single study. However, the current incentives, infrastructure, and institutions in academic science have all developed under the assumption that science is conducted by solo principal investigators and their dependent trainees, an assumption that creates barriers to sustainable big-team science. We also anticipate that big-team science carries unique risks, such as the potential for big-team-science organizations to be co-opted by unaccountable leaders, become overly conservative, and make mistakes at a grand scale. Big-team-science organizations must also acquire personnel who are properly compensated and have clear roles. Not doing so raises risks related to mismanagement and a lack of financial sustainability. If researchers can manage its unique barriers and risks, big-team science has the potential to spur great progress in psychology and beyond.},
  langid = {english},
  pmid = {36190899},
  keywords = {allied field: philosophy,allied field: sociology,big-team science,Humans,Interdisciplinary Research,metascience,methodology: scientific,Reproducibility of Results},
  file = {/Users/cristian/Zotero/storage/ET2VUSY5/Forscher et al. - 2023 - The Benefits, Barriers, and Risks of Big-Team Scie.pdf}
}

@article{forsellExperimentalEconomicsReplication2016,
  title = {Experimental {{Economics Replication Project}}},
  author = {Forsell, Eskil and Johannesson, Magnus and Camerer, Colin and Altmejd, Adam and Isaksson, Siri and Heikensten, Emma and Almenberg, Anna Dreber and Nave, Gideon and Pfeiffer, Thomas and Imai, Taisuke},
  year = {2016},
  month = feb,
  publisher = {OSF},
  urldate = {2021-03-15},
  abstract = {We replicate 18 laboratory experimental studies published in two high-impact economics journals in 2011-2014. All replications have a statistical power of {$\geq$}90\% to detect the original effect size at the 5\% significance level.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/9BSRH3G3/bzm54.html}
}

@article{fraleyNPactFactorEvaluating2014,
  title = {The {{N-Pact Factor}}: {{Evaluating}} the {{Quality}} of {{Empirical Journals}} with {{Respect}} to {{Sample Size}} and {{Statistical Power}}},
  author = {Fraley, R. Chris and Vazire, Simine},
  year = {2014},
  month = oct,
  journal = {PloS One},
  volume = {9},
  number = {10},
  pages = {e109019},
  doi = {10.1371/journal.pone.0109019},
  abstract = {The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their N-pact Factors (NF)---the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50\%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.},
  langid = {english},
  keywords = {Citation analysis,Cognitive psychology,Experimental psychology,Personality,Psychology,Research design,Scientific publishing,Social psychology},
  annotation = {https://doi.org/10.1371\\
/journal.pone.0109019},
  file = {/Users/cristian/Zotero/storage/HXKGBFAL/Fraley and Vazire - 2014 - The N-Pact Factor Evaluating the Quality of Empir.pdf;/Users/cristian/Zotero/storage/QSU4JRI6/Fraley and Vazire - 2014 - The N-Pact Factor Evaluating the Quality of Empir.pdf;/Users/cristian/Zotero/storage/U7A34TEG/article.html;/Users/cristian/Zotero/storage/ZHNA6VV5/article.html}
}

@article{francisPublicationBiasFailure2012,
  title = {Publication Bias and the Failure of Replication in Experimental Psychology},
  author = {Francis, Gregory},
  year = {2012},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {19},
  number = {6},
  pages = {975--991},
  doi = {10.3758/s13423-012-0322-y},
  abstract = {Replication of empirical findings plays a fundamental role in science. Among experimental psychologists, successful replication enhances belief in a finding, while a failure to replicate is often interpreted to mean that one of the experiments is flawed. This view is wrong. Because experimental psychology uses statistics, empirical findings should appear with predictable probabilities. In a misguided effort to demonstrate successful replication of empirical findings and avoid failures to replicate, experimental psychologists sometimes report too many positive results. Rather than strengthen confidence in an effect, too much successful replication actually indicates publication bias, which invalidates entire sets of experimental findings. Researchers cannot judge the validity of a set of biased experiments because the experiment set may consist entirely of type I errors. This article shows how an investigation of the effect sizes from reported experiments can test for publication bias by looking for too much successful replication. Simulated experiments demonstrate that the publication bias test is able to discriminate biased experiment sets from unbiased experiment sets, but it is conservative about reporting bias. The test is then applied to several studies of prominent phenomena that highlight how publication bias contaminates some findings in experimental psychology. Additional simulated experiments demonstrate that using Bayesian methods of data analysis can reduce (and in some cases, eliminate) the occurrence of publication bias. Such methods should be part of a systematic process to remove publication bias from experimental psychology and reinstate the important role of replication as a final arbiter of scientific findings.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/FMNXXYIH/Francis - 2012 - Publication bias and the failure of replication in.pdf}
}

@article{francoPublicationBiasSocial2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  doi = {10.1126/science.1255484},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/NHCYTMLV/articulo.html}
}

@misc{frankenhuisStrategicAmbiguitySocial2022,
  title = {Strategic Ambiguity in the Social Sciences},
  author = {Frankenhuis, Willem and Panchanathan, Karthik and Smaldino, Paul E.},
  year = {2022},
  month = jul,
  publisher = {MetaArXiv},
  doi = {10.31222/osf.io/kep5b},
  urldate = {2022-10-08},
  abstract = {In the wake of the replication crisis, there have been calls to increase the clarity and precision of theory in the social sciences. Here, we argue that the effects of these calls may be limited due to systematic and structural factors, and focus our attention on incentives favoring ambiguous theory. Intentionally or not, scientists can exploit theoretical ambiguities to make support for a claim appear stronger than is warranted. Practices include `theory stretching', interpreting an ambiguous claim more expansively to absorb data outside of the scope of the original claim, and `post-hoc precision', interpreting an ambiguous claim more narrowly so it appears more precisely aligned with the data. These practices lead to the overestimation of evidence for the original claim and create the appearance of consistent support and progressive research programs, which may in turn be rewarded by journals, funding agencies, and hiring committees. Selection for ambiguous research can occur outside of scientists' awareness and even when scientists act in good faith. We supplement our verbal argument with a simple mathematical model. Although ambiguity might be inevitable or even useful in the early stages of theory construction, scientists should aim for increased clarity as knowledge advances. Science benefits from transparently communicating about known ambiguities. To attain transparency about ambiguity, we provide a set of recommendations for authors, reviewers, and journals. We conclude with suggestions for research on how scientists use strategic ambiguity to advance their careers and on the ways in which norms, incentives, and practices favor strategic ambiguity.},
  langid = {american},
  keywords = {formal modeling,incentive structures,Other Social and Behavioral Sciences,post-hoc precision,Psychology,Social and Behavioral Sciences,strategic ambiguity,theory development,theory stretching},
  file = {/Users/cristian/Zotero/storage/G2XUYSLY/Frankenhuis et al. - 2022 - Strategic ambiguity in the social sciences.pdf}
}

@article{frankenhuisStrategicAmbiguitySocial2022a,
  title = {Strategic Ambiguity in the Social Sciences},
  author = {Frankenhuis, Willem and Panchanathan, Karthik and Smaldino, Paul E.},
  year = {2022},
  month = jul,
  publisher = {MetaArXiv},
  doi = {10.31222/osf.io/kep5b},
  urldate = {2022-10-08},
  abstract = {In the wake of the replication crisis, there have been calls to increase the clarity and precision of theory in the social sciences. Here, we argue that the effects of these calls may be limited due to systematic and structural factors, and focus our attention on incentives favoring ambiguous theory. Intentionally or not, scientists can exploit theoretical ambiguities to make support for a claim appear stronger than is warranted. Practices include `theory stretching', interpreting an ambiguous claim more expansively to absorb data outside of the scope of the original claim, and `post-hoc precision', interpreting an ambiguous claim more narrowly so it appears more precisely aligned with the data. These practices lead to the overestimation of evidence for the original claim and create the appearance of consistent support and progressive research programs, which may in turn be rewarded by journals, funding agencies, and hiring committees. Selection for ambiguous research can occur outside of scientists' awareness and even when scientists act in good faith. We supplement our verbal argument with a simple mathematical model. Although ambiguity might be inevitable or even useful in the early stages of theory construction, scientists should aim for increased clarity as knowledge advances. Science benefits from transparently communicating about known ambiguities. To attain transparency about ambiguity, we provide a set of recommendations for authors, reviewers, and journals. We conclude with suggestions for research on how scientists use strategic ambiguity to advance their careers and on the ways in which norms, incentives, and practices favor strategic ambiguity.},
  langid = {american},
  keywords = {formal modeling,incentive structures,Other Social and Behavioral Sciences,post-hoc precision,Psychology,Social and Behavioral Sciences,strategic ambiguity,theory development,theory stretching}
}

@article{fraserQuestionableResearchPractices2018,
  ids = {fraserQuestionableResearchPractices2018a},
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  urldate = {2021-09-28},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english},
  keywords = {Behavioral ecology,Community ecology,Evolutionary biology,Evolutionary ecology,Evolutionary rate,Psychology,Publication ethics,Statistical data},
  file = {/Users/cristian/Zotero/storage/BWD455TJ/Fraser et al. - 2018 - Questionable research practices in ecology and evo.pdf;/Users/cristian/Zotero/storage/T3KTKY9T/article.html}
}

@article{frickAcceptingNullHypothesis1995,
  title = {Accepting the Null Hypothesis},
  author = {Frick, Robert W.},
  year = {1995},
  journal = {Memory \& Cognition},
  volume = {23},
  number = {1},
  pages = {132--138},
  publisher = {Psychonomic Society},
  address = {US},
  issn = {1532-5946},
  doi = {10.3758/BF03210562},
  abstract = {Discusses use of the null hypothesis that 1 variable has no effect on another in psychological research. Despite opinions to the contrary, the author argues that this null hypothesis can be correct in some situations. Appropriate criteria for accepting the null hypothesis should be that the null hypothesis is possible, that results are consistent with the null hypothesis, and that the experiment was a good effort to find an effect. These criteria are consistent with the meta-rules for psychology. The good-effort criterion is subjective, which is somewhat undesirable, but the alternative of never accepting the null hypothesis is neither desirable nor practical. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Null Hypothesis Testing},
  file = {/Users/cristian/Zotero/storage/HDHQHKUB/1995-27802-001.html}
}

@article{frickAppropriateUseNull1996,
  title = {The Appropriate Use of Null Hypothesis Testing},
  author = {Frick, Robert W.},
  year = {1996},
  journal = {Psychological Methods},
  volume = {1},
  number = {4},
  pages = {379--390},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.1.4.379},
  abstract = {The many criticisms of null hypothesis testing suggest when it is not useful and what it should not be used for. This article explores when and why its use is appropriate. Null hypothesis testing is insufficient when size of effect is important, but it is ideal for testing ordinal claims relating the order of conditions, which are common in psychology. Null hypothesis testing also is insufficient for determining beliefs, but it is ideal for demonstrating sufficient evidential strength to support an ordinal claim, with sufficient evidence being 1 criterion for a finding entering the corpus of legitimate findings in psychology. The line between sufficient and insufficient evidence is currently set at p {$<$}.05; there is little reason for allowing experimenters to select their own value of alpha. Thus null hypothesis testing is an optimal method for demonstrating sufficient evidence for an ordinal claim. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Analysis,Statistical Validity},
  file = {/Users/cristian/Zotero/storage/A7TYG422/1996-06601-005.html}
}

@article{frickerAssessingStatisticalAnalyses2019,
  title = {Assessing the {{Statistical Analyses Used}} in {{Basic}} and {{Applied Social Psychology After Their}} P-{{Value Ban}}},
  author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {374--384},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537892},
  urldate = {2023-10-25},
  abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
  keywords = {Effect size,Inference ban,NHST,Psychology,Statistical significance},
  file = {/Users/cristian/Zotero/storage/5RQMFZJJ/Fricker et al. - 2019 - Assessing the Statistical Analyses Used in Basic a.pdf}
}

@article{friesePHackingPublicationBias2020,
  title = {P-{{Hacking}} and Publication Bias Interact to Distort Meta-Analytic Effect Size Estimates},
  author = {Friese, Malte and Frankenbach, Julius},
  year = {2020},
  journal = {Psychological Methods},
  doi = {10.1037/met0000246},
  abstract = {Science depends on trustworthy evidence. Thus, a biased scientific record is of questionable value because it impedes scientific progress, and the public receives advice on the basis of unreliable evidence that has the potential to have far-reaching detrimental consequences. Meta-analysis is a technique that can be used to summarize research evidence. However, meta-analytic effect size estimates may themselves be biased, threatening the validity and usefulness of meta-analyses to promote scientific progress. Here, we offer a large-scale simulation study to elucidate how p-hacking and publication bias distort meta-analytic effect size estimates under a broad array of circumstances that reflect the reality that exists across a variety of research areas. The results revealed that, first, very high levels of publication bias can severely distort the cumulative evidence. Second, p-hacking and publication bias interact: At relatively high and low levels of publication bias, p-hacking does comparatively little harm, but at medium levels of publication bias, p-hacking can considerably contribute to bias, especially when the true effects are very small or are approaching zero. Third, p-hacking can severely increase the rate of false positives. A key implication is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Estimation,Experimenter Bias,Meta Analysis,Scientific Communication,Simulation,Size},
  file = {/Users/cristian/Zotero/storage/DTUINWPN/Friese and Frankenbach - 2020 - p-Hacking and publication bias interact to distort.pdf;/Users/cristian/Zotero/storage/8WJXK4M7/2019-71476-001.html}
}

@article{fritzEffectSizeEstimates2011,
  title = {Effect {{Size Estimates}}: {{Current Use}}, {{Calculations}}, and {{Interpretation}}},
  shorttitle = {Effect {{Size Estimates}}},
  author = {Fritz, Catherine and Morris, Peter and Richler, Jennifer},
  year = {2011},
  month = aug,
  journal = {Journal of experimental psychology. General},
  volume = {141},
  pages = {2--18},
  doi = {10.1037/a0024338},
  abstract = {The Publication Manual of the American Psychological Association (American Psychological Association, 2001, American Psychological Association, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial {$\eta$}2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.}
}

@article{frohlichOutcomeEffectsEffects2009,
  title = {Outcome Effects and Effects Sizes in Sport Sciences},
  author = {Fr{\"o}hlich, Michael and Emrich, Eike and Pieter, Andrea and Stark, Robin},
  year = {2009},
  month = jan,
  journal = {International Journal of Sports Science and Engineering},
  volume = {3},
  pages = {175--179},
  abstract = {Abstract. In addition to statistical validation of an intervention in the context of experimental and quasi-experimental designs for hypothesis testing, the practical relevance of an intervention plays a major role. Practical relevance is considered a measure of an experimental effect with respect to various practical issues. Cohen's effect size has become the standard for assessment. However, empirical studies show that effect sizes should not be interpreted statically, but rather dynamically. Furthermore, it seems that prior experience, the target group, the way questions are posed and the context of the study influence the outcome. In the future, in addition to statistical validation, greater consideration should be given to effect sizes to allow a qualitative assessment of a measure's practical relevance. However, the applicable study context and theoretical criteria of the respective research domains must be taken into account. Key Words: statistical validation, practical relevance, effect size, strength training.},
  file = {/Users/cristian/Zotero/storage/E7TP3HI9/Frhlich et al. - 2009 - Outcome effects and effects sizes in sport science.pdf}
}

@misc{FullArticleAssessing,
  title = {Full Article: {{Assessing Statistical Results}}: {{Magnitude}}, {{Precision}}, and {{Model Uncertainty}}},
  urldate = {2023-10-16},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537889},
  file = {/Users/cristian/Zotero/storage/X8WDU9WY/00031305.2018.html}
}

@misc{FullArticleComparing,
  title = {Full Article: {{Comparing Three Groups}}},
  urldate = {2024-02-02},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/00031305.2021.2002188},
  file = {/Users/cristian/Zotero/storage/7KA684QR/00031305.2021.html}
}

@misc{FullArticleFalse,
  title = {Full Article: {{The False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  urldate = {2024-07-22},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1529622},
  file = {/Users/cristian/Zotero/storage/KNUQUUKS/00031305.2018.html}
}

@misc{FullArticleSample,
  title = {Full Article: {{Sample}} Size Estimation Revisited},
  urldate = {2025-06-24},
  howpublished = {https://www.tandfonline.com/doi/full/10.1080/02640414.2025.2499403?src=},
  file = {/Users/cristian/Zotero/storage/W4ZZM4C5/02640414.2025.html}
}

@article{funderEvaluatingEffectSize2019,
  title = {Evaluating {{Effect Size}} in {{Psychological Research}}: {{Sense}} and {{Nonsense}}},
  shorttitle = {Evaluating {{Effect Size}} in {{Psychological Research}}},
  author = {Funder, David C. and Ozer, Daniel J.},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245919847202},
  urldate = {2023-10-20},
  abstract = {Effect sizes are underappreciated and often misinterpreted---the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/9BKFX8XG/Funder and Ozer - 2019 - Evaluating Effect Size in Psychological Research .pdf}
}

@article{gandeviaPublicationsReplicationStatistics,
  title = {Publications, Replication, and Statistics in Physiology plus Two Neglected Curves},
  author = {Gandevia, Simon},
  journal = {The Journal of Physiology},
  volume = {n/a},
  number = {n/a},
  issn = {1469-7793},
  doi = {10.1113/JP281360},
  urldate = {2021-02-07},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/Q3XCA6MB/Gandevia - Publications, replication, and statistics in physi.pdf;/Users/cristian/Zotero/storage/GRJSUISY/JP281360.html}
}

@article{gaoPvaluesChronicConundrum2020,
  title = {P-Values -- a Chronic Conundrum},
  author = {Gao, Jian},
  year = {2020},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {167},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01051-6},
  urldate = {2021-02-01},
  abstract = {In medical research and practice, the p-value is arguably the most often used statistic and yet it is widely misconstrued as the probability of the type I error, which comes with serious consequences. This misunderstanding can greatly affect the reproducibility in research, treatment selection in medical practice, and model specification in empirical analyses. By using plain language and concrete examples, this paper is intended to elucidate the p-value confusion from its root, to explicate the difference between significance and hypothesis testing, to illuminate the consequences of the confusion, and to present a viable alternative to the conventional p-value.},
  keywords = {Calibrated P-values,Hypothesis testing,P-values,Research reproducibility,Significance testing,Type I error},
  file = {/Users/cristian/Zotero/storage/7QJZ836P/Gao - 2020 - P-values  a chronic conundrum.pdf;/Users/cristian/Zotero/storage/K4JSZ3IN/s12874-020-01051-6.html}
}

@article{garcia-garzonExploringCOVID19Research2022,
  title = {Exploring {{COVID-19}} Research Credibility among {{Spanish}} Scientists},
  author = {{Garcia-Garzon}, Eduardo and {Angulo-Brunet}, Ariadna and Lecuona, Oscar and Barrada, Juan Ram{\'o}n and Corradi, Guido},
  year = {2022},
  month = feb,
  journal = {Current Psychology},
  issn = {1936-4733},
  doi = {10.1007/s12144-022-02797-6},
  urldate = {2023-03-02},
  abstract = {Amidst a worldwide vaccination campaign, trust in science plays a significant role when addressing the COVID-19 pandemic. Given current concerns regarding research standards, we were interested in how Spanish scholars perceived COVID-19 research and the extent to which questionable research practices and potentially problematic academic incentives are commonplace. We asked researchers to evaluate~the expected quality of their COVID-19 projects and other peers' research and compared these assessments with those from scholars not involved in COVID-19 research. We investigated self-admitting and estimated rates of questionable research practices and attitudes towards current research status. Responses from 131 researchers suggested that COVID-19 evaluations followed partisan lines, with scholars~being more pessimistic~about others' colleagues' research~than their own. Additionally,researchers~not involved in COVID-19 projects were more negative~than~their participating peers. These differences were particularly notable for areas such as the expected theoretical foundations or overall quality of the research, among others. Most Spanish scholars expected questionable research practices and inadequate incentives to be widespread. In these two aspects, researchers tended to agree regardless of their involvement in COVID-19 research. We provide specific recommendations for improving future meta-science studies, such as redefining QRPs as inadequate research practices (IRP). This change could help avoid key controversies regarding QRPs' definition while highlighting their detrimental impact. Lastly, we join previous calls to improve transparency and academic career incentives as a cornerstone for generating trust in science.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UQDK4RXQ/Garcia-Garzon et al. - 2022 - Exploring COVID-19 research credibility among Span.pdf}
}

@article{garcia-sifuentesReportingMisreportingSex2021,
  title = {Reporting and Misreporting of Sex Differences in the Biological Sciences},
  author = {{Garcia-Sifuentes}, Yesenia and Maney, Donna L},
  year = {2021},
  journal = {eLife},
  volume = {10},
  pages = {e70817},
  issn = {2050-084X},
  doi = {10.7554/eLife.70817},
  urldate = {2024-01-25},
  abstract = {As part of an initiative to improve rigor and reproducibility in biomedical research, the U.S. National Institutes of Health now requires the consideration of sex as a biological variable in preclinical studies. This new policy has been interpreted by some as a call to compare males and females with each other. Researchers testing for sex differences may not be trained to do so, however, increasing risk for misinterpretation of results. Using a list of recently published articles curated by Woitowich et al. (eLife, 2020; 9:e56344), we examined reports of sex differences and non-differences across nine biological disciplines. Sex differences were claimed in the majority of the 147 articles we analyzed; however, statistical evidence supporting those differences was often missing. For example, when a sex-specific effect of a manipulation was claimed, authors usually had not tested statistically whether females and males responded differently. Thus, sex-specific effects may be over-reported. In contrast, we also encountered practices that could mask sex differences, such as pooling the sexes without first testing for a difference. Our findings support the need for continuing efforts to train researchers how to test for and report sex differences in order to promote rigor and reproducibility in biomedical research., Biomedical research has a long history of including only men or male laboratory animals in studies. To address this disparity, the United States National Institutes of Health (NIH) rolled out a policy in 2016 called Sex as a Biological Variable (or SABV). The policy requires researchers funded by the NIH to include males and females in every experiment unless there is a strong justification not to, such as studies of ovarian cancer. Since then, the number of research papers including both sexes has continued to grow., Although the NIH does not require investigators to compare males and females, many researchers have interpreted the SABV policy as a call to do so. This has led to reports of sex differences that would otherwise have been unrecognized or ignored. However, researchers may not be trained on how best to test for sex differences in their data, and if the data are not analyzed appropriately this may lead to misleading interpretations., Here, Garcia-Sifuentes and Maney have examined the methods of 147 papers published in 2019 that included both males and females. They discovered that more than half of these studies had reported sex differences, but these claims were not always backed by statistical evidence. Indeed, in a large majority (more than 70\%) of the papers describing differences in how males and females responded to a treatment, the impact of the treatment was not actually statistically compared between the sexes. This suggests that sex-specific effects may be over-reported. In contrast, Garcia-Sifuentes and Maney also encountered instances where an effect may have been masked due to data from males and females being pooled together without testing for a difference first., These findings reveal how easy it is to draw misleading conclusions from sex-based data. Garcia-Sifuentes and Maney hope their work raises awareness of this issue and encourages the development of more training materials for researchers.},
  pmcid = {PMC8562995},
  pmid = {34726154},
  file = {/Users/cristian/Zotero/storage/84MU3MU9/Garcia-Sifuentes and Maney - Reporting and misreporting of sex differences in t.pdf}
}

@article{garfieldCitationIndexesSociological1963,
  title = {Citation Indexes in Sociological and Historical Research},
  author = {Garfield, Eugene},
  year = {1963},
  journal = {American Documentation},
  volume = {14},
  number = {4},
  pages = {289--291},
  issn = {1936-6108},
  doi = {10.1002/asi.5090140405},
  urldate = {2021-11-15},
  abstract = {The use of citation data in the computer construction of historical network diagrams is proposed. A new ``critical path method'' of evaluating the impact of individual scientific discoveries is described as well as a ``critical impact'' factor which measures the arborization of specific papers or ideas through the subsequent literature. The danger of indiscriminate. unqualified use of quantitative citation data in sociological evaluations is emphasized. However, citation indexes do make thorough qualitative evaluations possible by providing a practical tool for locating subsequent criticisms of particular papers by specified authors.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/7FCGBDFX/asi.html}
}

@article{garfieldHistoryMeaningJournal2006,
  title = {The History and Meaning of the Journal Impact Factor},
  author = {Garfield, Eugene},
  year = {2006},
  month = jan,
  journal = {JAMA},
  volume = {295},
  number = {1},
  pages = {90--93},
  issn = {1538-3598},
  doi = {10.1001/jama.295.1.90},
  langid = {english},
  pmid = {16391221},
  keywords = {{Peer Review, Research},Bibliometrics,Periodicals as Topic}
}

@article{gelmanDifferenceSignificantNot2006,
  title = {The {{Difference Between}} ``{{Significant}}'' and ``{{Not Significant}}'' Is Not {{Itself Statistically Significant}}},
  author = {Gelman, Andrew and Stern, Hal},
  year = {2006},
  month = nov,
  journal = {The American Statistician},
  volume = {60},
  number = {4},
  pages = {328--331},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313006X152649},
  urldate = {2023-06-30},
  abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary---for example, only a small change is required to move an estimate from a 5.1\% significance level to 4.9\%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems---that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ``significant'' and ``not significant'' is not itself statistically significant.},
  keywords = {Hypothesis testing,Meta-analysis,Pairwise comparison,Replication}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The {{Statistical Crisis}} in {{Science}}},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  month = feb,
  journal = {American Scientist},
  volume = {102},
  pages = {460--465},
  doi = {10.1511/2014.111.460},
  urldate = {2022-11-17},
  abstract = {Data-dependent analysis---a ``garden of forking paths''--- explains why many statistically significant comparisons don't hold up.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/TJD5XS5J/the-statistical-crisis-in-science.html}
}

@article{gelmanWhyDidIt2021,
  title = {Why {{Did It Take So Many Decades}} for the {{Behavioral Sciences}} to {{Develop}} a {{Sense}} of {{Crisis Around Methodology}} and {{Replication}}?},
  author = {Gelman, Andrew and Vazire, Simine},
  year = {2021},
  month = nov,
  journal = {Journal of Methods and Measurement in the Social Sciences},
  volume = {12},
  number = {1},
  issn = {2159-7855},
  doi = {10.2458/jmmss.3062},
  urldate = {2021-11-11},
  abstract = {For several decades, leading behavioral scientists have offered strong criticisms of the common practice of null hypothesis significance testing as producing spurious findings without strong theoretical or empirical support. But only in the past decade has this manifested as a full-scale replication crisis. We consider some possible reasons why, on or about December 2010, the behavioral sciences changed.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/W89A4MGW/Gelman and Vazire - 2021 - Why Did It Take So Many Decades for the Behavioral.pdf}
}

@article{giboinEffectEgoDepletion2019,
  title = {The Effect of Ego Depletion or Mental Fatigue on Subsequent Physical Endurance Performance: {{A}} Meta-Analysis},
  shorttitle = {The Effect of Ego Depletion or Mental Fatigue on Subsequent Physical Endurance Performance},
  author = {Giboin, Louis-Solal and Wolff, Wanja},
  year = {2019},
  month = sep,
  journal = {Performance Enhancement \& Health},
  volume = {7},
  number = {1},
  pages = {100150},
  issn = {2211-2669},
  doi = {10.1016/j.peh.2019.100150},
  urldate = {2022-10-03},
  abstract = {Two independent lines of research propose that exertion of mental effort can impair subsequent performance due to ego depletion or mental fatigue. In this meta-analysis, we unite these research fields to facilitate a greater exchange between the two, to summarize the extant literature and to highlight open questions. We performed a meta-analysis to quantify the effect of ego-depletion and mental fatigue on subsequent physical endurance performance (42 independent effect sizes). We found that ego-depletion or mental fatigue leads to a reduction in subsequent physical endurance performance (ES = -0.506 [95\% CI: -0.649, -0.369]) and that the duration of prior mental effort exertion did not predict the magnitude of subsequent performance impairment (r = -0.043). Further, analyses revealed that effects of prior mental exertion are more pronounced in subsequent tasks that use isolation tasks (e.g., handgrip; ES = -0.719 [-0.946, -0.493]) compared to whole-body endurance tasks (e.g. cycling; coefficient\,=\,0.338 [0.057, 0.621]) and that the observed reduction in performance is higher when the person-situation fit is low (ES for high person-situation fit = -0.355 [-0.529, -0.181], coefficient for low person-situation fit = -0.336 [-0.599, -0.073]). Taken together, the aggregate of the published literature on ego depletion or mental fatigue indicates that prior mental exertion is detrimental to subsequent physical endurance performance. However, this analysis also highlights several open questions regarding the effects' mechanisms and moderators. Particularly, the surprising finding that the duration of prior mental exertion seems to be unrelated to subsequent performance impairment needs to be addressed systematically.},
  langid = {english},
  keywords = {Cognitive fatigue,Conservation of resources,Mental effort,Motivation,Self-control},
  file = {/Users/cristian/Zotero/storage/7IJ99978/Giboin and Wolff - 2019 - The effect of ego depletion or mental fatigue on s.pdf;/Users/cristian/Zotero/storage/FCRNUEC2/S2211266919300209.html}
}

@article{gielenNutritionalInterventionsImprove2021,
  title = {Nutritional Interventions to Improve Muscle Mass, Muscle Strength, and Physical Performance in Older People: {{An}} Umbrella Review of Systematic Reviews and Meta-Analyses},
  author = {Gielen, E. and Beckw{\'e}e, D. and Delaere, A. and De Breucker, S. and Vandewoude, M. and Bautmans, I. and Bautmans, I. and Beaudart, C. and Beckw{\'e}e, D. and Beyer, I. and Bruy{\`e}re, O. and De Breucker, S. and De Cock, A.-M. and Delaere, A. and {De Saint-Hubert}, M. and De Spiegeleer, A. and Gielen, E. and Perkisas, S. and Vandewoude, M. and {Sarcopenia Guidelines Development Group of the Belgian Society of Gerontology and Geriatrics (BSGG)}},
  year = {2021},
  journal = {Nutrition Reviews},
  volume = {79},
  number = {2},
  pages = {121--147},
  publisher = {Oxford University Press},
  doi = {10.1093/nutrit/nuaa011}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  urldate = {2022-03-29},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {Collective illusions,Editors,Rituals,Statistical significance,Textbooks},
  file = {/Users/cristian/Zotero/storage/XR9TDHZG/S1053535704000927.html}
}

@article{glennAcuteCitrullinemalateSupplementation2016,
  title = {Acute Citrulline-Malate Supplementation Improves Maximal Strength and Anaerobic Power in Female, Masters Athletes Tennis Players},
  author = {Glenn, Jordan M. and Gray, Michelle and Jensen, Austen and Stone, Matthew S. and Vincenzo, Jennifer L.},
  year = {2016},
  month = nov,
  journal = {European Journal of Sport Science},
  volume = {16},
  number = {8},
  pages = {1095--1103},
  issn = {1536-7290},
  doi = {10.1080/17461391.2016.1158321},
  abstract = {Citrulline-malate (CM) is a precursor to nitric-oxide (NO) in the NO synthase (NOS) pathway and is suggested to increase exercise performance in younger individuals. With age, NO production decreases and augmented NO production may provide beneficial effects on sports performance among masters athletes (MAs). PURPOSE: To examine the effects of acute CM supplementation on grip strength, vertical power, and anaerobic cycling performance in female, MA tennis players. METHODS: Seventeen female MA (51\,{\textpm}\,9 years) completed two double-blind, randomized trials consuming CM (12\>g dextrose\,+\,8\>g CM) and placebo (PLA) (12\>g dextrose). One hour after consumption, subjects completed grip strength, vertical power, and Wingate anaerobic cycling assessments in respective order. Maximal and average grip strength, peak and average vertical power, anaerobic capacity, peak power, explosive power, and ability to sustain anaerobic power were calculated from the tests. RESULTS: When consuming CM, participants exhibited greater maximal (p\,=\,.042) and average (p\,=\,.045) grip strength compared to PLA. No differences existed between trials for peak (p\,=\,.51) or average (p\,=\,.51) vertical power. For the Wingate, peak power (p\,{$<$}\,.001) and explosive power (p\,{$<$}\,.001) were significantly greater when consuming CM compared to PLA. For the ability to sustain power, a significant effect (p\,{$<$}\,.001) was observed for time within trials, but no significant differences were observed between trials regarding supplement consumed. CONCLUSIONS: These data suggest that consuming CM before competition has the potential to improve tennis match-play performance in masters tennis athletes. However, this study utilized a controlled laboratory environment and research evaluating direct application to on-court performance is warranted.},
  langid = {english},
  pmid = {27017895},
  keywords = {Adult,aging,Aging,amino-acid,Athletes,Athletic Performance,Bicycling,Citrulline,Ergogenic aid,Female,Hand Strength,Humans,Malates,Middle Aged,Muscle Strength,Nitric Oxide,nitric-oxide,Performance-Enhancing Substances,sport performance,Tennis,Wingate}
}

@article{glocknerEmpiricalContentTheories2011,
  title = {The Empirical Content of Theories in Judgment and Decision Making: {{Shortcomings}} and Remedies},
  shorttitle = {The Empirical Content of Theories in Judgment and Decision Making},
  author = {Gl{\"o}ckner, Andreas and Betsch, Tilmann},
  year = {2011},
  month = dec,
  journal = {Judgment and Decision Making},
  volume = {6},
  number = {8},
  pages = {711--721},
  issn = {1930-2975},
  doi = {10.1017/S1930297500004149},
  urldate = {2025-04-08},
  abstract = {According to Karl Popper, we can tell good theories from poor ones by assessing their empirical content (empirischer Gehalt), which basically reflects how much information they convey concerning the world. ``The empirical content of a statement increases with its degree of falsifiability: the more a statement forbids, the more it says about the world of experience.'' Two criteria to evaluate the empirical content of a theory are their level of universality (Allgemeinheit) and their degree of precision (Bestimmtheit). The former specifies how many situations it can be applied to. The latter refers to the specificity in prediction, that is, how many subclasses of realizations it allows. We conduct an analysis of the empirical content of theories in Judgment and Decision Making (JDM) and identify the challenges in theory formulation for different classes of models. Elaborating on classic Popperian ideas, we suggest some guidelines for publication of theoretical work.},
  langid = {english},
  keywords = {critical rationalism,critical testing,empirical content,falsification,formalization,methodology,theory of science},
  file = {/Users/cristian/Zotero/storage/BHPX2PMU/Glckner and Betsch - 2011 - The empirical content of theories in judgment and .pdf}
}

@article{gloverLikelihoodRatiosSimple2004,
  title = {Likelihood Ratios: {{A}} Simple and Flexible Statistic for Empirical Psychologists},
  shorttitle = {Likelihood Ratios},
  author = {Glover, Scott and Dixon, Peter},
  year = {2004},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {11},
  number = {5},
  pages = {791--806},
  issn = {1531-5320},
  doi = {10.3758/BF03196706},
  urldate = {2023-08-30},
  abstract = {Empirical studies in psychology typically employ null hypothesis significance testing to draw statistical inferences. We propose that likelihood ratios are a more straightforward alternative to this approach. Likelihood ratios provide a measure of the fit of two competing models; the statistic represents a direct comparison of the relative likelihood of the data, given the best fit of the two models. Likelihood ratios offer an intuitive, easily interpretable statistic that allows the researcher great flexibility in framing empirical arguments. In support of this position, we report the results of a survey of empirical articles in psychology, in which the common uses of statistics by empirical psychologists is examined. From the results of this survey, we show that likelihood ratios are able to serve all the important statistical needs of researchers in empirical psychology in a format that is more straightforward and easier to interpret than traditional inferential statistics.},
  langid = {english},
  keywords = {ANOVA Table,Likelihood Ratio,Null Model,Quadratic Component,Unexplained Variation},
  file = {/Users/cristian/Zotero/storage/XIWW4PFD/Glover and Dixon - 2004 - Likelihood ratios A simple and flexible statistic.pdf}
}

@article{goemanComparingThreeGroups2022,
  title = {Comparing {{Three Groups}}},
  author = {Goeman, Jelle J. and Solari, Aldo},
  year = {2022},
  month = apr,
  journal = {The American Statistician},
  volume = {76},
  number = {2},
  pages = {168--176},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2021.2002188},
  urldate = {2024-02-02},
  abstract = {For multiple comparisons in analysis of variance, the practitioners' handbooks generally advocate standard methods such as Bonferroni, or an F-test followed by Tukey's honest significant difference method. These methods are known to be suboptimal compared to closed testing procedures, but improved methods can be complex in the general multigroup set-up. In this note, we argue that the case of three-groups is special: with three groups, closed testing procedures are powerful and easy to use. We describe four different closed testing procedures specifically for the three-group set-up. The choice of method should be determined by assessing which of the comparisons are considered primary and which are secondary, as dictated by subject-matter considerations. We describe how all four methods can be used with any standard software.},
  keywords = {Analysis of variance,Closed testing,Dunnett,Multiple comparisons,Multiple testing,Tukey's honest significant difference},
  file = {/Users/cristian/Zotero/storage/T2Q8NUXK/Goeman and Solari - 2022 - Comparing Three Groups.pdf}
}

@article{goodmanDirtyDozenTwelve2008,
  title = {A Dirty Dozen: Twelve p-Value Misconceptions},
  shorttitle = {A Dirty Dozen},
  author = {Goodman, Steven},
  year = {2008},
  month = jul,
  journal = {Seminars in Hematology},
  volume = {45},
  number = {3},
  pages = {135--140},
  issn = {0037-1963},
  doi = {10.1053/j.seminhematol.2008.04.003},
  abstract = {The P value is a measure of statistical evidence that appears in virtually all medical research papers. Its interpretation is made extraordinarily difficult because it is not part of any formal system of statistical inference. As a result, the P value's inferential meaning is widely and often wildly misconstrued, a fact that has been pointed out in innumerable papers and books appearing since at least the 1940s. This commentary reviews a dozen of these common misinterpretations and explains why each is wrong. It also reviews the possible consequences of these improper understandings or representations of its meaning. Finally, it contrasts the P value with its Bayesian counterpart, the Bayes' factor, which has virtually all of the desirable properties of an evidential measure that the P value lacks, most notably interpretability. The most serious consequence of this array of P-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism.},
  langid = {english},
  pmid = {18582619},
  keywords = {Probability,Research Design}
}

@article{goodmanUsePredictedConfidence1994,
  title = {The Use of Predicted Confidence Intervals When Planning Experiments and the Misuse of Power When Interpreting Results},
  author = {Goodman, S. N. and Berlin, J. A.},
  year = {1994},
  month = aug,
  journal = {Annals of Internal Medicine},
  volume = {121},
  number = {3},
  pages = {200--206},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-121-3-199408010-00008},
  abstract = {Although there is a growing understanding of the importance of statistical power considerations when designing studies and of the value of confidence intervals when interpreting data, confusion exists about the reverse arrangement: the role of confidence intervals in study design and of power in interpretation. Confidence intervals should play an important role when setting sample size, and power should play no role once the data have been collected, but exactly the opposite procedure is widely practiced. In this commentary, we present the reasons why the calculation of power after a study is over is inappropriate and how confidence intervals can be used during both study design and study interpretation.},
  langid = {english},
  pmid = {8017747},
  keywords = {{Data Interpretation, Statistical},Bayes Theorem,Confidence Intervals,Research Design}
}

@article{goodmanValuesHypothesisTests1993,
  title = {P Values, Hypothesis Tests, and Likelihood: Implications for Epidemiology of a Neglected Historical Debate},
  shorttitle = {P Values, Hypothesis Tests, and Likelihood},
  author = {Goodman, S. N.},
  year = {1993},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {137},
  number = {5},
  pages = {485-496; discussion 497-501},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a116700},
  abstract = {It is not generally appreciated that the p value, as conceived by R. A. Fisher, is not compatible with the Neyman-Pearson hypothesis test in which it has become embedded. The p value was meant to be a flexible inferential measure, whereas the hypothesis test was a rule for behavior, not inference. The combination of the two methods has led to a reinterpretation of the p value simultaneously as an "observed error rate" and as a measure of evidence. Both of these interpretations are problematic, and their combination has obscured the important differences between Neyman and Fisher on the nature of the scientific method and inhibited our understanding of the philosophic implications of the basic methods in use today. An analysis using another method promoted by Fisher, mathematical likelihood, shows that the p value substantially overstates the evidence against the null hypothesis. Likelihood makes clearer the distinction between error rates and inferential evidence and is a quantitative tool for expressing evidential strength that is more appropriate for the purposes of epidemiology than the p value.},
  langid = {english},
  pmid = {8465801},
  keywords = {{History, 20th Century},Epidemiologic Methods,Likelihood Functions,Probability,Statistics as Topic}
}

@article{gopalakrishnaPrevalenceQuestionableResearch2022a,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  urldate = {2022-02-17},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english},
  keywords = {Deception,Linear regression analysis,Medical humanities,Medicine and health sciences,Open science,Research integrity,Scientific misconduct,Surveys}
}

@article{gordonEffectsCreatineMonohydrate2023,
  title = {The {{Effects}} of {{Creatine Monohydrate Loading}} on {{Exercise Recovery}} in {{Active Women}} throughout the {{Menstrual Cycle}}},
  author = {Gordon, Amanda N. and Moore, Sam R. and Patterson, Noah D. and Hostetter, Maggie E. and Cabre, Hannah E. and Hirsch, Katie R. and Hackney, Anthony C. and {Smith-Ryan}, Abbie E.},
  year = {2023},
  month = jan,
  journal = {Nutrients},
  volume = {15},
  number = {16},
  pages = {3567},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-6643},
  doi = {10.3390/nu15163567},
  urldate = {2023-08-13},
  abstract = {Creatine supplementation improves anaerobic performance and recovery; however, to date, these outcomes have not been well explored in females. This study evaluated the effect of creatine monohydrate loading on exercise recovery, measured by heart rate variability (HRV) and repeated sprint performance, in women across the menstrual cycle. In this randomized, double-blind, cross-over study, 39 women (mean {\textpm} standard deviation: age: 24.6 {\textpm} 5.9 years, height: 172.5 {\textpm} 42.3 cm, weight: 65.1 {\textpm} 8.1 kg, BF: 27.4 {\textpm} 5.8\%) were randomized to a creatine monohydrate (n = 19; 20 g per day in 4 {\texttimes} 5 g doses) or non-caloric PL group (n = 20). HRV was measured at rest and after participants completed a repeated sprint cycling test (10 {\texttimes} 6 s maximal sprints). Measurements were conducted before and after supplementation in the follicular/low hormone and luteal/high hormone phases. Creatine monohydrate supplementation did not influence HRV values, as no significant differences were seen in HRV values at rest or postexercise. For repeated sprint outcomes, there was a significant phase {\texttimes} supplement interaction (p = 0.048) for fatigue index, with the greatest improvement seen in high hormone in the creatine monohydrate group (-5.8 {\textpm} 19.0\%) compared to changes in the PL group (0.1 {\textpm} 8.1\%). Sprint performance and recovery were reduced by the high hormone for both groups. Though not statistically significant, the data suggests that creatine monohydrate could help counteract performance decrements caused by the high hormone. This data can help inform creatine monohydrate loading strategies for females, demonstrating potential benefits in the high hormone phase.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dietary supplement,female physiology,menstrual cycle},
  file = {/Users/cristian/Zotero/storage/CVY52UXI/Gordon et al. - 2023 - The Effects of Creatine Monohydrate Loading on Exe.pdf}
}

@article{gorgolewskiBrainImagingData2016,
  title = {The Brain Imaging Data Structure, a Format for Organizing and Describing Outputs of Neuroimaging Experiments},
  author = {Gorgolewski, Krzysztof J. and Auer, Tibor and Calhoun, Vince D. and Craddock, R. Cameron and Das, Samir and Duff, Eugene P. and Flandin, Guillaume and Ghosh, Satrajit S. and Glatard, Tristan and Halchenko, Yaroslav O. and Handwerker, Daniel A. and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B. Nolan and Nichols, Thomas E. and Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A. and Varoquaux, Ga{\"e}l and Poldrack, Russell A.},
  year = {2016},
  month = jun,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.44},
  urldate = {2025-06-17},
  abstract = {The development of magnetic resonance imaging (MRI) techniques has defined modern neuroimaging. Since its inception, tens of thousands of studies using techniques such as functional MRI and diffusion weighted imaging have allowed for the non-invasive study of the brain. Despite the fact that MRI is routinely used to obtain data for neuroscience research, there has been no widely adopted standard for organizing and describing the data collected in an imaging experiment. This renders sharing and reusing data (within or between labs) difficult if not impossible and unnecessarily complicates the application of automatic pipelines and quality assurance protocols. To solve this problem, we have developed the Brain Imaging Data Structure (BIDS), a standard for organizing and describing MRI datasets. The BIDS standard uses file formats compatible with existing software, unifies the majority of practices already common in the field, and captures the metadata necessary for most common data processing operations.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Data publication and archiving,Research data},
  file = {/Users/cristian/Zotero/storage/Y59I454L/Gorgolewski et al. - 2016 - The brain imaging data structure, a format for org.pdf}
}

@article{gotzSmallEffectsIndispensable2022,
  title = {Small {{Effects}}: {{The Indispensable Foundation}} for a {{Cumulative Psychological Science}}},
  shorttitle = {Small {{Effects}}},
  author = {G{\"o}tz, Friedrich M. and Gosling, Samuel D. and Rentfrow, Peter J.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {205--215},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620984483},
  urldate = {2022-12-07},
  abstract = {We draw on genetics research to argue that complex psychological phenomena are most likely determined by a multitude of causes and that any individual cause is likely to have only a small effect. Building on this, we highlight the dangers of a publication culture that continues to demand large effects. First, it rewards inflated effects that are unlikely to be real and encourages practices likely to yield such effects. Second, it overlooks the small effects that are most likely to be real, hindering attempts to identify and understand the actual determinants of complex psychological phenomena. We then explain the theoretical and practical relevance of small effects, which can have substantial consequences, especially when considered at scale and over time. Finally, we suggest ways in which scholars can harness these insights to advance research and practices in psychology (i.e., leveraging the power of big data, machine learning, and crowdsourcing science; promoting rigorous preregistration, including prespecifying the smallest effect size of interest; contextualizing effects; changing cultural norms to reward accurate and meaningful effects rather than exaggerated and unreliable effects). Only once small effects are accepted as the norm, rather than the exception, can a reliable and reproducible cumulative psychological science be built.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/7HZNKLYK/Gtz et al. - 2022 - Small Effects The Indispensable Foundation for a .pdf}
}

@article{goulet-pelletierReviewEffectSizes2018,
  title = {A Review of Effect Sizes and Their Confidence Intervals, {{Part I}}: {{The Cohen}}'s d Family},
  shorttitle = {A Review of Effect Sizes and Their Confidence Intervals, {{Part I}}},
  author = {{Goulet-Pelletier}, Jean-Christophe and Cousineau, Denis},
  year = {2018},
  journal = {The Quantitative Methods for Psychology},
  volume = {14},
  number = {4},
  pages = {242--265},
  publisher = {The Quantitative Methods for Psychology},
  address = {Canada},
  issn = {2292-1354},
  doi = {10.20982/tqmp.14.4.p242},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 16(4) of The Quantitative Methods for Psychology (see record 2020-73716-013). The results of the method for estimating confidence intervals according to Steiger and Fouladi (1997) were not displayed correctly in the initially published paper (Figure 7 in Appendix C). The figure is corrected in the erratum. The new figure changes the interpretation of the method to a more favorable appreciation.] Effect sizes and confidence intervals are important statistics to assess the magnitude and the precision of an effect. The various standardized effect sizes can be grouped in three categories depending on the experimental design: measures of the difference between two means (the d family), measures of strength of association (e.g., r, R{$^2$}, {$\eta^2$}, {$\omega^2$}), and risk estimates (e.g., odds ratio, relative risk, phi; Kirk, 1996). Part I of this study reviews the d family, with a special focus on Cohen's d and Hedges' g for two-independent groups and two-repeated measures (or paired samples) designs. The present paper answers questions concerning the d family via Monte Carlo simulations. First, four different denominators are often proposed to standardize the mean difference in a repeated measures design. Which one should be used? Second, the literature proposes several approximations to estimate the standard error. Which one most closely estimates the true standard deviation of the distribution? Lastly, central and noncentral methods have been proposed to construct a confidence interval around d. Which method leads to more precise coverage, and how to calculate it? Results suggest that the best way to standardize the effect in both designs is by using the pooled standard deviation in conjunction with a correction factor to unbias d. Likewise, the best standard error approximation is given by substituting the gamma function from the true formula by its approximation. Lastly, results from the confidence interval simulations show that, under the normality assumption, the noncentral method is always superior, especially with small sample sizes. However, the central method is equivalent to the noncentral method when n is greater than 20 in each group for a between-group design and when n is greater than 24 pairs of observations for a repeated measures design. A practical guide to apply the findings of this study can be found after the general discussion. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Effect Size (Statistical),Risk Assessment,Simulation,Social Psychology},
  file = {/Users/cristian/Zotero/storage/A7AZVWKW/Goulet-Pelletier and Cousineau - 2018 - A review of effect sizes and their confidence inte.pdf;/Users/cristian/Zotero/storage/5MNBL5QA/2018-66887-002.html}
}

@article{gowOmega3FattyAcid2014,
  title = {Omega-3 Fatty Acid and Nutrient Deficits in Adverse Neurodevelopment and Childhood Behaviors},
  author = {Gow, R.V. and Hibbeln, J.R.},
  year = {2014},
  journal = {Child and Adolescent Psychiatric Clinics of North America},
  volume = {23},
  number = {3},
  pages = {555--590},
  publisher = {W.B. Saunders},
  doi = {10.1016/j.chc.2014.02.002}
}

@article{gravinaN3FattyAcid2017,
  title = {N-3 {{Fatty Acid Supplementation During}} 4 {{Weeks}} of {{Training Leads}} to {{Improved Anaerobic Endurance Capacity}}, but Not {{Maximal Strength}}, {{Speed}}, or {{Power}} in {{Soccer Players}}},
  author = {Gravina, Leyre and Brown, Frankie F. and Alexander, Lee and Dick, James and Bell, Gordon and Witard, Oliver C. and Galloway, Stuart D. R.},
  year = {2017},
  month = aug,
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {27},
  number = {4},
  pages = {305--313},
  issn = {1543-2742},
  doi = {10.1123/ijsnem.2016-0325},
  abstract = {Omega-3 fatty acid (n-3 FA) supplementation could promote adaptation to soccer-specific training. We examined the impact of a 4-week period of n-3 FA supplementation during training on adaptations in 1RM knee extensor strength, 20-m sprint speed, vertical jump power, and anaerobic endurance capacity (Yo-Yo test) in competitive soccer players. Twenty six soccer players were randomly assigned to one of two groups: n-3 FA supplementation (n-3 FA; n = 13) or placebo (n = 13). Both groups performed two experimental trial days. Assessments of physical function and respiratory function were conducted pre (PRE) and post (POST) supplementation. Training session intensity, competitive games and nutritional intake were monitored during the 4-week period. No differences were observed in respiratory measurements (FEV1, FVC) between groups. No main effect of treatment was observed for 1RM knee extensor strength, explosive leg power, or 20 m sprint performance, but strength improved as a result of the training period in both groups (p {$<$} .05). Yo-Yo test distance improved with training in the n-3 FA group only (p {$<$} .01). The mean difference (95\% CI) in Yo-Yo test distance completed from PRE to POST was 203 (66-340) m for n-3 FA, and 62 (-94-217) m for placebo, with a moderate effect size (Cohen's d of 0.52). We conclude that 4 weeks of n-3 FA supplementation does not improve strength, power or speed assessments in competitive soccer players. However, the increase in anaerobic endurance capacity evident only in the n-3 FA treatment group suggests an interaction that requires further study.},
  langid = {english},
  pmid = {28387540},
  keywords = {{Adaptation, Physiological},{Fatty Acids, Omega-3},adaptation,Adolescent,Adult,Athletic Performance,exercise,Fatty Acids,Female,fish oils,Forced Expiratory Volume,Humans,lipids,Male,Peak Expiratory Flow Rate,Physical Endurance,Soccer,Sports Nutritional Physiological Phenomena,Young Adult},
  file = {/Users/cristian/Zotero/storage/XBG33IVV/Gravina et al. - 2017 - n-3 Fatty Acid Supplementation During 4 Weeks of T.pdf}
}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  doi = {10.1007/s10654-016-0149-3},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english},
  pmcid = {PMC4877414},
  pmid = {27209009},
  keywords = {{Data Interpretation, Statistical},Confidence intervals,Confidence Intervals,Humans,Hypothesis testing,Null testing,P value,Power,Probability,Significance tests,Statistical testing},
  file = {/Users/cristian/Zotero/storage/E55UF76Z/Greenland et al. - 2016 - Statistical tests, P values, confidence intervals,.pdf}
}

@article{greenVisualAuditoryChoice1984,
  title = {Visual and Auditory Choice Reaction Times},
  author = {Green, David M. and Von Gierke, Susanne M.},
  year = {1984},
  month = may,
  journal = {Acta Psychologica},
  volume = {55},
  number = {3},
  pages = {231--247},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(84)90043-X},
  urldate = {2024-07-29},
  abstract = {Choice reaction times are measured for three values of a priori signal probability with three well-practiced observers. Two sets of data are taken with the only difference being the modality of the reaction signal. In one set of conditions it is auditory, in the other, visual. The auditory reaction times are faster than the visual and in addition several other differences are noted. The latency of the errors and correct responses are nearly equal for the auditory data. Error latencies are nearly 30\% faster for the visual data. Non-stationary effects, autocorrelation between successive latencies and non-homogeneous distribution of errors, are clearly evident in the visual data, but are small or non-existent in the auditory data. The data are compared with several models of the choice reaction time process but none of the models is completely adequate.},
  file = {/Users/cristian/Zotero/storage/LS62SHRR/000169188490043X.html}
}

@article{greenwaldConsequencesPrejudiceNull1975,
  title = {Consequences of Prejudice against the Null Hypothesis},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  pages = {1--20},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0076157},
  abstract = {Examined the consequences of prejudice against accepting the null hypothesis through (a) a mathematical model intended to stimulate the research-publication process and (b) case studies of apparent erroneous rejections of the null hypothesis in published psychological research. The input parameters for the model characterize investigators' probabilities of selecting a problem for which the null hypothesis is true, of reporting, following up on, or abandoning research when data do or do not reject the null hypothesis, and they characterize editors' probabilities of publishing manuscripts concluding in favor of or against the null hypothesis. With estimates of the input parameters based on a questionnaire survey of 75 social psychologists, the model output indicates a dysfunctional research-publication system. Particularly, the model indicates that there may be relatively few publications on problems for which the null hypothesis is (at least to a reasonable approximation) true, and of these, a high proportion will erroneously reject the null hypothesis. The case studies provide additional support for this conclusion. It is concluded that research traditions and customs of discrimination against accepting the null hypothesis may be very detrimental to research progress. (44 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Consequence,Experimentation,Mathematical Modeling,Null Hypothesis Testing},
  file = {/Users/cristian/Zotero/storage/5FFF5HFD/Greenwald - 1975 - Consequences of prejudice against the null hypothe.pdf;/Users/cristian/Zotero/storage/3G8G5JUD/doiLanding.html}
}

@article{grgicCaffeineIngestionEnhances2018,
  ids = {grgicCaffeineIngestionEnhances2018a,grgicCaffeineIngestionEnhances2018b},
  title = {Caffeine Ingestion Enhances {{Wingate}} Performance: A Meta-Analysis.},
  author = {Grgic, Jozo},
  year = {2018},
  month = mar,
  journal = {European journal of sport science},
  volume = {18},
  number = {2},
  pages = {219--225},
  publisher = {{Taylor and Francis Ltd.}},
  address = {England},
  issn = {1536-7290},
  doi = {10.1080/17461391.2017.1394371},
  abstract = {The positive effects of caffeine ingestion on aerobic performance are well-established; however, recent findings are suggesting that caffeine ingestion  might also enhance components of anaerobic performance. A commonly used test of  anaerobic performance and power output is the 30-second Wingate test. Several  studies explored the effects of caffeine ingestion on Wingate performance, with  equivocal findings. To elucidate this topic, this paper aims to determine the  effects of caffeine ingestion on Wingate performance using meta-analytic  statistical techniques. Following a search through PubMed/MEDLINE, Scopus, and  SportDiscus({\textregistered}), 16 studies were found meeting the inclusion criteria (pooled  number of participants\,=\,246). Random-effects meta-analysis of standardized mean  differences (SMD) for peak power output and mean power output was performed.  Study quality was assessed using the modified version of the PEDro checklist.  Results of the meta-analysis indicated a significant difference (p\,=\,.005)  between the placebo and caffeine trials on mean power output with SMD values of  small magnitude (0.18; 95\% confidence interval: 0.05, 0.31; +3\%). The  meta-analysis performed for peak power output indicated a significant difference  (p\,=\,.006) between the placebo and caffeine trials (SMD\,=\,0.27; 95\% confidence  interval: 0.08, 0.47 [moderate magnitude]; +4\%). The results from the PEDro  checklist indicated that, in general, studies are of good and excellent  methodological quality. This meta-analysis adds on to the current body of  evidence showing that caffeine ingestion can also enhance components of anaerobic  performance. The results presented herein may be helpful for developing more  efficient evidence-based recommendations regarding caffeine supplementation.},
  langid = {english},
  pmid = {29087785},
  keywords = {{Muscle, Skeletal/*drug effects/physiology},Bicycling,Caffeine/*pharmacology,Exercise,Exercise Test,Humans,nutrition,performance}
}

@article{grgicEffectsBetaalanineSupplementation2021,
  ids = {grgicEffectsBetaalanineSupplementation2021a},
  title = {Effects of Beta-Alanine Supplementation on {{Yo-Yo}} Test Performance: {{A}} Meta-Analysis.},
  author = {Grgic, Jozo},
  year = {2021},
  month = jun,
  journal = {Clinical nutrition ESPEN},
  volume = {43},
  pages = {158--162},
  publisher = {Elsevier Ltd},
  address = {England},
  issn = {2405-4577},
  doi = {10.1016/j.clnesp.2021.03.027},
  abstract = {OBJECTIVE: The aim of this meta-analysis was to explore the effects of beta-alanine supplementation on Yo-Yo test performance. METHODS: Nine databases  were searched to find relevant studies. A random-effects meta-analysis of  standardized mean differences (SMD) was performed for data analysis. Subgroup  meta-analyses were conducted to explore the effects of beta-alanine  supplementation duration on Yo-Yo test performance, and the effects of  beta-alanine supplementation on performance only in Yo-Yo level 2 test variants.  RESULTS: Ten study groups were included in the meta-analysis. All studies  included athletes as study participants. When considering all available studies,  there was no significant difference between the placebo/control and beta-alanine  groups (SMD: 0.68; 95\% confidence interval [CI]:~-0.30, 1.67). When considering  only the studies that used supplementation protocols lasting between 6 and 12  weeks, there was a significant ergogenic effect of beta-alanine (SMD: 1.02; 95\%  CI: 0.01, 2.05). When considering only the studies that used the level 2 variants  of the Yo-Yo test, there was a significant ergogenic effect of beta-alanine (SMD:  1.41; 95\% CI: 0.35, 2.48). CONCLUSIONS: This meta-analysis found that  beta-alanine is ergogenic for Yo-Yo test performance in athletes when the  supplementation protocol lasts between 6 and 12 weeks and when using the level 2  variants of the Yo-Yo test.},
  copyright = {Copyright {\copyright} 2021 European Society for Clinical Nutrition and Metabolism. Published by Elsevier Ltd. All rights reserved.},
  langid = {english},
  pmid = {34024507},
  keywords = {*Dietary Supplements,*Performance-Enhancing Substances,beta-Alanine/pharmacology,Buffering,Carnosine,Humans,L-histidine,Sport}
}

@article{griffinCalculatingStatisticalPower2021,
  title = {Calculating Statistical Power for Meta-Analysis Using Metapower},
  author = {Griffin, Jason W.},
  year = {2021},
  month = mar,
  journal = {The Quantitative Methods for Psychology},
  volume = {17},
  number = {1},
  pages = {24--39},
  issn = {2292-1354},
  doi = {10.20982/tqmp.17.1.p024},
  urldate = {2025-05-11},
  abstract = {Meta-analysis is an influential evidence synthesis technique that summarizes a body of research. Though impactful, meta-analyses fundamentally depend on the literature being sufficiently large to generate meaningful conclusions. Power analysis plays an important role in determining the number of studies required to conduct a substantive meta-analysis. Despite this, power analysis is rarely conducted or reported in published meta-analyses. A significant barrier to the widespread implementation of power analysis is the lack of available and accessible software for calculating statistical power for meta-analysis. In this paper, I provide an introduction to power analysis and present a practical tutorial for calculating statistical power using the R package metapower. The main functionality includes computing statistical power for summary effect sizes, tests of homogeneity, categorical moderator analysis, and subgroup analysis. This software is free, easy-to-use, and can be integrated into a continuous work flow with other meta-analysis packages in R.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/I38BQXMW/Griffin - 2021 - Calculating statistical power for meta-analysis us.pdf}
}

@article{grimesModellingScienceTrustworthiness2018,
  title = {Modelling Science Trustworthiness under Publish or Perish Pressure},
  author = {Grimes, David Robert and Bauch, Chris T. and Ioannidis, John P. A.},
  year = {2018},
  journal = {Royal Society Open Science},
  volume = {5},
  number = {1},
  pages = {171511},
  publisher = {Royal Society},
  doi = {10.1098/rsos.171511},
  urldate = {2022-03-11},
  abstract = {Scientific publication is immensely important to the scientific endeavour. There is, however, concern that rewarding scientists chiefly on publication creates a perverse incentive, allowing careless and fraudulent conduct to thrive, compounded by the predisposition of top-tier journals towards novel, positive findings rather than investigations confirming null hypothesis. This potentially compounds a reproducibility crisis in several fields, and risks undermining science and public trust in scientific findings. To date, there has been comparatively little modelling on factors that influence science trustworthiness, despite the importance of quantifying the problem. We present a simple phenomenological model with cohorts of diligent, careless and unethical scientists, with funding allocated by published outputs. This analysis suggests that trustworthiness of published science in a given field is influenced by false positive rate, and pressures for positive results. We find decreasing available funding has negative consequences for resulting trustworthiness, and examine strategies to combat propagation of irreproducible science.},
  keywords = {public trust in science,publish or perish,research ethics,research fraud,science trustworthiness},
  file = {/Users/cristian/Zotero/storage/B5NZTD9K/Grimes et al. - Modelling science trustworthiness under publish or.pdf}
}

@article{grossoLongtermCoffeeConsumption2017,
  title = {Long-Term Coffee Consumption Is Associated with Decreased Incidence of New-Onset Hypertension: {{A}} Dose--Response Meta-Analysis},
  author = {Grosso, G. and Micek, A. and Godos, J. and Pajak, A. and Sciacca, S. and {Bes-Rastrollo}, M. and Galvano, F. and {Martinez-Gonzalez}, M.A.},
  year = {2017},
  journal = {Nutrients},
  volume = {9},
  number = {8},
  publisher = {MDPI AG},
  doi = {10.3390/nu9080890}
}

@misc{gschneidnerEffectsLengthenedpartialRange2024,
  title = {The Effects of Lengthened-Partial Range of Motion Resistance Training of the Limbs on Arm and Thigh Muscle Cross-Sectional Area: {{A}} Multi-Site Cluster Trial},
  shorttitle = {The Effects of Lengthened-Partial Range of Motion Resistance Training of the Limbs on Arm and Thigh Muscle Cross-Sectional Area},
  author = {Gschneidner, David and Carlson, Luke and Steele, James and Fisher, James},
  year = {2024},
  month = dec,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.485},
  urldate = {2025-02-24},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {hypertrophy,range of motion,resistance training,strength,trained participants},
  file = {/Users/cristian/Zotero/storage/KGHH434F/Gschneidner et al. - 2024 - The effects of lengthened-partial range of motion .pdf}
}

@article{gucciardiHandlingEffectSize2022,
  title = {Handling Effect Size Dependency in Meta-Analysis},
  author = {Gucciardi, Daniel F. and Lines, Robin L. J. and Ntoumanis, Nikos},
  year = {2022},
  journal = {International Review of Sport and Exercise Psychology},
  volume = {15},
  number = {1},
  pages = {152--178},
  publisher = {Taylor \& Francis},
  address = {United Kingdom},
  issn = {1750-9858},
  doi = {10.1080/1750984X.2021.1946835},
  abstract = {The statistical synthesis of quantitative effects within primary studies via meta-analysis is an important analytical technique in the scientific toolkit of modern researchers. As with any scientific method or technique, knowledge of the weaknesses that might render findings limited or potentially erroneous as well as strategies by which to mitigate these biases is essential for high-quality scientific evidence. In this paper, we focus on one prevalent consideration for meta-analytical investigations, namely dependency among effects. We provide readers with a non-technical introduction to and overview of statistical solutions for handling dependent effects for their efforts to integrate evidence within primary studies. This goal is achieved via a series of seven reflective questions that scholars might consider when planning and executing a meta-analysis in which some degree of dependency among effect sizes from primary studies may exist. We also provide an example application of the recommendations with real-world data, including an analytical script that readers can adapt for their own purposes. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Analysis of Variance,Estimation,Experimental Methods,Meta Analysis,Open Science},
  file = {/Users/cristian/Zotero/storage/PUMQBR66/Gucciardi et al. - 2022 - Handling effect size dependency in meta-analysis.pdf;/Users/cristian/Zotero/storage/6362W67H/2021-62915-001.html}
}

@article{guerrero-boteFurtherStepForward2012,
  title = {A Further Step Forward in Measuring Journals' Scientific Prestige: {{The SJR2}} Indicator},
  shorttitle = {A Further Step Forward in Measuring Journals' Scientific Prestige},
  author = {{Guerrero-Bote}, Vicente P. and {Moya-Aneg{\'o}n}, F{\'e}lix},
  year = {2012},
  month = oct,
  journal = {Journal of Informetrics},
  volume = {6},
  number = {4},
  pages = {674--688},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2012.07.001},
  urldate = {2021-04-21},
  abstract = {A new size-independent indicator of scientific journal prestige, the SJR2 indicator, is proposed. This indicator takes into account not only the prestige of the citing scientific journal but also its closeness to the cited journal using the cosine of the angle between the vectors of the two journals' cocitation profiles. To eliminate the size effect, the accumulated prestige is divided by the fraction of the journal's citable documents, thus eliminating the decreasing tendency of this type of indicator and giving meaning to the scores. Its method of computation is described, and the results of its implementation on the Scopus 2008 dataset is compared with those of an ad hoc Journal Impact Factor, JIF(3y), and SNIP, the comparison being made both overall and within specific scientific areas. All three, the SJR2 indicator, the SNIP indicator and the JIF distributions, were found to fit well to a logarithmic law. Although the three metrics were strongly correlated, there were major changes in rank. In addition, the SJR2 was distributed more equalized than the JIF by Subject Area and almost as equalized as the SNIP, and better than both at the lower level of Specific Subject Areas. The incorporation of the cosine increased the values of the flows of prestige between thematically close journals.},
  langid = {english},
  keywords = {Academic journals,Citation networks,Eigenvector centrality,Journal prestige,SJR2 indicator},
  file = {/Users/cristian/Zotero/storage/L8MHIW7I/Guerrero-Bote and Moya-Anegn - 2012 - A further step forward in measuring journals scie.pdf;/Users/cristian/Zotero/storage/K5ALAAGM/S1751157712000521.html}
}

@article{guoyunEfficacySafetyFenofibrate2022,
  title = {Efficacy and Safety of Fenofibrate Add-on Therapy in Patients with Primary Biliary Cholangitis Refractory to Ursodeoxycholic Acid: {{A}} Retrospective Study and Updated Meta-Analysis},
  author = {Guoyun, X. and Dawei, D. and Ning, L. and Yinan, H. and Fangfang, Y. and Siyuan, T. and Hao, S. and Jiaqi, Y. and Ang, X. and Guanya, G. and Xi, C. and Yulong, S. and Ying, H.},
  year = {2022},
  journal = {Frontiers in Pharmacology},
  volume = {13},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fphar.2022.948362}
}

@article{gurwitzExclusionElderlyWomen1992,
  title = {The {{Exclusion}} of the {{Elderly}} and {{Women From Clinical Trials}} in {{Acute Myocardial Infarction}}},
  author = {Gurwitz, J.H. and Col, N.F. and Avorn, J.},
  year = {1992},
  journal = {JAMA: The Journal of the American Medical Association},
  volume = {268},
  number = {11},
  pages = {1417--1422},
  doi = {10.1001/jama.1992.03490110055029}
}

@article{habayInterindividualVariabilityMental2023,
  ids = {habayInterindividualVariabilityMental2023a},
  title = {Interindividual {{Variability}} in {{Mental Fatigue-Related Impairments}} in {{Endurance Performance}}: {{A Systematic Review}} and {{Multiple Meta-regression}}},
  author = {Habay, J. and Uylenbroeck, R. and Van Droogenbroeck, R. and De Wachter, J. and Proost, M. and Tassignon, B. and De Pauw, K. and Meeusen, R. and Pattyn, N. and Van Cutsem, J. and Roelands, B.},
  year = {2023},
  journal = {Sports Medicine - Open},
  volume = {9},
  number = {1},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1186/s40798-023-00559-7}
}

@article{hadefiNoninvasiveDiagnosisAlcoholrelated2020,
  title = {Noninvasive Diagnosis in Alcohol-Related Liver Disease},
  author = {Hadefi, A. and Degr{\'e}, D. and Tr{\'e}po, E. and Moreno, C.},
  year = {2020},
  journal = {Health Science Reports},
  volume = {3},
  number = {1},
  publisher = {{John Wiley and Sons Inc}},
  doi = {10.1002/hsr2.146}
}

@article{hagerStatisticalTheoriesFisher2013,
  title = {The Statistical Theories of {{Fisher}} and of {{Neyman}} and {{Pearson}}: {{A}} Methodological Perspective},
  shorttitle = {The Statistical Theories of {{Fisher}} and of {{Neyman}} and {{Pearson}}},
  author = {Hager, Willi},
  year = {2013},
  month = apr,
  journal = {Theory \& Psychology},
  volume = {23},
  number = {2},
  pages = {251--270},
  issn = {0959-3543, 1461-7447},
  doi = {10.1177/0959354312465483},
  urldate = {2025-06-07},
  abstract = {Most of the debates around statistical testing suffer from a failure to identify clearly the features specific to the theories invented by Fisher and by Neyman and Pearson. These features are outlined. The hybrids of Fisher's and Neyman--Pearson's theory are briefly addressed. The lack of random sampling and its consequences for statistical inference are also highlighted, leading to the recommendation to dispense with inferences and perform approximate randomization tests instead. A possible scheme for the appraisal of substantive hypotheses is offered, the corroboration of which is a necessary prerequisite for scientific explanations and predictions. The scheme is partly based on the Neyman--Pearson theory. This theory, though not perfect, is superior to its competitors, especially when examining substantive hypotheses. The many statistical and extra-statistical decisions prior to experimentation and the inevitable subjectivity of our research endeavors are emphasized. If feasible, statistical problems should be discussed from an extra-statistical methodological/epistemological viewpoint.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/PW6JLHRM/Hager - 2013 - The statistical theories of Fisher and of Neyman a.pdf}
}

@article{hagger_multilab_2016,
  title = {A {{Multilab Preregistered Replication}} of the {{Ego-Depletion Effect}}},
  author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Sch{\"u}tz, A. and Stamos, A. and Tingh{\"o}g, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
  year = {2016},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {4},
  pages = {546--573},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691616652873},
  urldate = {2022-12-22},
  abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals (CIs) that encompassed zero (d = 0.04, 95\% CI [?0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/H5JTJS3S/Hagger et al. - 2016 - A Multilab Preregistered Replication of the Ego-De.pdf}
}

@article{hallerMisinterpretationsSignificanceProblem2002,
  title = {Misinterpretations of Significance: {{A}} Problem Students Share with Their Teachers?},
  shorttitle = {Misinterpretations of Significance},
  author = {Haller, Heiko and Kraus, Stefan},
  year = {2002},
  journal = {Methods of Psychological Research},
  volume = {7},
  pages = {1--20},
  publisher = {Institute for Science Education},
  address = {Germany},
  issn = {1432-8534},
  abstract = {The use of significance tests in science has been debated from the invention of these tests until the present time. Apart from theoretical critiques on their appropriateness for evaluating scientific hypotheses, significance tests also receive criticism for inviting misinterpretations. The authors presented 6 common misinterpretations to psychologists who work in German universities and found out that they are still surprisingly widespread, even among instructors who teach statistics to psychology students. Although these misinterpretations are well documented among students, until now there has been little research on pedagogical methods to remove them. Rather, they are considered "hard facts" that are impervious to correction. The authors discuss the roots of these misinterpretations and propose a pedagogical concept to teach significance tests, which involves explaining the meaning of statistical significance in an appropriate way. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Comprehension,Psychologists,Statistical Significance,Statistics,Teaching},
  file = {/Users/cristian/Zotero/storage/AE57WJJ5/2002-14044-001.html}
}

@article{halperinStrengtheningPracticeExercise2018,
  title = {Strengthening the {{Practice}} of {{Exercise}} and {{Sport-Science Research}}},
  author = {Halperin, Israel and Vigotsky, Andrew D. and Foster, Carl and Pyne, David B.},
  year = {2018},
  month = feb,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {13},
  number = {2},
  pages = {127--134},
  doi = {htpps://doi.org/10.1123/ijspp.2017-0322},
  abstract = {Exercise and sport sciences continue to grow as a collective set of disciplines investigating a broad array of basic and applied research questions. Despite the progress, there is room for improvement. A number of problems pertaining to reliability and validity of research practices hinder advancement and the potential impact of the field. These problems include inadequate validation of surrogate outcomes, too few longitudinal and replication studies, limited reporting of null or trivial results, and insufficient scientific transparency. The purpose of this review is to discuss these problems as they pertain to exercise and sport sciences based on their treatment in other disciplines, namely psychology and medicine, and to propose a number of solutions and recommendations.},
  langid = {english},
  keywords = {Exercise,Humans,methodology,null results,replication,Reproducibility of Results,Research Design,Sports},
  file = {/Users/cristian/Zotero/storage/EY4K47ZS/Halperin et al. - 2018 - Strengthening the Practice of Exercise and Sport-S.pdf}
}

@article{halperinThreatsInternalValidity2015,
  title = {Threats to Internal Validity in Exercise Science: A Review of Overlooked Confounding Variables},
  shorttitle = {Threats to Internal Validity in Exercise Science},
  author = {Halperin, Israel and Pyne, David B. and Martin, David T.},
  year = {2015},
  month = oct,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {10},
  number = {7},
  pages = {823--829},
  issn = {1555-0273},
  doi = {10.1123/ijspp.2014-0566},
  abstract = {Internal validity refers to the degree of control exerted over potential confounding variables to reduce alternative explanations for the effects of various treatments. In exercise and sports-science research and routine testing, internal validity is commonly achieved by controlling variables such as exercise and warm-up protocols, prior training, nutritional intake before testing, ambient temperature, time of testing, hours of sleep, age, and gender. However, a number of other potential confounding variables often do not receive adequate attention in sports physiology and performance research. These confounding variables include instructions on how to perform the test, volume and frequency of verbal encouragement, knowledge of exercise endpoint, number and gender of observers in the room, influence of music played before and during testing, and the effects of mental fatigue on performance. In this review the authors discuss these variables in relation to common testing environments in exercise and sports science and present some recommendations with the goal of reducing possible threats to internal validity.},
  langid = {english},
  pmid = {25756869},
  keywords = {{Confounding Factors, Epidemiologic},Athletic Performance,Attention,Behavior Observation Techniques,Exercise Test,Formative Feedback,Humans,Mental Fatigue,Music,Reproducibility of Results,Research Design}
}

@article{halseyFickleValueGenerates2015,
  title = {The Fickle {{P}} Value Generates Irreproducible Results},
  author = {Halsey, Lewis G. and {Curran-Everett}, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
  year = {2015},
  month = mar,
  journal = {Nature Methods},
  volume = {12},
  number = {3},
  pages = {179--185},
  doi = {10.1038/nmeth.3288},
  abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/JTAEB7S4/Halsey et al. - 2015 - The fickle P value generates irreproducible result.pdf;/Users/cristian/Zotero/storage/JJ5M3JTA/nmeth.html}
}

@article{handDeconstructingStatisticalQuestions1994,
  title = {Deconstructing {{Statistical Questions}}},
  author = {Hand, David J.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {157},
  number = {3},
  eprint = {2983526},
  eprinttype = {jstor},
  pages = {317--356},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0964-1998},
  doi = {10.2307/2983526},
  urldate = {2024-01-03},
  abstract = {Too much current statistical work takes a superficial view of the client's research question, adopting techniques which have a solid history, a sound mathematical basis or readily available software, but without considering in depth whether the questions being answered are in fact those which should be asked. Examples, some familiar and others less so, are given to illustrate this assertion. It is clear that establishing the mapping from the client's domain to a statistical question is one of the most difficult parts of a statistical analysis. It is a part in which the responsibility is shared by both client and statistician. A plea is made for more research effort to go in this direction and some suggestions are made for ways to tackle the problem.},
  file = {/Users/cristian/Zotero/storage/RLGAB6U6/Hand - 1994 - Deconstructing Statistical Questions.pdf}
}

@article{hanssenEffectsDifferentEndurance2018,
  title = {Effects of Different Endurance Exercise Modalities on Migraine Days and Cerebrovascular Health in Episodic Migraineurs: {{A}} Randomized Controlled Trial},
  shorttitle = {Effects of Different Endurance Exercise Modalities on Migraine Days and Cerebrovascular Health in Episodic Migraineurs},
  author = {Hanssen, H. and Minghetti, A. and Magon, S. and Rossmeissl, A. and Rasenack, M. and Papadopoulou, A. and Klenk, C. and Faude, O. and Zahner, L. and Sprenger, T. and Donath, L.},
  year = {2018},
  journal = {Scandinavian Journal of Medicine \& Science in Sports},
  volume = {28},
  number = {3},
  pages = {1103--1112},
  issn = {1600-0838},
  doi = {10.1111/sms.13023},
  urldate = {2023-07-06},
  abstract = {Aerobic exercise training is a promising complementary treatment option in migraine and can reduce migraine days and improve retinal microvascular function. Our aim was to elucidate whether different aerobic exercise programs at high vs moderate intensities distinctly affect migraine days as primary outcome and retinal vessel parameters as a secondary. In this randomized controlled trial, migraine days were recorded by a validated migraine diary in 45 migraineurs of which 36 (female: 28; age: 36 (SD:10)/BMI: 23.1 (5.3) completed the training period (dropout: 20\%). Participants were assigned (Strata: age, gender, fitness and migraine symptomatology) to either high intensity interval training (HIT), moderate continuous training (MCT), or a control group (CON). Intervention groups trained twice a week over a 12-week intervention period. Static retinal vessel analysis, central retinal arteriolar (CRAE) and venular (CRVE) diameters, as well as the arteriolar-to-venular diameter ratio (AVR) were obtained for cerebrovascular health assessment. Incremental treadmill testing yielded maximal and submaximal fitness parameters. Overall, moderate migraine day reductions were observed ( = .12): HIT revealed 89\% likely beneficial effects (SMD = 1.05) compared to MCT (SMD = 0.50) and CON (SMD = 0.59). Very large intervention effects on AVR improvement ( = 0.27), slightly favoring HIT (SMD=-0.43) over CON (SMD=0), were observed. HIT seems more effective for migraine day reduction and improvement of cerebrovascular health compared to MCT. Intermittent exercise programs of higher intensities may need to be considered as an additional treatment option in migraine patients.},
  copyright = {{\copyright} 2017 John Wiley \& Sons A/S. Published by John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {cerebral circulation,exercise training,migraine,retinal vessel diameters},
  file = {/Users/cristian/Zotero/storage/LLQXTM7R/Hanssen et al. - 2018 - Effects of different endurance exercise modalities.pdf;/Users/cristian/Zotero/storage/QJ25Y3JF/sms.html}
}

@inproceedings{hardiyantoPerformanceCarcassTraits2022,
  title = {Performance, Carcass Traits, and Relative Organ Weight of Broiler Supplemented by Guanidinoacetic Acid: {{A}} Meta-Analysis},
  booktitle = {{{IOP Conference Series}}: {{Earth}} and {{Environmental Science}}},
  author = {Hardiyanto, Y. and Jayanegara, A. and Mutia, R. and Nofyangtri, S.},
  year = {2022},
  volume = {951},
  publisher = {IOP Publishing Ltd},
  doi = {10.1088/1755-1315/951/1/012030}
}

@article{hardwickeDataAvailabilityReusability2018,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J. and Henry Tessler, Michael and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
  year = {2018},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {8},
  pages = {180448},
  publisher = {Royal Society},
  doi = {10.1098/rsos.180448},
  urldate = {2023-02-28},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (`analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  keywords = {interrupted time series,journal policy,meta-science,open data,open science,reproducibility},
  file = {/Users/cristian/Zotero/storage/GYGMAV5F/Hardwicke et al. - 2018 - Data availability, reusability, and analytic repro.pdf}
}

@article{harmsMakingNullEffects2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  month = jul,
  journal = {Journal of Clinical and Translational Research},
  volume = {3},
  number = {Suppl 2},
  pages = {382--393},
  issn = {2382-6533},
  urldate = {2023-04-28},
  abstract = {Being able to interpret `null effects?is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  pmcid = {PMC6412612},
  pmid = {30873486},
  file = {/Users/cristian/Zotero/storage/A9ABK28G/Harms and Lakens - 2018 - Making 'null effects' informative statistical tec.pdf}
}

@book{harrer2021doing,
  title = {Doing Meta-Analysis with {{R}}: A Hands-on Guide},
  author = {Harrer, Mathias and Cuijpers, Pim and A, Furukawa Toshi and Ebert, David D},
  year = {2021},
  edition = {1},
  publisher = {Chapman \& Hall/CRC Press},
  address = {Boca Raton, FL and London},
  isbn = {978-0-367-61007-4}
}

@misc{harrerDmetarCompanionPackage2019,
  title = {Dmetar: {{Companion R Package For The Guide}} '{{Doing Meta-Analysis}} in {{R}}'},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert, David D.},
  year = {2019},
  urldate = {2023-02-02},
  file = {/Users/cristian/Zotero/storage/TTRVMBJ9/authors.html}
}

@article{harrisonRecommendationsStatisticalAnalysis2020,
  title = {Recommendations for Statistical Analysis Involving Null Hypothesis Significance Testing},
  author = {Harrison, Andrew J. and {McErlain-Naylor}, Stuart A. and Bradshaw, Elizabeth J. and Dai, Boyi and Nunome, Hiroyuki and Hughes, Gerwyn T.G. and Kong, Pui W. and Vanwanseele, Benedicte and {Vilas-Boas}, J. Paulo and Fong, Daniel T. P.},
  year = {2020},
  month = sep,
  journal = {Sports Biomechanics},
  volume = {19},
  number = {5},
  pages = {561--568},
  publisher = {Routledge},
  issn = {1476-3141},
  doi = {10.1080/14763141.2020.1782555},
  urldate = {2023-07-31},
  pmid = {32672099},
  file = {/Users/cristian/Zotero/storage/HB2XJT3C/Harrison et al. - 2020 - Recommendations for statistical analysis involving.pdf}
}

@article{hartgerinkDistributionsPvaluesSmaller2016,
  title = {Distributions of P-Values Smaller than .05 in Psychology: What Is Going On?},
  author = {Hartgerink, Chris H.J. and {van Aert}, Robbie C.M. and Nuijten, Mich{\`e}le B. and Wicherts, Jelte M. and {van Assen}, Marcel A.L.M.},
  year = {2016},
  month = apr,
  journal = {PeerJ},
  volume = {4},
  pages = {e1935},
  doi = {10.7717/peerj.1935},
  abstract = {Previous studies provided mixed findings on pecularities in p-value distributions in psychology. This paper examined 258,050 test results across 30,710 articles from eight high impact journals to investigate the existence of a peculiar prevalence of p-values just below .05 (i.e., a bump) in the psychological literature, and a potential increase thereof over time. We indeed found evidence for a bump just below .05 in the distribution of exactly reported p-values in the journals Developmental Psychology, Journal of Applied Psychology, and Journal of Personality and Social Psychology, but the bump did not increase over the years and disappeared when using recalculated p-values. We found clear and direct evidence for the QRP ``incorrect rounding of p-value'' () in all psychology journals. Finally, we also investigated monotonic excess of p-values, an effect of certain QRPs that has been neglected in previous research, and developed two measures to detect this by modeling the distributions of statistically significant p-values. Using simulations and applying the two measures to the retrieved test results, we argue that, although one of the measures suggests the use of QRPs in psychology, it is difficult to draw general conclusions concerning QRPs based on modeling of p-value distributions.},
  file = {/Users/cristian/Zotero/storage/BKYT98GH/Hartgerink et al. - 2016 - Distributions of p-values smaller than .05 in psyc.pdf}
}

@article{havenPerceivedPublicationPressure2019,
  title = {Perceived Publication Pressure in {{Amsterdam}}: {{Survey}} of All Disciplinary Fields and Academic Ranks},
  shorttitle = {Perceived Publication Pressure in {{Amsterdam}}},
  author = {Haven, Tamarinde L. and Bouter, Lex M. and Smulders, Yvo M. and Tijdink, Joeri K.},
  year = {2019},
  month = jun,
  journal = {PLoS ONE},
  volume = {14},
  number = {6},
  pages = {e0217931},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0217931},
  urldate = {2022-03-05},
  abstract = {Publications determine to a large extent the possibility to stay in academia (``publish or perish''). While some pressure to publish may incentivise high quality research, too much publication pressure is likely to have detrimental effects on both the scientific enterprise and on individual researchers. Our research question was: What is the level of perceived publication pressure in the four academic institutions in Amsterdam and does the pressure to publish differ between academic ranks and disciplinary fields? Investigating researchers in Amsterdam with the revised Publication Pressure Questionnaire, we find that a negative attitude towards the current publication climate is present across academic ranks and disciplinary fields. Postdocs and assistant professors (M = 3.42) perceive the greatest publication stress and PhD-students (M = 2.44) perceive a significant lack of resources to relieve publication stress. Results indicate the need for a healthier publication climate where the quality and integrity of research is rewarded.},
  pmcid = {PMC6583945},
  pmid = {31216293},
  file = {/Users/cristian/Zotero/storage/ZESJREXG/Haven et al. - 2019 - Perceived publication pressure in Amsterdam Surve.pdf}
}

@article{headExtentConsequencesPHacking2015,
  title = {The {{Extent}} and {{Consequences}} of {{P-Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {PLOS Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  urldate = {2022-03-22},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  langid = {english},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,Publication ethics,Reproducibility,Statistical data,Test statistics},
  file = {/Users/cristian/Zotero/storage/H7YKRCAL/Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf;/Users/cristian/Zotero/storage/RBDHQ8JL/article.html}
}

@article{heAssociationCoffeeIntake2020,
  title = {Association between Coffee Intake and the Risk of Oral Cavity Cancer: A Meta-Analysis of Observational Studies},
  author = {He, T. and Guo, X. and Li, X. and Liao, C. and Yin, W.},
  year = {2020},
  journal = {European Journal of Cancer Prevention},
  volume = {29},
  number = {1},
  pages = {80--88},
  publisher = {{Lippincott Williams and Wilkins}},
  doi = {10.1097/CEJ.0000000000000515}
}

@article{heckJustification4mmolLactate1985,
  title = {Justification of the 4-Mmol/l Lactate Threshold},
  author = {Heck, H. and Mader, A. and Hess, G. and M{\"u}cke, S. and M{\"u}ller, R. and Hollmann, W.},
  year = {1985},
  month = jun,
  journal = {International Journal of Sports Medicine},
  volume = {6},
  number = {3},
  pages = {117--130},
  issn = {0172-4622},
  doi = {10.1055/s-2008-1025824},
  langid = {english},
  pmid = {4030186},
  keywords = {Adult,Exercise Test,Humans,Lactates,Lactic Acid,Male,Oxygen Consumption,Physical Endurance,Physical Exertion,Respiration,Running,Time Factors}
}

@article{hedgesEstimationEffectSize1984,
  title = {Estimation of {{Effect Size}} under {{Nonrandom Sampling}}: {{The Effects}} of {{Censoring Studies Yielding Statistically Insignificant Mean Differences}}},
  shorttitle = {Estimation of {{Effect Size}} under {{Nonrandom Sampling}}},
  author = {Hedges, Larry V.},
  year = {1984},
  journal = {Journal of Educational Statistics},
  volume = {9},
  number = {1},
  eprint = {1164832},
  eprinttype = {jstor},
  pages = {61--85},
  publisher = {[Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
  issn = {0362-9791},
  doi = {10.2307/1164832},
  urldate = {2022-02-21},
  abstract = {Quantitative research synthesis usually involves the combination of estimates of the standardized mean difference (effect size) derived from independent research studies. In some cases, effect size estimates are available only if the difference between experimental and control group means is statistically significant. If the quantitative result of a study is observed only when the mean difference is statistically significant, the observed mean difference, variance, and effect size are biased estimators of the corresponding population parameters. The exact distribution of the sample effect size is derived for the case in which only studies yielding statistically significant results may be observed. The maximum likelihood estimator of effect size also is derived under the model in which only significant results are observed. The exact distribution of the maximum likelihood estimator is obtained numerically and is used to study the bias of the maximum likelihood estimator. An empirical sampling study is used to supplement the analytic results.}
}

@article{hedgesMoreOneReplication2019,
  title = {More {{Than One Replication Study Is Needed}} for {{Unambiguous Tests}} of {{Replication}}},
  author = {Hedges, Larry V. and Schauer, Jacob M.},
  year = {2019},
  month = oct,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {44},
  number = {5},
  pages = {543--570},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/1076998619852953},
  urldate = {2022-10-19},
  abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98\%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
  langid = {english},
  keywords = {Experimental Design,Experimental Replication,Measurement,Methodology,Sciences,Statistical Power,Statistical Tests},
  file = {/Users/cristian/Zotero/storage/99FUD7G6/2019-53284-002.html}
}

@article{hedgesRandomEffectsModel1983,
  title = {A Random Effects Model for Effect Sizes},
  author = {Hedges, Larry V.},
  year = {1983},
  journal = {Psychological Bulletin},
  volume = {93},
  pages = {388--395},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.93.2.388},
  abstract = {Recent interest in quantitative research synthesis has led to the development of rigorous statistical theory for some of the methods used in meta-analysis. Statistical theory proposed previously has stressed the estimation of fixed but unknown population effect sizes (standardized mean differences). Theoretical considerations often suggest that treatment effects are not fixed but vary across different implementations of a treatment. The present author presents a random effects model (analogous to random effects ANOVA) in which the population effect sizes are not fixed but are sample realizations from a distribution of possible population effect sizes. An analogy to variance component estimation is used to derive an unbiased estimator of the variance of the effect-size distribution. An example shows that these methods may suggest insights that are not available from inspection of means and standard deviation of effect-size estimates. (13 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Estimation,Mathematical Modeling,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/J88R9M52/doiLanding.html}
}

@article{hedgesRobustVarianceEstimation2010,
  title = {Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates},
  author = {Hedges, Larry V. and Tipton, Elizabeth and Johnson, Matthew C.},
  year = {2010},
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {1},
  pages = {39--65},
  issn = {1759-2887},
  doi = {10.1002/jrsm.5},
  urldate = {2025-05-17},
  abstract = {Conventional meta-analytic techniques rely on the assumption that effect size estimates from different studies are independent and have sampling distributions with known conditional variances. The independence assumption is violated when studies produce several estimates based on the same individuals or there are clusters of studies that are not independent (such as those carried out by the same investigator or laboratory). This paper provides an estimator of the covariance matrix of meta-regression coefficients that are applicable when there are clusters of internally correlated estimates. It makes no assumptions about the specific form of the sampling distributions of the effect sizes, nor does it require knowledge of the covariance structure of the dependent estimates. Moreover, this paper demonstrates that the meta-regression coefficients are consistent and asymptotically normally distributed and that the robust variance estimator is valid even when the covariates are random. The theory is asymptotic in the number of studies, but simulations suggest that the theory may yield accurate results with as few as 20--40 studies. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {dependent effects,meta analysis,meta regression,robust standard errors}
}

@article{hedgesStatisticalAnalysesStudying2019,
  title = {Statistical Analyses for Studying Replication: {{Meta-analytic}} Perspectives},
  shorttitle = {Statistical Analyses for Studying Replication},
  author = {Hedges, Larry V. and Schauer, Jacob M.},
  year = {2019},
  month = oct,
  journal = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {557--570},
  issn = {1939-1463},
  doi = {10.1037/met0000189},
  abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of "negligible heterogeneity," and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {30070547},
  keywords = {{Data Interpretation, Statistical},{Models, Statistical},Humans,Meta-Analysis as Topic,Psychology,Reproducibility of Results,Research Design}
}

@article{heEfficacySafetyProgrammed2021,
  title = {The {{Efficacy}} and {{Safety}} of {{Programmed Death-1}} and {{Programmed Death Ligand}} 1 {{Inhibitors}} for the {{Treatment}} of {{Hepatocellular Carcinoma}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  author = {He, S. and Jiang, W. and Fan, K. and Wang, X.},
  year = {2021},
  journal = {Frontiers in Oncology},
  volume = {11},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fonc.2021.626984}
}

@article{hempelStudiesLogicConfirmation1945,
  title = {Studies in the {{Logic}} of {{Confirmation}}},
  author = {Hempel, Carl G.},
  year = {1945},
  journal = {Mind},
  volume = {54},
  number = {213},
  eprint = {2250886},
  eprinttype = {jstor},
  pages = {1--26},
  publisher = {[Oxford University Press, Mind Association]},
  issn = {0026-4423},
  urldate = {2024-01-19},
  file = {/Users/cristian/Zotero/storage/NH6NGS4I/Hempel - 1945 - Studies in the Logic of Confirmation (I.).pdf}
}

@article{heneghanEvidenceUnderpinningSports2012,
  title = {The Evidence Underpinning Sports Performance Products: A Systematic Assessment},
  shorttitle = {The Evidence Underpinning Sports Performance Products},
  author = {Heneghan, Carl and Howick, Jeremy and O'Neill, Braden and Gill, Peter J. and Lasserson, Daniel S. and Cohen, Deborah and Davis, Ruth and Ward, Alison and Smith, Adam and Jones, Greg and Thompson, Matthew},
  year = {2012},
  month = jan,
  journal = {BMJ Open},
  volume = {2},
  number = {4},
  pages = {e001702},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2012-001702},
  urldate = {2021-03-15},
  abstract = {Background To assess the extent and nature of claims regarding improved sports performance made by advertisers for a broad range of sports-related products, and the quality of the evidence on which these claims are based. Methods The authors analysed magazine adverts and associated websites of a broad range of sports products. The authors searched for references supporting the performance and/or recovery claims of these products. The authors critically appraised the methods in the retrieved references by assessing the level of evidence and the risk of bias. The authors also collected information on the included participants, adverse events, study limitations, the primary outcome of interest and whether the intervention had been retested. Results The authors viewed 1035 web pages and identified 431 performance-enhancing claims for 104 different products. The authors found 146 references that underpinned these claims. More than half (52.8\%) of the websites that made performance claims did not provide any references, and the authors were unable to perform critical appraisal for approximately half (72/146) of the identified references. None of the references referred to systematic reviews (level 1 evidence). Of the critically appraised studies, 84\% were judged to be at high risk of bias. Randomisation was used in just over half of the studies (58.1\%), allocation concealment was only clear in five (6.8\%) studies; and blinding of the investigators, outcome assessors or participants was only clearly reported as used in 20 (27.0\%) studies. Only three of the 74 (2.7\%) studies were judged to be of high quality and at low risk of bias. Conclusions The current evidence is not of sufficient quality to inform the public about the benefits and harms of sports products. There is a need to improve the quality and reporting of research, a move towards using systematic review evidence to inform decisions.},
  chapter = {Sports and exercise medicine},
  copyright = {{\copyright} 2012, Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/ and http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {22815461},
  file = {/Users/cristian/Zotero/storage/5HRREMYN/Heneghan et al. - 2012 - The evidence underpinning sports performance produ.pdf;/Users/cristian/Zotero/storage/UYGSFXTQ/e001702.html}
}

@article{hettchenEffectsCompressionTights2019,
  title = {Effects of {{Compression Tights}} on {{Recovery Parameters}} after {{Exercise Induced Muscle Damage}}: {{A Randomized Controlled Crossover Study}}},
  author = {Hettchen, M. and Gl{\"o}ckler, K. and Von Stengel, S. and Piechele, A. and L{\"o}tzerich, H. and Kohl, M. and Kemmler, W.},
  year = {2019},
  journal = {Evidence-based Complementary and Alternative Medicine},
  volume = {2019},
  publisher = {Hindawi Limited},
  doi = {10.1155/2019/5698460}
}

@article{hicksBibliometricsLeidenManifesto2015,
  title = {Bibliometrics: {{The Leiden Manifesto}} for Research Metrics},
  shorttitle = {Bibliometrics},
  author = {Hicks, Diana and Wouters, Paul and Waltman, Ludo and {de Rijcke}, Sarah and Rafols, Ismael},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7548},
  pages = {429--431},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/520429a},
  urldate = {2022-03-16},
  abstract = {Use these ten principles to guide research evaluation, urge Diana Hicks, Paul Wouters and colleagues.},
  copyright = {2015 Nature Publishing Group},
  langid = {english},
  keywords = {Careers,Publishing,Research management},
  file = {/Users/cristian/Zotero/storage/SD2WIWFK/csp_14_295.bib;/Users/cristian/Zotero/storage/ZMWDGIBK/Hicks et al. - 2015 - Bibliometrics The Leiden Manifesto for research m.pdf;/Users/cristian/Zotero/storage/MY7EQQH8/520429a.html}
}

@incollection{higginsChoosingEffectMeasures2019,
  title = {Choosing Effect Measures and Computing Estimates of Effect},
  booktitle = {Cochrane {{Handbook}} for {{Systematic Reviews}} of {{Interventions}}},
  author = {Higgins, Julian PT and Li, Tianjing and Deeks, Jonathan J},
  year = {2019},
  pages = {143--176},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119536604.ch6},
  urldate = {2023-02-01},
  abstract = {A key early step in analysing results of studies of effectiveness is identifying the data type for the outcome measurements. This chapter considers outcome data of five common types: dichotomous (or binary) data, continuous data, ordinal data, count or rate data and time-to-event data. The ways in which the effect of an intervention can be assessed depend on the nature of the data being collected. For each of the types of data, the chapter reviews definitions, properties and interpretation of standard measures of intervention effect, and provides tips on how effect estimates may be computed from data likely to be reported in sources such as journal articles. Formulae to estimate effects for the commonly used effect measures are provided in a supplementary document statistical algorithms in Review Manager, as well as other standard textbooks. Effect measures are either ratio measures or difference measures. Ratio measures are typically analysed on a logarithmic scale.},
  chapter = {6},
  isbn = {978-1-119-53660-4},
  langid = {english},
  keywords = {computing estimates,continuous data,count data,dichotomous data,difference measures,effect measures,ordinal data,outcome measurements,ratio measures,time-to-event data},
  file = {/Users/cristian/Zotero/storage/PBJXMAC9/9781119536604.html}
}

@article{higginsonCurrentIncentivesScientists2016,
  title = {Current {{Incentives}} for {{Scientists Lead}} to {{Underpowered Studies}} with {{Erroneous Conclusions}}},
  author = {Higginson, Andrew D. and Munaf{\`o}, Marcus R.},
  year = {2016},
  month = nov,
  journal = {PLOS Biology},
  volume = {14},
  number = {11},
  pages = {e2000995},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000995},
  urldate = {2022-03-15},
  abstract = {We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10\%--40\% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved.},
  langid = {english},
  keywords = {Careers,Careers in research,Drug discovery,Ecosystems,Peer review,Research assessment,Research errors,Scientists},
  file = {/Users/cristian/Zotero/storage/GMSUTDUX/Higginson and Munaf - 2016 - Current Incentives for Scientists Lead to Underpow.pdf;/Users/cristian/Zotero/storage/8G2HDLD4/article.html}
}

@article{higginsQuantifyingHeterogeneityMetaanalysis2002,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2002},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {11},
  pages = {1539--1558},
  issn = {0277-6715},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  langid = {english},
  pmid = {12111919},
  keywords = {{Chemotherapy, Adjuvant},Albumins,Clinical Trials as Topic,Cognition Disorders,Cytidine Diphosphate Choline,Fibrosis,Fracture Fixation,Hip Fractures,Humans,Meta-Analysis as Topic,Resuscitation,Sarcoma,Sclerotherapy,Statistics as Topic}
}

@article{higginsQuantifyingHeterogeneityMetaanalysis2002a,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  year = {2002},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {11},
  pages = {1539--1558},
  issn = {0277-6715},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  langid = {english},
  pmid = {12111919},
  keywords = {{Chemotherapy, Adjuvant},Albumins,Clinical Trials as Topic,Cognition Disorders,Cytidine Diphosphate Choline,Fibrosis,Fracture Fixation,Hip Fractures,Humans,Meta-Analysis as Topic,Resuscitation,Sarcoma,Sclerotherapy,Statistics as Topic}
}

@misc{hilgardPETPEESE2020,
  title = {{{PETPEESE}}},
  author = {Hilgard, Joe},
  year = {2020},
  month = dec,
  urldate = {2022-05-23},
  abstract = {Functions for R to make PET-PEESE modeling and output easier.}
}

@article{hoekstraProbabilityCertaintyDichotomous2006,
  title = {Probability as Certainty: Dichotomous Thinking and the Misuse of p Values},
  shorttitle = {Probability as Certainty},
  author = {Hoekstra, Rink and Finch, Sue and Kiers, Henk A. L. and Johnson, Addie},
  year = {2006},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {13},
  number = {6},
  pages = {1033--1037},
  issn = {1069-9384},
  doi = {10.3758/bf03213921},
  abstract = {Significance testing is widely used and often criticized. The Task Force on Statistical Inference of the American Psychological Association (TFSI, APA; Wilkinson \& TFSI, 1999) addressed the use of significance testing and made recommendations that were incorporated in the fifth edition of the APA Publication Manual (APA, 2001). They emphasized the interpretation of significance testing and the importance of reporting confidence intervals and effect sizes. We examined whether 286 Psychonomic Bulletin \& Review articles submitted before and after the publication of the TFSI recommendations by APA complied with these recommendations. Interpretation errors when using significance testing were still made frequently, and the new prescriptions were not yet followed on a large scale. Changing the practice of reporting statistics seems doomed to be a slow process.},
  langid = {english},
  pmid = {17484431},
  keywords = {{Models, Statistical},Attention,Humans,Judgment,Reaction Time,Thinking,Time Perception},
  file = {/Users/cristian/Zotero/storage/QE6KLB58/Hoekstra et al. - 2006 - Probability as certainty dichotomous thinking and.pdf}
}

@article{hoenigAbusePowerPervasive2001,
  title = {The {{Abuse}} of {{Power}}: {{The Pervasive Fallacy}} of {{Power Calculations}} for {{Data Analysis}}},
  shorttitle = {The {{Abuse}} of {{Power}}},
  author = {Hoenig, John M and Heisey, Dennis M},
  year = {2001},
  month = feb,
  journal = {The American Statistician},
  volume = {55},
  number = {1},
  pages = {19--24},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313001300339897},
  urldate = {2022-08-09},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/JRVGWPKN/Hoenig and Heisey - 2001 - The Abuse of Power The Pervasive Fallacy of Power.pdf}
}

@article{hoferIntegrativeDataAnalysis2009,
  title = {Integrative Data Analysis through Coordination of Measurement and Analysis Protocol across Independent Longitudinal Studies},
  author = {Hofer, Scott M. and Piccinin, Andrea M.},
  year = {2009},
  month = jun,
  journal = {Psychological Methods},
  volume = {14},
  number = {2},
  pages = {150--164},
  issn = {1082-989X},
  doi = {10.1037/a0015566},
  abstract = {Replication of research findings across independent longitudinal studies is essential for a cumulative and innovative developmental science. Meta-analysis of longitudinal studies is often limited by the amount of published information on particular research questions, the complexity of longitudinal designs and the sophistication of analyses, and practical limits on full reporting of results. In many cases, cross-study differences in sample composition and measurements impede or lessen the utility of pooled data analysis. A collaborative, coordinated analysis approach can provide a broad foundation for cumulating scientific knowledge by facilitating efficient analysis of multiple studies in ways that maximize comparability of results and permit evaluation of study differences. The goal of such an approach is to maximize opportunities for replication and extension of findings across longitudinal studies through open access to analysis scripts and output for published results, permitting modification, evaluation, and extension of alternative statistical models and application to additional data sets. Drawing on the cognitive aging literature as an example, the authors articulate some of the challenges of meta-analytic and pooled-data approaches and introduce a coordinated analysis approach as an important avenue for maximizing the comparability, replication, and extension of results from longitudinal studies.},
  langid = {english},
  pmcid = {PMC2773828},
  pmid = {19485626},
  keywords = {{Factor Analysis, Statistical},Aging,Cognition,Cross-Cultural Comparison,Humans,Longitudinal Studies,Meta-Analysis as Topic,Psychometrics,Research Design,Statistics as Topic},
  file = {/Users/cristian/Zotero/storage/X78E86TD/Hofer and Piccinin - 2009 - Integrative data analysis through coordination of .pdf}
}

@article{hoffmannComparisonTrainingResponses2021,
  title = {Comparison of Training Responses and Performance Adaptations in Endurance-Trained Men and Women Performing High-Intensity Interval Training},
  author = {Hoffmann, Samantha and Skinner, Tina L. and {van Rosendal}, Simon P. and Emmerton, Lynne M. and Jenkins, David G.},
  year = {2021},
  month = may,
  journal = {Journal of Sports Sciences},
  volume = {39},
  number = {9},
  pages = {1010--1020},
  issn = {1466-447X},
  doi = {10.1080/02640414.2020.1853960},
  abstract = {The efficacy of high-intensity interval training (HIIT) to elicit physiological and performance adaptations in endurance athletes has been established in men and to a lesser extent in women. This study compared lactate threshold (LT2) and performance adaptations to HIIT between men and women. Nine male and eight female cyclists and triathletes completed trials to determine their LT2 and 40 km cycling performance before, and after 10 HIIT sessions. Each HIIT session consisted of 10~{\texttimes}~90 s at peak power output, separated by 60 s active recovery. Main effects showed that HIIT improved peak power output (p~=~0.05; ES: 0.2); relative peak power output (W.kg-1; p~=~0.04; ES: 0.3 and W.kg-0.32; p~=~0.04; ES: 0.3); incremental time to fatigue (p~=~0.01; ES: 0.4), time trial time (p~{$<~$}0.001; ES: 0.7) and time trial power output (p~{$<~$}0.001; ES: 0.7) equally in both sexes. Although LT2 power output explained 77\% of the performance improvement in women, no variable explained the performance improvement in men, suggesting another mechanism(s) was involved. Although HIIT improved cycling performance in men and women, it might not be appropriate to evaluate the effectiveness of HIIT using the same variables for both sexes.},
  langid = {english},
  pmid = {33320059},
  keywords = {{Adaptation, Physiological},Adult,anaerobic threshold,Analysis of Variance,Athletic performance,Athletic Performance,Bicycling,Endurance Training,Fatigue,Female,High-Intensity Interval Training,Humans,lactic acid,Lactic Acid,Male,modified D-max,Physical Endurance,Running,Sex Factors,sex-specific adaptations,Swimming,Time Factors,Young Adult}
}

@article{hohenauerEffectPostexerciseCryotherapy2015,
  title = {The Effect of Post-Exercise Cryotherapy on Recovery Characteristics: {{A}} Systematic Review and Meta-Analysis},
  author = {Hohenauer, E. and Taeymans, J. and Baeyens, J.-P. and Clarys, P. and Clijsen, R.},
  year = {2015},
  journal = {PLoS ONE},
  volume = {10},
  number = {9},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0139028}
}

@article{holgadoAssessingEvidentialValue2023,
  title = {Assessing the {{Evidential Value}} of {{Mental Fatigue}} and {{Exercise Research}}},
  author = {Holgado, Dar{\'i}as and Mesquida, Cristian and {Rom{\'a}n-Caballero}, Rafael},
  year = {2023},
  month = dec,
  journal = {Sports Medicine},
  volume = {53},
  number = {12},
  pages = {2293--2307},
  issn = {1179-2035},
  doi = {10.1007/s40279-023-01926-w},
  urldate = {2025-03-29},
  abstract = {It has often been reported that mental exertion, presumably leading to mental fatigue, can negatively affect exercise performance; however, recent findings have questioned the strength of the effect. To further complicate this issue, an overlooked problem might be the presence of publication bias in studies using underpowered designs, which is known to inflate false positive report probability and effect size estimates. Altogether, the presence of bias is likely to reduce the evidential value of the published literature on this topic, although it is unknown to what extent. The purpose of the current work was to assess the evidential value of studies published to date on the effect of mental exertion on exercise performance by assessing the presence of publication bias and the observed statistical power achieved by these studies. A traditional meta-analysis revealed a Cohen's dz effect size of -\,0.54, 95\% CI [-\,0.68, -\,0.40], p\,{$<$}\,.001. However, when we applied methods for estimating and correcting for publication bias (based on funnel plot asymmetry and observed p-values), we found that the bias-corrected effect size became negligible with most of publication-bias methods and decreased to -\,0.36 in the more optimistic of all the scenarios. A robust Bayesian meta-analysis found strong evidence in favor of publication bias, BFpb\,{$>$}\,1000, and inconclusive evidence in favor of the effect, adjusted dz\,=\,0.01, 95\% CrI [-\,0.46, 0.37], BF10\,=\,0.90. Furthermore, the median observed statistical power assuming the unadjusted meta-analytic effect size (i.e., -\,0.54) as the true effect size was 39\% (min\,=\,19\%, max\,=\,96\%), indicating that, on average, these studies only had a 39\% chance of observing a significant result if the true effect was Cohen's dz\,=\,-\,0.54. If the more optimistic adjusted effect size (-\,0.36) was assumed as the true effect, the median statistical power was just 20\%. We conclude that the current literature is a useful case study for illustrating the dangers of conducting underpowered studies to detect the effect size of interest.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/X8NRRZN5/Holgado et al. - 2023 - Assessing the Evidential Value of Mental Fatigue a.pdf}
}

@article{holgadoCorrectionMentalFatigue2021,
  title = {Correction: {{Mental Fatigue Might}} Be {{Not So Bad}} for {{Exercise Performance After All}}: {{A Systematic Review}} and {{Bias-Sensitive Meta-Analysis}}},
  shorttitle = {Correction},
  author = {Holgado, Dar{\'i}as and Sanabria, Daniel and Perales, Jos{\'e} C. and Vadillo, Miguel A.},
  year = {2021},
  journal = {Journal of Cognition},
  volume = {4},
  number = {1},
  pages = {32},
  issn = {2514-4820},
  doi = {10.5334/joc.178},
  abstract = {[This corrects the article DOI: 10.5334/joc.126.].},
  langid = {english},
  pmcid = {PMC8300579},
  pmid = {34327303},
  keywords = {Attention,Cognitive Control,EEG,Emotion and cognition,Face perception},
  file = {/Users/cristian/Zotero/storage/SQ52AS8G/Holgado et al. - 2021 - Correction Mental Fatigue Might be Not So Bad for.pdf}
}

@article{holgadoDoesMentalFatigue2021,
  title = {Does Mental Fatigue Impair Physical Performance? {{A}} Replication Study},
  shorttitle = {Does Mental Fatigue Impair Physical Performance?},
  author = {Holgado, Dar{\'i}as and Troya, Esther and Perales, Jos{\'e} C. and Vadillo, Miguel A. and Sanabria, Daniel},
  year = {2021},
  month = may,
  journal = {European Journal of Sport Science},
  volume = {21},
  number = {5},
  pages = {762--770},
  issn = {1536-7290},
  doi = {10.1080/17461391.2020.1781265},
  abstract = {The aim of this study is to replicate the hypothesis that mental fatigue impairs physical performance in a pre-registered (https://osf.io/wqkap/) within-subjects experiment. 30 recreationally active adults completed a time-to-exhaustion test (TTE) at 80\% VO2max in two separate sessions, after completing a mental fatigue task or watching a documentary for 90 min. We measured power output, heart rate, (session) rating of perceived exertion (RPE) and subjective mental fatigue. Bayes factor analyses revealed extreme evidence supporting the alternative hypothesis that the mental fatigue task was more mentally fatiguing than the control task, BF01 = 0.009. However, we found moderate-to-strong evidence for the null hypothesis (i.e., no evidence of reduced performance) for average time in TTE (BF01 = 9.762) and anecdotal evidence for the null hypothesis in (session) RPE (BF01 = 2.902) and heart rate (BF01 = 2.587). Our data seem to challenge the idea that mental fatigue has a negative influence on exercise performance. Although we did succeed at manipulating subjective mental fatigue, this did not impair physical performance. However, we cannot discard the possibility that mental fatigue may have a negative influence under conditions not explored here, e.g., individualizing mentally fatiguing tasks. In sum, further research is warranted to determine the role of mental fatigue on exercise and sport performance.},
  langid = {english},
  pmid = {32519588},
  keywords = {Adult,Athletic Performance,Bayes Theorem,Cognition,endurance,Exercise Test,fatigue,Female,Heart Rate,Humans,Male,Mental Fatigue,Motivation,neuroscience,Perception,Physical Exertion,Physical Functional Performance,psychology,Reproducibility of Results,Time Factors,Young Adult},
  file = {/Users/cristian/Zotero/storage/BRAXDTES/Holgado et al. - 2021 - Does mental fatigue impair physical performance A.pdf}
}

@article{holgadoMentalFatigueMight,
  title = {Mental {{Fatigue Might Be Not So Bad}} for {{Exercise Performance After All}}: {{A Systematic Review}} and {{Bias-Sensitive Meta-Analysis}}},
  shorttitle = {Mental {{Fatigue Might Be Not So Bad}} for {{Exercise Performance After All}}},
  author = {Holgado, Dar{\'i}as and Sanabria, Daniel and Perales, Jos{\'e} C. and Vadillo, Miguel A.},
  journal = {Journal of Cognition},
  volume = {3},
  number = {1},
  pages = {38},
  issn = {2514-4820},
  doi = {10.5334/joc.126},
  urldate = {2022-04-12},
  abstract = {There is an ongoing debate in the scientific community regarding whether a state of mental fatigue may have a negative effect upon a range of objective and subjective measures of human performance. This issue has attracted attention from several fields, including sport and exercise sciences. In fact, a considerable body of literature in the sport science field has suggested that performing a long and demanding cognitive task might lead to a state of mental fatigue, impairing subsequent exercise performance, although research in this field has shown contradictory results. Here, we performed a meta-analysis to investigate these inconsistent findings. The analysis yielded small-to-medium effects of mental fatigue on exercise performance, dz = 0.50, and RPE, dz = 0.21. However, a three-parameter selection model also revealed evidence of publication or reporting biases, suggesting that the bias-corrected estimates might be substantially lower (0.08 and 0.10, respectively) and non-significant. In sum, current evidence does not provide conclusive support for the claim that mental fatigue has a negative influence on exercise performance.},
  pmcid = {PMC7546119},
  pmid = {33103052},
  file = {/Users/cristian/Zotero/storage/ZTIMVCZX/Holgado et al. - Mental Fatigue Might Be Not So Bad for Exercise Pe.pdf}
}

@article{holmanEvidenceExperimentalBias2015,
  title = {Evidence of {{Experimental Bias}} in the {{Life Sciences}}: {{Why We Need Blind Data Recording}}},
  shorttitle = {Evidence of {{Experimental Bias}} in the {{Life Sciences}}},
  author = {Holman, Luke and Head, Megan L. and Lanfear, Robert and Jennions, Michael D.},
  year = {2015},
  month = jul,
  journal = {PLoS biology},
  volume = {13},
  number = {7},
  pages = {e1002190},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002190},
  abstract = {Observer bias and other "experimenter effects" occur when researchers' expectations influence study outcome. These biases are strongest when researchers expect a particular result, are measuring subjective variables, and have an incentive to produce data that confirm predictions. To minimize bias, it is good practice to work "blind," meaning that experimenters are unaware of the identity or treatment group of their subjects while conducting research. Here, using text mining and a literature review, we find evidence that blind protocols are uncommon in the life sciences and that nonblind studies tend to report higher effect sizes and more significant p-values. We discuss methods to minimize bias and urge researchers, editors, and peer reviewers to keep blind protocols in mind.},
  langid = {english},
  pmcid = {PMC4496034},
  pmid = {26154287},
  keywords = {Biology,Data Collection,Data Mining},
  file = {/Users/cristian/Zotero/storage/AE7QXXV8/Holman et al. - 2015 - Evidence of Experimental Bias in the Life Sciences.pdf}
}

@article{honekoppHeterogeneityEstimatesBiased2022,
  title = {Heterogeneity Estimates in a Biased World},
  author = {H{\"o}nekopp, Johannes and Linden, Audrey Helen},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0262809},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0262809},
  urldate = {2022-05-02},
  abstract = {Meta-analyses typically quantify heterogeneity of results, thus providing information about the consistency of the investigated effect across studies. Numerous heterogeneity estimators have been devised. Past evaluations of their performance typically presumed lack of bias in the set of studies being meta-analysed, which is often unrealistic. The present study used computer simulations to evaluate five heterogeneity estimators under a range of research conditions broadly representative of meta-analyses in psychology, with the aim to assess the impact of biases in sets of primary studies on estimates of both mean effect size and heterogeneity in meta-analyses of continuous outcome measures. To this end, six orthogonal design factors were manipulated: Strength of publication bias; 1-tailed vs. 2-tailed publication bias; prevalence of p-hacking; true heterogeneity of the effect studied; true average size of the studied effect; and number of studies per meta-analysis. Our results showed that biases in sets of primary studies caused much greater problems for the estimation of effect size than for the estimation of heterogeneity. For the latter, estimation bias remained small or moderate under most circumstances. Effect size estimations remained virtually unaffected by the choice of heterogeneity estimator. For heterogeneity estimates, however, relevant differences emerged. For unbiased primary studies, the REML estimator and (to a lesser extent) the Paule-Mandel performed well in terms of bias and variance. In biased sets of primary studies however, the Paule-Mandel estimator performed poorly, whereas the DerSimonian-Laird estimator and (to a slightly lesser extent) the REML estimator performed well. The complexity of results notwithstanding, we suggest that the REML estimator remains a good choice for meta-analyses of continuous outcome measures across varied circumstances.},
  langid = {english},
  keywords = {Computer modeling,Inertia,Metaanalysis,Monte Carlo method,Psychology,Publication ethics,Simulation and modeling,Statistical data},
  file = {/Users/cristian/Zotero/storage/STPG3BBA/Hnekopp and Linden - 2022 - Heterogeneity estimates in a biased world.pdf;/Users/cristian/Zotero/storage/3JC9FWYW/article.html}
}

@article{hongUsingMonteCarlo2021,
  title = {Using {{Monte Carlo}} Experiments to Select Meta-Analytic Estimators},
  author = {Hong, Sanghyun and Reed, W. Robert},
  year = {2021},
  journal = {Research Synthesis Methods},
  volume = {12},
  number = {2},
  pages = {192--215},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1467},
  urldate = {2022-09-26},
  abstract = {The purpose of this study is to show how Monte Carlo analysis of meta-analytic estimators can be used to select estimators for specific research situations. Our analysis conducts 1620 individual experiments, where each experiment is defined by a unique combination of sample size, effect size, effect size heterogeneity, publication selection mechanism, and other research characteristics. We compare 11 estimators commonly used in medicine, psychology, and the social sciences. These are evaluated on the basis of bias, mean squared error (MSE), and coverage rates. For our experimental design, we reproduce simulation environments from four recent studies. We demonstrate that relative estimator performance differs across performance measures. Estimator performance is a complex interaction of performance indicator and aspects of the application. An estimator that may be especially good with respect to MSE may perform relatively poorly with respect to coverage rates. We also show that the size of the meta-analyst's sample and effect heterogeneity are important determinants of relative estimator performance. We use these results to demonstrate how these observable characteristics can guide the meta-analyst to choose the most appropriate estimator for their research circumstances.},
  langid = {english},
  keywords = {estimator performance,experiments,meta-analysis,Monte Carlo,publication bias,simulation design},
  file = {/Users/cristian/Zotero/storage/5VLCSHSX/Hong and Reed - 2021 - Using Monte Carlo experiments to select meta-analy.pdf;/Users/cristian/Zotero/storage/A2R47WRE/jrsm.html}
}

@article{hopewellCONSORT2025Statement2025,
  title = {{{CONSORT}} 2025 Statement: Updated Guideline for Reporting Randomised Trials.},
  author = {Hopewell, Sally and Chan, An-Wen and Collins, Gary S. and Hr{\'o}bjartsson, Asbj{\o}rn and Moher, David and Schulz, Kenneth F. and Tunn, Ruth and Aggarwal, Rakesh and Berkwits, Michael and Berlin, Jesse A. and Bhandari, Nita and Butcher, Nancy J. and Campbell, Marion K. and Chidebe, Runcie C. W. and Elbourne, Diana and Farmer, Andrew and Fergusson, Dean A. and Golub, Robert M. and Goodman, Steven N. and Hoffmann, Tammy C. and Ioannidis, John P. A. and Kahan, Brennan C. and Knowles, Rachel L. and Lamb, Sarah E. and Lewis, Steff and Loder, Elizabeth and Offringa, Martin and Ravaud, Philippe and Richards, Dawn P. and Rockhold, Frank W. and Schriger, David L. and Siegfried, Nandi L. and Staniszewska, Sophie and Taylor, Rod S. and Thabane, Lehana and Torgerson, David and Vohra, Sunita and White, Ian R. and Boutron, Isabelle},
  year = {2025},
  month = apr,
  journal = {BMJ (Clinical research ed.)},
  volume = {389},
  pages = {e081123},
  address = {England},
  issn = {1756-1833 0959-8138},
  doi = {10.1136/bmj-2024-081123},
  abstract = {BACKGROUND: Well designed and properly executed randomised trials are considered the most reliable evidence on the benefits of healthcare interventions. However,  there is overwhelming evidence that the quality of reporting is not optimal. The  CONSORT (Consolidated Standards of Reporting Trials) statement was designed to  improve the quality of reporting and provides a minimum set of items to be  included in a report of a randomised trial. CONSORT was first published in 1996,  then updated in 2001 and 2010. Here, we present the updated CONSORT 2025  statement, which aims to account for recent methodological advancements and  feedback from end users. METHODS: We conducted a scoping review of the literature  and developed a project-specific database of empirical and theoretical evidence  related to CONSORT, to generate a list of potential changes to the checklist. The  list was enriched with recommendations provided by the lead authors of existing  CONSORT extensions (Harms, Outcomes, Non-pharmacological Treatment), other  related reporting guidelines (TIDieR) and recommendations from other sources (eg,  personal communications). The list of potential changes to the checklist was  assessed in a large, international, online, three-round Delphi survey involving  317 participants and discussed at a two-day online expert consensus meeting of 30  invited international experts. RESULTS: We have made substantive changes to the  CONSORT checklist. We added seven new checklist items, revised three items,  deleted one item, and integrated several items from key CONSORT extensions. We  also restructured the CONSORT checklist, with a new section on open science. The  CONSORT 2025 statement consists of a 30-item checklist of essential items that  should be included when reporting the results of a randomised trial and a diagram  for documenting the flow of participants through the trial. To facilitate  implementation of CONSORT 2025, we have also developed an expanded version of the  CONSORT 2025 checklist, with bullet points eliciting critical elements of each  item. CONCLUSION: Authors, editors, reviewers, and other potential users should  use CONSORT 2025 when writing and evaluating manuscripts of randomised trials to  ensure that trial reports are clear and transparent.},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmcid = {PMC11995449},
  pmid = {40228833},
  keywords = {*Guidelines as Topic,*Publishing/standards,*Randomized Controlled Trials as Topic/standards,*Research Design/standards,*Research Report/standards,Checklist/standards,Delphi Technique,Humans}
}

@article{hopewellCONSORT2025Statement2025a,
  title = {{{CONSORT}} 2025 Statement: Updated Guideline for Reporting Randomised Trials.},
  author = {Hopewell, Sally and Chan, An-Wen and Collins, Gary S. and Hr{\'o}bjartsson, Asbj{\o}rn and Moher, David and Schulz, Kenneth F. and Tunn, Ruth and Aggarwal, Rakesh and Berkwits, Michael and Berlin, Jesse A. and Bhandari, Nita and Butcher, Nancy J. and Campbell, Marion K. and Chidebe, Runcie C. W. and Elbourne, Diana and Farmer, Andrew and Fergusson, Dean A. and Golub, Robert M. and Goodman, Steven N. and Hoffmann, Tammy C. and Ioannidis, John P. A. and Kahan, Brennan C. and Knowles, Rachel L. and Lamb, Sarah E. and Lewis, Steff and Loder, Elizabeth and Offringa, Martin and Ravaud, Philippe and Richards, Dawn P. and Rockhold, Frank W. and Schriger, David L. and Siegfried, Nandi L. and Staniszewska, Sophie and Taylor, Rod S. and Thabane, Lehana and Torgerson, David and Vohra, Sunita and White, Ian R. and Boutron, Isabelle},
  year = {2025},
  month = apr,
  journal = {BMJ (Clinical research ed.)},
  volume = {389},
  pages = {e081123},
  address = {England},
  issn = {1756-1833 0959-8138},
  doi = {10.1136/bmj-2024-081123},
  abstract = {BACKGROUND: Well designed and properly executed randomised trials are considered the most reliable evidence on the benefits of healthcare interventions. However,  there is overwhelming evidence that the quality of reporting is not optimal. The  CONSORT (Consolidated Standards of Reporting Trials) statement was designed to  improve the quality of reporting and provides a minimum set of items to be  included in a report of a randomised trial. CONSORT was first published in 1996,  then updated in 2001 and 2010. Here, we present the updated CONSORT 2025  statement, which aims to account for recent methodological advancements and  feedback from end users. METHODS: We conducted a scoping review of the literature  and developed a project-specific database of empirical and theoretical evidence  related to CONSORT, to generate a list of potential changes to the checklist. The  list was enriched with recommendations provided by the lead authors of existing  CONSORT extensions (Harms, Outcomes, Non-pharmacological Treatment), other  related reporting guidelines (TIDieR) and recommendations from other sources (eg,  personal communications). The list of potential changes to the checklist was  assessed in a large, international, online, three-round Delphi survey involving  317 participants and discussed at a two-day online expert consensus meeting of 30  invited international experts. RESULTS: We have made substantive changes to the  CONSORT checklist. We added seven new checklist items, revised three items,  deleted one item, and integrated several items from key CONSORT extensions. We  also restructured the CONSORT checklist, with a new section on open science. The  CONSORT 2025 statement consists of a 30-item checklist of essential items that  should be included when reporting the results of a randomised trial and a diagram  for documenting the flow of participants through the trial. To facilitate  implementation of CONSORT 2025, we have also developed an expanded version of the  CONSORT 2025 checklist, with bullet points eliciting critical elements of each  item. CONCLUSION: Authors, editors, reviewers, and other potential users should  use CONSORT 2025 when writing and evaluating manuscripts of randomised trials to  ensure that trial reports are clear and transparent.},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmcid = {PMC11995449},
  pmid = {40228833},
  keywords = {*Guidelines as Topic,*Publishing/standards,*Randomized Controlled Trials as Topic/standards,*Research Design/standards,*Research Report/standards,Checklist/standards,Delphi Technique,Humans}
}

@article{hopkinsProgressiveStatisticsStudies2009,
  title = {Progressive Statistics for Studies in Sports Medicine and Exercise Science},
  author = {Hopkins, William G. and Marshall, Stephen W. and Batterham, Alan M. and Hanin, Juri},
  year = {2009},
  month = jan,
  journal = {Medicine and Science in Sports and Exercise},
  volume = {41},
  number = {1},
  pages = {3--13},
  issn = {1530-0315},
  doi = {10.1249/MSS.0b013e31818cb278},
  abstract = {Statistical guidelines and expert statements are now available to assist in the analysis and reporting of studies in some biomedical disciplines. We present here a more progressive resource for sample-based studies, meta-analyses, and case studies in sports medicine and exercise science. We offer forthright advice on the following controversial or novel issues: using precision of estimation for inferences about population effects in preference to null-hypothesis testing, which is inadequate for assessing clinical or practical importance; justifying sample size via acceptable precision or confidence for clinical decisions rather than via adequate power for statistical significance; showing SD rather than SEM, to better communicate the magnitude of differences in means and nonuniformity of error; avoiding purely nonparametric analyses, which cannot provide inferences about magnitude and are unnecessary; using regression statistics in validity studies, in preference to the impractical and biased limits of agreement; making greater use of qualitative methods to enrich sample-based quantitative projects; and seeking ethics approval for public access to the depersonalized raw data of a study, to address the need for more scrutiny of research and better meta-analyses. Advice on less contentious issues includes the following: using covariates in linear models to adjust for confounders, to account for individual differences, and to identify potential mechanisms of an effect; using log transformation to deal with nonuniformity of effects and error; identifying and deleting outliers; presenting descriptive, effect, and inferential statistics in appropriate formats; and contending with bias arising from problems with sampling, assignment, blinding, measurement error, and researchers' prejudices. This article should advance the field by stimulating debate, promoting innovative approaches, and serving as a useful checklist for authors, reviewers, and editors.},
  langid = {english},
  pmid = {19092709},
  keywords = {{Data Interpretation, Statistical},Biomedical Research,Exercise,Humans,Research Design,Sample Size,Sports Medicine}
}

@misc{HowManyParticipants,
  title = {How {{Many Participants Do I Need}} to {{Test}} an {{Interaction}}? {{Conducting}} an {{Appropriate Power Analysis}} and {{Achieving Sufficient Power}} to {{Detect}} an {{Interaction}} - {{Nicolas Sommet}}, {{David L}}. {{Weissman}}, {{Nicolas Cheutin}}, {{Andrew J}}. {{Elliot}}, 2023},
  urldate = {2024-09-30},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/25152459231178728},
  file = {/Users/cristian/Zotero/storage/UF4U3ZUN/25152459231178728.html}
}

@misc{HowManyParticipantsa,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables {\textbar} {{Journal}} of {{Cognition}}},
  urldate = {2024-10-04},
  howpublished = {https://journalofcognition.org/articles/10.5334/joc.72},
  file = {/Users/cristian/Zotero/storage/GRKD3EP5/joc.html}
}

@misc{HowRunLinear,
  title = {How to {{Run Linear Mixed Effects Analysis}} for {{Pairwise Comparisons}}? {{A Tutorial}} and a {{Proposal}} for the {{Calculation}} of {{Standardized Effect Sizes}} {\textbar} {{Journal}} of {{Cognition}}},
  urldate = {2025-09-01},
  howpublished = {https://journalofcognition.org/articles/10.5334/joc.409},
  file = {/Users/cristian/Zotero/storage/55FMSQZZ/joc.html}
}

@article{hristovSymposiumReviewEffective2022,
  title = {Symposium Review: {{Effective}} Nutritional Strategies to Mitigate Enteric Methane in Dairy Cattle},
  author = {Hristov, A.N. and Melgar, A. and Wasson, D. and Arndt, C.},
  year = {2022},
  journal = {Journal of Dairy Science},
  volume = {105},
  number = {10},
  pages = {8543--8557},
  publisher = {Elsevier Inc.},
  doi = {10.3168/jds.2021-21398}
}

@article{hrobjartssonObserverBiasRandomised2012,
  title = {Observer Bias in Randomised Clinical Trials with Binary Outcomes: Systematic Review of Trials with Both Blinded and Non-Blinded Outcome Assessors},
  shorttitle = {Observer Bias in Randomised Clinical Trials with Binary Outcomes},
  author = {Hr{\'o}bjartsson, Asbj{\o}rn and Thomsen, Ann Sofia Skou and Emanuelsson, Frida and Tendal, Britta and Hilden, J{\o}rgen and Boutron, Isabelle and Ravaud, Philippe and Brorson, Stig},
  year = {2012},
  month = feb,
  journal = {BMJ (Clinical research ed.)},
  volume = {344},
  pages = {e1119},
  issn = {1756-1833},
  doi = {10.1136/bmj.e1119},
  abstract = {OBJECTIVE: To evaluate the impact of non-blinded outcome assessment on estimated treatment effects in randomised clinical trials with binary outcomes. DESIGN: Systematic review of trials with both blinded and non-blinded assessment of the same binary outcome. For each trial we calculated the ratio of the odds ratios--the odds ratio from non-blinded assessments relative to the corresponding odds ratio from blinded assessments. A ratio of odds ratios {$<$}1 indicated that non-blinded assessors generated more optimistic effect estimates than blinded assessors. We pooled the individual ratios of odds ratios with inverse variance random effects meta-analysis and explored reasons for variation in ratios of odds ratios with meta-regression. We also analysed rates of agreement between blinded and non-blinded assessors and calculated the number of patients needed to be reclassified to neutralise any bias. DATA SOURCES: PubMed, Embase, PsycINFO, CINAHL, Cochrane Central Register of Controlled Trials, HighWire Press, and Google Scholar. ELIGIBILITY CRITERIA FOR SELECTING STUDIES: Randomised clinical trials with blinded and non-blinded assessment of the same binary outcome. RESULTS: We included 21 trials in the main analysis (with 4391 patients); eight trials provided individual patient data. Outcomes in most trials were subjective--for example, qualitative assessment of the patient's function. The ratio of the odds ratios ranged from 0.02 to 14.4. The pooled ratio of odds ratios was 0.64 (95\% confidence interval 0.43 to 0.96), indicating an average exaggeration of the non-blinded odds ratio by 36\%. We found no significant association between low ratios of odds ratios and scores for outcome subjectivity (P=0.27); non-blinded assessor's overall involvement in the trial (P=0.60); or outcome vulnerability to non-blinded patients (P=0.52). Blinded and non-blinded assessors agreed in a median of 78\% of assessments (interquartile range 64-90\%) in the 12 trials with available data. The exaggeration of treatment effects associated with non-blinded assessors was induced by the misclassification of a median of 3\% of the assessed patients per trial (1-7\%). CONCLUSIONS: On average, non-blinded assessors of subjective binary outcomes generated substantially biased effect estimates in randomised clinical trials, exaggerating odds ratios by 36\%. This bias was compatible with a high rate of agreement between blinded and non-blinded outcome assessors and driven by the misclassification of few patients.},
  langid = {english},
  pmid = {22371859},
  keywords = {{Outcome Assessment, Health Care},Double-Blind Method,Humans,Observer Variation,Odds Ratio,Randomized Controlled Trials as Topic,Single-Blind Method,Treatment Outcome},
  file = {/Users/cristian/Zotero/storage/VYNI3LNF/Hrbjartsson et al. - 2012 - Observer bias in randomised clinical trials with b.pdf}
}

@article{hrobjartssonObserverBiasRandomized2013,
  title = {Observer Bias in Randomized Clinical Trials with Measurement Scale Outcomes: A Systematic Review of Trials with Both Blinded and Nonblinded Assessors},
  shorttitle = {Observer Bias in Randomized Clinical Trials with Measurement Scale Outcomes},
  author = {Hr{\'o}bjartsson, Asbj{\o}rn and Thomsen, Ann Sofia Skou and Emanuelsson, Frida and Tendal, Britta and Hilden, J{\o}rgen and Boutron, Isabelle and Ravaud, Philippe and Brorson, Stig},
  year = {2013},
  month = mar,
  journal = {CMAJ},
  volume = {185},
  number = {4},
  pages = {E201-E211},
  publisher = {CMAJ},
  issn = {0820-3946, 1488-2329},
  doi = {10.1503/cmaj.120744},
  urldate = {2022-11-14},
  abstract = {Background: Clinical trials are commonly done without blinded outcome assessors despite the risk of bias. We wanted to evaluate the effect of nonblinded outcome assessment on estimated effects in randomized clinical trials with outcomes that involved subjective measurement scales. Methods: We conducted a systematic review of randomized clinical trials with both blinded and nonblinded assessment of the same measurement scale outcome. We searched PubMed, EMBASE, PsycINFO, CINAHL, Cochrane Central Register of Controlled Trials, HighWire Press and Google Scholar for relevant studies. Two investigators agreed on the inclusion of trials and the outcome scale. For each trial, we calculated the difference in effect size (i.e., standardized mean difference between nonblinded and blinded assessments). A difference in effect size of less than 0 suggested that nonblinded assessors generated more optimistic estimates of effect. We pooled the differences in effect size using inverse variance random-effects meta-analysis and used metaregression to identify potential reasons for variation. Results: We included 24 trials in our review. The main meta-analysis included 16 trials (involving 2854 patients) with subjective outcomes. The estimated treatment effect was more beneficial when based on nonblinded assessors (pooled difference in effect size -0.23 [95\% confidence interval (CI) -0.40 to -0.06]). In relative terms, nonblinded assessors exaggerated the pooled effect size by 68\% (95\% CI 14\% to 230\%). Heterogeneity was moderate (I2 = 46\%, p = 0.02) and unexplained by metaregression. Interpretation: We provide empirical evidence for observer bias in randomized clinical trials with subjective measurement scale outcomes. A failure to blind assessors of outcomes in such trials results in a high risk of substantial bias.},
  chapter = {Research},
  copyright = {{\copyright} 2013 Canadian Medical Association or its licensors},
  langid = {english},
  pmid = {23359047},
  file = {/Users/cristian/Zotero/storage/7XHKXYML/Hrbjartsson et al. - 2013 - Observer bias in randomized clinical trials with m.pdf;/Users/cristian/Zotero/storage/X5ACBPD5/E201.html}
}

@article{huangAntiobesityEffectsGreen2014,
  title = {The Anti-Obesity Effects of Green Tea in Human Intervention and Basic Molecular Studies},
  author = {Huang, J. and Wang, Y. and Xie, Z. and Zhou, Y. and Zhang, Y. and Wan, X.},
  year = {2014},
  journal = {European Journal of Clinical Nutrition},
  volume = {68},
  number = {10},
  pages = {1075--1087},
  publisher = {Nature Publishing Group},
  doi = {10.1038/ejcn.2014.143}
}

@article{huangEffectivenessDrugInterventions2021,
  title = {Effectiveness of Drug Interventions in Nonalcoholic Fatty Liver Disease: {{A}} Network Meta-Analysis.},
  author = {Huang, Yi-Zhou and Yang, Gang-Yi and Wang, Cong and Chen, Xing-Yu and Zhang, Li-Li},
  year = {2021},
  month = sep,
  journal = {World journal of diabetes},
  volume = {12},
  number = {9},
  pages = {1576--1586},
  address = {United States},
  issn = {1948-9358},
  doi = {10.4239/wjd.v12.i9.1576},
  abstract = {BACKGROUND: Nonalcoholic fatty liver disease (NAFLD) is a major chronic liver disorder worldwide, and there is no established treatment for this disease. We  conducted a network meta-analysis (NMA) to compare existing treatments, which  include four classes of antidiabetic drugs, and examined the optimum treatments  for NAFLD. AIM: To compare the effectiveness of different treatments for NAFLD.  METHODS: An NMA was conducted using Stata 14.0 (Corporation LLC, College Station,  United States) and R (X64 3.6.3 version) in this study. Eligible randomized  controlled trials (RCTs) were searched in the PubMed, Cochrane Library, Embase,  Medline and Web of Science databases from database inception to April 2021. Two  researchers independently screened the available studies in strict accordance  with inclusion and exclusion criteria. The Cochrane Risk of Bias tool was used to  evaluate the risk of bias of the included studies. The variables with and without  dimensional differences were calculated as the standardized mean difference and  weighted mean difference, respectively. An inconsistency model and  "node-splitting" technique were used to test for inconsistency. Funnel plots were  used to evaluate publication bias. RESULTS: Twenty-two eligible RCTs involving  1377 participants were eventually included in our analysis. Data were pooled  using a random-effects model. Our NMA results revealed that glucagon-like  peptide-1 receptor agonists (GLP-1RAs) were the most effective treatment,  yielding improvements in hepatic fat content (HFC), alanine aminotransferase  (ALT), aspartate aminotransferase (AST), serum {$\gamma$}-glutamyl transferase (GGT) and  body weight [surface under the cumulative ranking curve (SUCRA) = 99.6\%, 92.6\%,  82.8\%, 92.3\% and 99.6\%, respectively], while thiazolidinediones (TZDs) were the  best intervention for reducing the NAFLD activity score (NAS; SUCRA = 98.9\%). In  addition, moderate performance was observed for the sodium glucose  cotransporter-2 inhibitors groups (SUCRA = 25.1\%, 66.2\%, 63.5\%, 58.2\% and 71.9\%  for HFC, ALT, AST, GGT and body weight, respectively). However, metformin  performed poorly according to most indicators (SUCRA = 54.5\%, 0.3\%, 19.5\%, 33.7\%,  57.7\% and 44.3\% for HFC, NAS, ALT, AST, GGT and body weight, respectively).  CONCLUSION: GLP-1RAs may be the optimum choice for most patients with NAFLD.  However, TZDs are considered the most effective therapies in NAFLD patients with  histological disease activity.},
  copyright = {{\copyright}The Author(s) 2021. Published by Baishideng Publishing Group Inc. All rights reserved.},
  langid = {english},
  pmcid = {PMC8472495},
  pmid = {34630909},
  keywords = {Antidiabetic drugs,Glucagon-like peptide-1 receptor agonists,Network meta-analysis,Nonalcoholic fatty liver disease,Thiazolidinediones}
}

@article{huangEfficacySafetyApatinib2022,
  title = {Efficacy and Safety of Apatinib Monotherapy for Patients with Advanced Breast Cancer: A Systematic Review and Meta-Analysis},
  author = {Huang, X. and Hu, X. and Yi, T.},
  year = {2022},
  journal = {Frontiers in Oncology},
  volume = {12},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fonc.2022.940171}
}

@article{hubbardConfusionMeasuresEvidence2003,
  title = {Confusion over {{Measures}} of {{Evidence}} (p's) versus {{Errors}} ({$\alpha$}'s) in {{Classical Statistical Testing}}},
  author = {Hubbard, Raymond and Bayarri, M. J. and Berk, Kenneth N. and Carlton, Matthew A.},
  year = {2003},
  journal = {The American Statistician},
  volume = {57},
  number = {3},
  eprint = {30037265},
  eprinttype = {jstor},
  pages = {171--182},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  urldate = {2022-03-29},
  abstract = {Confusion surrounding the reporting and interpretation of results of classical statistical tests is widespread among applied researchers, most of whom erroneously believe that such tests are prescribed by a single coherent theory of statistical inference. This is not the case: Classical statistical testing is an anonymous hybrid of the competing and frequently contradictory approaches formulated by R. A. Fisher on the one hand, and Jerzy Neyman and Egon Pearson on the other. In particular, there is a widespread failure to appreciate the incompatibility of Fisher's evidential p value with the Type I error rate, a, of Neyman-Pearson statistical orthodoxy. The distinction between evidence (p's) and error ({$\alpha$}'s) is not trivial. Instead, it reflects the fundamental differences between Fisher's ideas on significance testing and inductive inference, and Neyman-Pearson's views on hypothesis testing and inductive behavior. The emphasis of the article is to expose this incompatibility, but we also briefly note a possible reconciliation.}
}

@article{hubbardWhyValuesAre2008,
  title = {Why {{P Values Are Not}} a {{Useful Measure}} of {{Evidence}} in {{Statistical Significance Testing}}},
  author = {Hubbard, Raymond and Lindsay, R. Murray},
  year = {2008},
  month = feb,
  journal = {Theory \& Psychology},
  volume = {18},
  number = {1},
  pages = {69--88},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/0959354307086923},
  urldate = {2023-03-07},
  abstract = {Reporting p values from statistical significance tests is common in psychology's empirical literature. Sir Ronald Fisher saw the p value as playing a useful role in knowledge development by acting as an `objective' measure of inductive evidence against the null hypothesis. We review several reasons why the p value is an unobjective and inadequate measure of evidence when statistically testing hypotheses. A common theme throughout many of these reasons is that p values exaggerate the evidence against H0. This, in turn, calls into question the validity of much published work based on comparatively small, including .05, p values. Indeed, if researchers were fully informed about the limitations of the p value as a measure of evidence, this inferential index could not possibly enjoy its ongoing ubiquity. Replication with extension research focusing on sample statistics, effect sizes, and their confidence intervals is a better vehicle for reliable knowledge development than using p values. Fisher would also have agreed with the need for replication research.},
  langid = {english}
}

@article{huckUsingRepeatedMeasures1975,
  title = {Using a Repeated Measures {{ANOVA}} to Analyze the Data from a Pretest-Posttest Design: {{A}} Potentially Confusing Task},
  shorttitle = {Using a Repeated Measures {{ANOVA}} to Analyze the Data from a Pretest-Posttest Design},
  author = {Huck, Schuyler W. and McLean, Robert A.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {4},
  pages = {511--518},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0076767},
  abstract = {The pretest-posttest control group design (or an extension of it) is a highly prestigious experimental design. A popular analytic strategy involves subjecting the data provided by this design to a repeated measures analysis of variance (ANOVA). Unfortunately, the statistical results yielded by this type of analysis can easily be misinterpreted, since the score model underlying the analysis is not correct. Examples from recently published articles are used to demonstrate that this statistical procedure has led to (a) incorrect statements regarding treatment effects, (b) completely redundant reanalyses of the same data, and (c) problems with respect to post hoc investigations. 2 alternative strategies-gain scores and covariance-are discussed and compared. (19 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Variance,Experimental Design},
  file = {/Users/cristian/Zotero/storage/KNCYGZUM/1975-26619-001.html}
}

@article{huedo-medinaAssessingHeterogeneityMetaanalysis2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I2}} Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {{Huedo-Medina}, Tania B. and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and Botella, Juan},
  year = {2006},
  month = jun,
  journal = {Psychological Methods},
  volume = {11},
  number = {2},
  pages = {193--206},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.11.2.193},
  abstract = {In meta-analysis, the usual way of assessing whether a set of single studies is homogeneous is by means of the Q test. However, the Q test only informs meta-analysts about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I(2) index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this article, the performances of the Q test and the confidence interval around the I(2) index are compared by means of a Monte Carlo simulation. The results show the utility of the I(2) index as a complement to the Q test, although it has the same problems of power with a small number of studies.},
  langid = {english},
  pmid = {16784338},
  keywords = {{Data Interpretation, Statistical},{Effect Modifier, Epidemiologic},Analysis of Variance,Bias,Computer Simulation,Confidence Intervals,Controlled Clinical Trials as Topic,Humans,Meta-Analysis as Topic,Monte Carlo Method},
  file = {/Users/cristian/Zotero/storage/JKB5IGP7/Huedo-Medina et al. - 2006 - Assessing heterogeneity in meta-analysis Q statis.pdf}
}

@article{huffmanNeuropsychiatricConsequencesCardiovascular2007,
  title = {Neuropsychiatric Consequences of Cardiovascular Medications},
  author = {Huffman, J.C. and Stern, T.A.},
  year = {2007},
  journal = {Dialogues in Clinical Neuroscience},
  volume = {9},
  number = {1},
  pages = {29--45}
}

@article{hungBehaviorPValueWhen1997,
  title = {The {{Behavior}} of the {{P-Value When}} the {{Alternative Hypothesis}} Is {{True}}},
  author = {Hung, H. M. James and O'Neill, Robert T. and Bauer, Peter and Kohne, Karl},
  year = {1997},
  journal = {Biometrics},
  volume = {53},
  number = {1},
  pages = {11--22},
  doi = {10.2307/2533093},
  abstract = {The P-value is a random variable derived from the distribution of the test statistic used to analyze a data set and to test a null hypothesis. Under the null hypothesis, the P-value based on a continuous test statistic has a uniform distribution over the interval [0, 1], regardless of the sample size of the experiment. In contrast, the distribution of the P-value under the alternative hypothesis is a function of both sample size and the true value or range of true values of the tested parameter. The characteristics, such as mean and percentiles, of the P-value distribution can give valuable insight into how the P-value behaves for a variety of parameter values and sample sizes. Potential applications of the P-value distribution under the alternative hypothesis to the design, analysis, and interpretation of results of clinical trials are considered.},
  file = {/Users/cristian/Zotero/storage/U9TJ8JFR/Hung et al. - 1997 - The Behavior of the P-Value When the Alternative H.pdf}
}

@article{impellizzeriRegisteredReportsComing2019,
  title = {Registered Reports Coming Soon: Our Contribution to Better Science in Football Research},
  author = {Impellizzeri, Franco M. and McCall, Alan and Meyer, Tim},
  year = {2019},
  journal = {Science and Medicine in Football},
  volume = {3},
  number = {2},
  pages = {87--88},
  doi = {10.1080/24733938.2019.1603659},
  keywords = {Bias,Good practice,Quality,Registration,Science},
  file = {/Users/cristian/Zotero/storage/F5SJWYCC/Impellizzeri et al. - 2019 - Registered reports coming soon our contribution t.pdf;/Users/cristian/Zotero/storage/7KQP3YNY/24733938.2019.html}
}

@book{internetarchiveSociologySocialResearch1928,
  title = {Sociology and {{Social Research}}  {{March-April}} 1928: {{Vol}} 12 {{Iss}} 4},
  shorttitle = {Sociology and {{Social Research}}  {{March-April}} 1928},
  year = {1928-03/1928-04},
  publisher = {University of Southern California},
  urldate = {2025-07-14},
  abstract = {Sociology and Social Research March-April 1928: Volume 12, Issue 4. Digitized from IA1627736-06. Previous issue: sim\_sociology-and-social-research\_january-february-1928\_12\_3. Next issue: sim\_sociology-and-social-research\_may-june-1928\_12\_5.},
  collaborator = {{Internet Archive}},
  langid = {english},
  keywords = {Sociology}
}

@article{inthoutPleaRoutinelyPresenting2016,
  ids = {inthoutPleaRoutinelyPresenting2016a},
  title = {Plea for Routinely Presenting Prediction Intervals in Meta-Analysis},
  author = {IntHout, Joanna and Ioannidis, John P. A. and Rovers, Maroeska M. and Goeman, Jelle J.},
  year = {2016},
  month = jul,
  journal = {BMJ Open},
  volume = {6},
  number = {7},
  pages = {e010247},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2015-010247},
  urldate = {2023-02-08},
  abstract = {Objectives Evaluating the variation in the strength of the effect across studies is a key feature of meta-analyses. This variability is reflected by measures like {$\tau$}2 or I2, but their clinical interpretation is not straightforward. A prediction interval is less complicated: it presents the expected range of true effects in similar studies. We aimed to show the advantages of having the prediction interval routinely reported in meta-analyses. Design We show how the prediction interval can help understand the uncertainty about whether an intervention works or not. To evaluate the implications of using this interval to interpret the results, we selected the first meta-analysis per intervention review of the Cochrane Database of Systematic Reviews Issues 2009--2013 with a dichotomous (n=2009) or continuous (n=1254) outcome, and generated 95\% prediction intervals for them. Results In 72.4\% of 479 statistically significant (random-effects p{$<$}0.05) meta-analyses in the Cochrane Database 2009--2013 with heterogeneity (I2{$>$}0), the 95\% prediction interval suggested that the intervention effect could be null or even be in the opposite direction. In 20.3\% of those 479 meta-analyses, the prediction interval showed that the effect could be completely opposite to the point estimate of the meta-analysis. We demonstrate also how the prediction interval can be used to calculate the probability that a new trial will show a negative effect and to improve the calculations of the power of a new trial. Conclusions The prediction interval reflects the variation in treatment effects over different settings, including what effect is to be expected in future patients, such as the patients that a clinician is interested to treat. Prediction intervals should be routinely reported to allow more informative inferences in meta-analyses.},
  chapter = {Research methods},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://www.bmj.com/company/products-services/rights-and-licensing/. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  pmid = {27406637},
  keywords = {Clinical trial,Cochrane Database of Systematic Reviews,Heterogeneity,Meta-analysis,Prediction interval,Random effects},
  file = {/Users/cristian/Zotero/storage/AW9ZW67X/IntHout et al. - 2016 - Plea for routinely presenting prediction intervals.pdf;/Users/cristian/Zotero/storage/R7GZCPEQ/IntHout et al. - 2016 - Plea for routinely presenting prediction intervals.pdf}
}

@misc{inzlichtBiasCorrectionTechniquesAlone2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {Bias-{{Correction Techniques Alone Cannot Determine Whether Ego Depletion}} Is {{Different}} from {{Zero}}: {{Commentary}} on {{Carter}}, {{Kofler}}, {{Forster}}, \& {{McCullough}}, 2015},
  shorttitle = {Bias-{{Correction Techniques Alone Cannot Determine Whether Ego Depletion}} Is {{Different}} from {{Zero}}},
  author = {Inzlicht, Michael and Gervais, Will and Berkman, Elliot},
  year = {2015},
  month = sep,
  number = {2659409},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.2659409},
  urldate = {2023-02-20},
  abstract = {Carter, Kofler, Forster, \& McCullough (2015) conducted a bias-corrected meta-analysis of the so-called ego depletion effect to determine its real size and robustness. Their efforts have raised awareness of how badly meta-analyses can mislead when the articles that go into them are products of publication bias. Despite our genuine enthusiasm for their work, we worry that in their zeal to correct the record of publication bias, they have drawn too heavily on largely untested statistical techniques that can be insensitive and sometimes misleading. We tested a set of bias-correction techniques, including those favored by Carter and colleagues, by simulating 40,000 meta-analyses in a range of situations that approximate what is found in the ego depletion literature, most notably the presence of heterogeneous effects filtered by publication bias. Our simulations revealed that not one of the bias-correction techniques revealed itself superior in all conditions, with corrections performing adequately in some situations but inadequately in others. Such a result implies that meta-analysts ought to present a range of possible effect sizes and to consider them all as being possible. The problem with the ego depletion literature is that the bias-corrected estimates for the overall effect do not converge, with estimates ranging from g=0 to g=0.24 to g=0.26. Despite our admiration for this program of meta-research, we suggest that bias-corrected meta-analyses cannot yet resolve whether the overall ego depletion is different from zero or not.},
  langid = {english},
  keywords = {ego depletion,meta-analysis,publication bias,self-control},
  file = {/Users/cristian/Zotero/storage/8QVMVGEJ/Inzlicht et al. - 2015 - Bias-Correction Techniques Alone Cannot Determine .pdf}
}

@article{ioannidisMetaresearchWhyResearch2018,
  title = {Meta-Research: {{Why}} Research on Research Matters},
  shorttitle = {Meta-Research},
  author = {Ioannidis, John P. A.},
  year = {2018},
  month = mar,
  journal = {PLoS biology},
  volume = {16},
  number = {3},
  pages = {e2005468},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2005468},
  abstract = {Meta-research is the study of research itself: its methods, reporting, reproducibility, evaluation, and incentives. Given that science is the key driver of human progress, improving the efficiency of scientific investigation and yielding more credible and more useful research results can translate to major benefits. The research enterprise grows very fast. Both new opportunities for knowledge and innovation and new threats to validity and scientific integrity emerge. Old biases abound, and new ones continuously appear as novel disciplines emerge with different standards and challenges. Meta-research uses an interdisciplinary approach to study, promote, and defend robust science. Major disruptions are likely to happen in the way we pursue scientific investigation, and it is important to ensure that these disruptions are evidence based.},
  langid = {english},
  pmcid = {PMC5865753},
  pmid = {29534060},
  keywords = {Interdisciplinary Research,Meta-Analysis as Topic,Reproducibility of Results,Research Design},
  file = {/Users/cristian/Zotero/storage/SW9DSFR3/Ioannidis - 2018 - Meta-research Why research on research matters.pdf}
}

@article{ioannidisWhatHaveWe2019,
  title = {What {{Have We}} ({{Not}}) {{Learnt}} from {{Millions}} of {{Scientific Papers}} with {{P Values}}?},
  author = {Ioannidis, John P. A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {1},
  pages = {20--25},
  doi = {10.1080/00031305.2018.1447512},
  abstract = {P values linked to null hypothesis significance testing (NHST) is the most widely (mis)used method of statistical inference. Empirical data suggest that across the biomedical literature (1990--2015), when abstracts use P values 96\% of them have P values of 0.05 or less. The same percentage (96\%) applies for full-text articles. Among 100 articles in PubMed, 55 report P values, while only 4 present confidence intervals for all the reported effect sizes, none use Bayesian methods and none use false-discovery rate. Over 25 years (1990--2015), use of P values in abstracts has doubled for all PubMed, and tripled for meta-analyses, while for some types of designs such as randomized trials the majority of abstracts report P values. There is major selective reporting for P values. Abstracts tend to highlight most favorable P values and inferences use even further spin to reach exaggerated, unreliable conclusions. The availability of large-scale data on P values from many papers has allowed the development and applications of methods that try to detect and model selection biases, for example, p-hacking, that cause patterns of excess significance. Inferences need to be cautious as they depend on the assumptions made by these models and can be affected by the presence of other biases (e.g., confounding in observational studies). While much of the unreliability of past and present research is driven by small, underpowered studies, NHST with P values may be also particularly problematic in the era of overpowered big data. NHST and P values are optimal only in a minority of current research. Using a more stringent threshold, as in the recently proposed shift from P {$<$} 0.05 to P {$<$} 0.005, is a temporizing measure to contain the flood and death-by-significance. NHST and P values may be replaced in many fields by other, more fit-for-purpose, inferential methods. However, curtailing selection biases requires additional measures, beyond changes in inferential methods, and in particular reproducible research practices.},
  keywords = {Bias,P-value,Statistical significance},
  annotation = {https://doi.org/10.1080/00031305.2018.1447512},
  file = {/Users/cristian/Zotero/storage/CSQZFWTQ/Ioannidis - 2019 - What Have We (Not) Learnt from Millions of Scienti.pdf;/Users/cristian/Zotero/storage/QIWZ84XU/00031305.2018.html}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/Users/cristian/Zotero/storage/RG8V7I6J/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf;/Users/cristian/Zotero/storage/DUMWIW7Z/article.html}
}

@article{isagerDecidingWhatReplicate2021,
  title = {Deciding What to Replicate: {{A}} Decision Model for Replication Study Selection under Resource and Knowledge Constraints},
  author = {Isager, Peder Mortvedt and {van Aert}, Robbie C. M. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Brandt, Mark J. and DeSoto, K. Andrew and {Giner-Sorolla}, Roger and Krueger, Joachim I. and Perugini, Marco and Ropovik, Ivan and {van 't Veer}, Anna E. and Vranka, Marek and Lakens, Dani{\"e}l},
  year = {2021},
  journal = {Psychological Methods},
  doi = {10.1037/met0000438},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, replicating researchers are constrained by resources, and will almost always have to choose one replication effort to focus on from a set of potential candidates. To select a candidate efficiently in these cases, we need methods for deciding which out of all candidates considered would be the most useful to replicate, given some overall goal researchers wish to achieve. In this article we assume that the overall goal researchers wish to achieve is to maximize the utility gained by conducting the replication study. We then propose a general rule for study selection in replication research based on the replication value of the set of claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (a) the value of being certain about the claim, and (b) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value. Our goal in this article is not to define concrete guidelines for study selection, but to provide the necessary theoretical foundations on which such concrete guidelines could be built. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Concepts,Decision Theory,Experimental Replication,Models,Simulation,Test Validity,Uncertainty},
  file = {/Users/cristian/Zotero/storage/3ZQGQD85/Isager et al. - 2021 - Deciding what to replicate A decision model for r.pdf;/Users/cristian/Zotero/storage/QE7YAHVJ/2022-14587-001.html}
}

@article{jacksonCardiovascularSafetySildenafil2006,
  title = {Cardiovascular Safety of Sildenafil Citrate ({{Viagra}}{\textregistered}): {{An}} Updated Perspective},
  author = {Jackson, G. and Montorsi, P. and Cheitlin, M.D.},
  year = {2006},
  journal = {Urology},
  volume = {68},
  number = {3 SUPPL.},
  pages = {47--60},
  doi = {10.1016/j.urology.2006.05.047}
}

@article{jakubowskiSupplementationLeucineMetabolite2020,
  title = {Supplementation with the Leucine Metabolite {$\beta$}-Hydroxy-{$\beta$}-Methylbutyrate ({{Hmb}}) Does Not Improve Resistance Exercise-Induced Changes in Body Composition or Strength in Young Subjects: {{A}} Systematic Review and Meta-Analysis},
  author = {Jakubowski, J.S. and Nunes, E.A. and Teixeira, F.J. and Vescio, V. and Morton, R.W. and Banfield, L. and Phillips, S.M.},
  year = {2020},
  journal = {Nutrients},
  volume = {12},
  number = {5},
  publisher = {MDPI AG},
  doi = {10.3390/nu12051523}
}

@misc{jane_guidelines_es,
  title = {Guide to {{Effect Sizes}} and {{Confidence Intervals}}},
  author = {Jan{\'e}, Matthew B and Xiao, Qinyu and Yeung, Siu Kit and Azevedo, Flavio and {Ben-Shachar}, Mattan S and Caldwell, Aaron R and Cousineau, Denis and Dunleavy, Daniel J and Elsherif, Mahmoud and Harlow, Tylor J and Johnson, Blair T and Moreau, David and Riesthuis, Paul and R{\"o}seler, Lukas and Steele, James and Vieira, Felipe F and Zloteanu, Mircea and Feldman, Gilad},
  year = {2024},
  publisher = {OSF},
  doi = {10.17605/OSF.IO/D8C4G},
  archiveprefix = {OSF},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/3CKC3YHH/Jan et al. - Guide to Effect Sizes and Confidence Intervals.pdf}
}

@article{janeGuideEffectSizes2023,
  title = {Guide to {{Effect Sizes}} and {{Confidence Intervals}}},
  author = {Jan{\'e}, Matthew B. and Xiao, Qinyu and Yeung, Siu Kit and {Ben-Shachar}, Mattan S. and Caldwell, Aaron R. and Cousineau, Denis and Dunleavy, Daniel J. and Elsherif, Mahmoud and Johnson, Blair T. and Moreau, David},
  year = {2023},
  month = jan,
  publisher = {OSF},
  doi = {10.17605/OSF.IO/D8C4G},
  urldate = {2024-06-05},
  abstract = {This effect sizes and confidence intervals collaborative guide aims to provide students and early-career researchers with hands-on, step-by-step instructions for calculating effect sizes and confidence intervals for common statistical tests used in psychology, social sciences and behavioral sciences, particularly when original data are not available and when reported information is incomplete. It also introduces general background information on effect sizes and confidence intervals, as well as useful R packages for their calculation. Many of the methods and procedures described in this Guide are based on R or R-based Shiny Apps developed by the science community. We were motivated to focus on R as we aim to maximize the reproducibility of our research outcomes and encourage the most reproducible study planning and data analysis workflow, though we also document other methods whenever possible for the reference of our readers. We regularly update this open educational resource, as packages are updated frequently and new packages are developed from time to time in this rapidly changing Open Scholarship era.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/KZSI3N8L/d8c4g.html}
}

@article{jarvisWhatGoesMust2021,
  title = {What Goes up Must Come down, Part {{II}}: {{Consequences}} of Jump Strategy Modification on Dance Leap Landing Biomechanics},
  shorttitle = {What Goes up Must Come down, Part {{II}}},
  author = {Jarvis, Danielle N. and Sigward, Susan M. and Lerch, Katie and Kulig, Kornelia},
  year = {2021},
  month = feb,
  journal = {Journal of Sports Sciences},
  volume = {39},
  number = {4},
  pages = {446--452},
  publisher = {Routledge},
  issn = {0264-0414},
  doi = {10.1080/02640414.2020.1825059},
  urldate = {2021-12-10},
  abstract = {Knee injuries are common in jumping athletes; modifying jump strategy may impact loads placed on the body and reduce injury risk. The purpose of this study was to determine if modifying strategy in a saut de chat leap to focus on height would decrease sagittal plane knee loading. Biomechanical data were collected while 28 dancers performed saut de chat leaps with instructions to jump far (FAR) or jump high (UP). In the UP condition, there was greater vertical GRF and less braking GRF. Also in UP, lower extremity contact angle was greater (71.3 {\textpm} 2.9 FAR; 75.8 {\textpm} 3.3 UP; p = 0.0178), peak knee extensor moment was greater (2.8 {\textpm} 0.7 Nm FAR; 3.2 {\textpm} 0.8 Nm UP; p = 0.01), and peak ankle plantar flexor moment was lower (3.19 {\textpm} 0.4 Nm FAR; 2.94 {\textpm} 0.4 Nm UP; p {$<$} 0.01). A more acute LECA was related to greater braking force and braking force was related to greater knee extensor moments. Despite these relationships, we observed greater knee extensor moments in UP. While the relationship among these whole-body variables and knee joint loading exists, it may not be the primary factor driving load distribution during dance leap landings.},
  pmid = {32966154},
  keywords = {Jumping,knee injuries,saut de chat},
  file = {/Users/cristian/Zotero/storage/N4JENLRN/02640414.2020.html}
}

@article{jarwalMeasuringResearchQuality2009,
  title = {Measuring Research Quality Using the Journal Impact Factor, Citations and `{{Ranked Journals}}': Blunt Instruments or Inspired Metrics?},
  shorttitle = {Measuring Research Quality Using the Journal Impact Factor, Citations and `{{Ranked Journals}}'},
  author = {Jarwal, Som D. and Brion, Andrew M. and King, Maxwell L.},
  year = {2009},
  month = oct,
  journal = {Journal of Higher Education Policy and Management},
  volume = {31},
  number = {4},
  pages = {289--300},
  publisher = {Routledge},
  issn = {1360-080X},
  doi = {10.1080/13600800903191930},
  urldate = {2021-02-07},
  abstract = {This paper examines whether three bibliometric indicators---the journal impact factor, citations per paper and the Excellence in Research for Australia (ERA) initiative's list of `ranked journals'---can predict the quality of individual research articles as assessed by international experts, both overall and within broad disciplinary groupings. The analysis is based on data obtained from a Mock Research Quality Framework (RQF) exercise conducted by Monash University during 2006--07 in which external assessors rated research articles for their quality using a five-point scale. Although a significant relationship exists between all three bibliometric variables and the overall Mock RQF assessor quality scores, only a relatively small amount of the variance (generally {$<$}20 per cent) could be explained. There is some evidence that the relationship is stronger within some disciplinary groupings than others. The findings suggest caution should be exercised when using these indicators as proxies for research quality.},
  keywords = {bibliometric indicators,citations,Excellence in Research for Australia (ERA),impact factor,journal ranks,peer review,quality scores,research excellence,research quality},
  file = {/Users/cristian/Zotero/storage/CTXSEBSL/13600800903191930.html}
}

@book{jeffreysTheoryProbability1961,
  title = {The {{Theory}} of {{Probability}}},
  author = {Jeffreys, Harold},
  year = {1961},
  publisher = {Oxford University Press},
  address = {Oxford},
  abstract = {Another title in the reissued Oxford Classic Texts in the Physical Sciences series, Jeffrey's Theory of Probability, first published in 1939, was the first to develop a fundamental theory of scientific inference based on the ideas of Bayesian statistics. His ideas were way ahead of their time and it is only in the past ten years that the subject of Bayes' factors has been significantly developed and extended. Until recently the two schools of statistics (Bayesian and Frequentist) were distinctly different and set apart. Recent work (aided by increased computer power and availability) has changed all that and today's graduate students and researchers all require an understanding of Bayesian ideas. This book is their starting point.},
  googlebooks = {vh9Act9rtzQC},
  isbn = {978-0-19-158967-6},
  langid = {english},
  keywords = {Mathematics / Applied,Mathematics / Discrete Mathematics,Mathematics / Probability \& Statistics / General,Science / Physics / General}
}

@article{jemniVibrationCyclingDid2019,
  title = {Vibration {{Cycling Did Not Affect Energy Demands Compared}} to {{Normal Cycling During Maximal Graded Test}}},
  author = {Jemni, Mon{\`e}m and Gu, Yaodong and Hu, Qiuli and Marina, Michel and Fessi, Mohamed Saifeddin and Moalla, Wassim and Mkaouer, Bessem and Konukman, Ferman},
  year = {2019},
  journal = {Frontiers in Physiology},
  volume = {10},
  pages = {1083},
  issn = {1664-042X},
  doi = {10.3389/fphys.2019.01083},
  abstract = {The aim of this study was to compare the physiological responses between a vibration induced cycling step protocol (Vib) and normal cycling (without vibration, no-Vib). Eighteen moderate trained males (age 24.1 {\textpm} 4.3 years; weight 76.5 {\textpm} 10.5 kg; height 178.0 {\textpm} 6.4 cm) have participated in this study. They randomly performed two gradual maximal exercise tests on two separate days using a new bike that automatically induces vibration cycling and the Corival cycle ergometer. The choice of two different bikes was made because of the impossibility to recreate the same power output without altering the cycling cadence on the vibration Bike. Both protocols were matched for power output and cycling cadence incrementations. Oxygen uptake (VO2), carbon dioxide production (VCO2), ventilation (VE), heart rate (HR), blood lactate and rating of perceived exertion (RPE) during each stage were continuously recorded. No statistical differences were founded for all variables when comparing the Vib to no-Vib trials, except a higher ventilation during the vibration trial at submaximal levels. The results of this study do not confirm those of previous studies stated that Vib increased metabolic demands during cycling exercise. Added vibration stimulus to an incremental cycling protocol does not affect physiological parameters.}
}

@article{jinClinicalPreclinicalSystematic2020,
  title = {Clinical and {{Preclinical Systematic Review}} of {{Panax}} Ginseng {{C}}. {{A}}. {{Mey}} and {{Its Compounds}} for {{Fatigue}}},
  author = {Jin, T.-Y. and Rong, P.-Q. and Liang, H.-Y. and Zhang, P.-P. and Zheng, G.-Q. and Lin, Y.},
  year = {2020},
  journal = {Frontiers in Pharmacology},
  volume = {11},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fphar.2020.01031}
}

@article{jodraEffectBeetrootJuice2020,
  title = {Effect of {{Beetroot Juice Supplementation}} on {{Mood}}, {{Perceived Exertion}}, and {{Performance During}} a 30-{{Second Wingate Test}}},
  author = {Jodra, Pablo and Dom{\'i}nguez, Ra{\'u}l and {S{\'a}nchez-Oliver}, Antonio J. and {Veiga-Herreros}, Pablo and Bailey, Stephen J.},
  year = {2020},
  journal = {International Journal of Sports Physiology and Performance},
  volume = {15},
  number = {2},
  pages = {243--248},
  publisher = {Human Kinetics},
  address = {Champaign IL, USA},
  doi = {10.1123/ijspp.2019-0149}
}

@article{johanssonHailImpossiblePvalues2011,
  title = {Hail the Impossible: P-Values, Evidence, and Likelihood},
  shorttitle = {Hail the Impossible},
  author = {Johansson, Tobias},
  year = {2011},
  month = apr,
  journal = {Scandinavian Journal of Psychology},
  volume = {52},
  number = {2},
  pages = {113--125},
  issn = {1467-9450},
  doi = {10.1111/j.1467-9450.2010.00852.x},
  abstract = {Significance testing based on p-values is standard in psychological research and teaching. Typically, research articles and textbooks present and use p as a measure of statistical evidence against the null hypothesis (the Fisherian interpretation), although using concepts and tools based on a completely different usage of p as a tool for controlling long-term decision errors (the Neyman-Pearson interpretation). There are four major problems with using p as a measure of evidence and these problems are often overlooked in the domain of psychology. First, p is uniformly distributed under the null hypothesis and can therefore never indicate evidence for the null. Second, p is conditioned solely on the null hypothesis and is therefore unsuited to quantify evidence, because evidence is always relative in the sense of being evidence for or against a hypothesis relative to another hypothesis. Third, p designates probability of obtaining evidence (given the null), rather than strength of evidence. Fourth, p depends on unobserved data and subjective intentions and therefore implies, given the evidential interpretation, that the evidential strength of observed data depends on things that did not happen and subjective intentions. In sum, using p in the Fisherian sense as a measure of statistical evidence is deeply problematic, both statistically and conceptually, while the Neyman-Pearson interpretation is not about evidence at all. In contrast, the likelihood ratio escapes the above problems and is recommended as a tool for psychologists to represent the statistical evidence conveyed by obtained data relative to two hypotheses.},
  langid = {english},
  pmid = {21077903},
  keywords = {{Models, Statistical},Probability}
}

@article{johnMeasuringPrevalenceQuestionable2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  langid = {english},
  keywords = {disclosure,judgment,methodology,professional standards}
}

@article{jolicoeurdesrochesEffectGlycerolInducedHyperhydration2023,
  ids = {jolicoeurdesrochesEffectGlycerolInducedHyperhydration2023a,jolicoeurdesrochesEffectGlycerolInducedHyperhydration2023b},
  title = {Effect of {{Glycerol-Induced Hyperhydration}} on a 5-Kilometer {{Running Time-Trial Performance}} in the {{Heat}} in {{Recreationally Active Individuals}}},
  author = {Jolicoeur Desroches, A. and Naulleau, C. and Deshayes, T.A. and {Parent-Roberge}, H. and Pancrate, T. and Goulet, E.D.B.},
  year = {2023},
  journal = {Nutrients},
  volume = {15},
  number = {3},
  publisher = {MDPI},
  doi = {10.3390/nu15030599},
  file = {/Users/cristian/Zotero/storage/3JP4HYA3/599.html;/Users/cristian/Zotero/storage/L7JN2Z2H/599.html}
}

@article{jonesBiomarkersAssociatedLower2022,
  title = {Biomarkers Associated with Lower Limb Muscle Function in Individuals with Sarcopenia: A Systematic Review},
  author = {Jones, R.L. and Paul, L. and Steultjens, M.P.M. and Smith, S.L.},
  year = {2022},
  journal = {Journal of Cachexia, Sarcopenia and Muscle},
  volume = {13},
  number = {6},
  pages = {2791--2806},
  publisher = {{John Wiley and Sons Inc}},
  doi = {10.1002/jcsm.13064}
}

@article{joyalPerspectiveCurrentStrategies2004,
  title = {A Perspective on the Current Strategies for the Treatment of Obesity},
  author = {Joyal, S.V.},
  year = {2004},
  journal = {Current Drug Targets: CNS and Neurological Disorders},
  volume = {3},
  number = {5},
  pages = {341--356},
  doi = {10.2174/1568007043336978}
}

@article{jurikRoleNutritionExercise2019,
  title = {Role of Nutrition and Exercise Programs in Reducing Blood Pressure: {{A}} Systematic Review},
  author = {Jurik, R. and Stastny, P.},
  year = {2019},
  journal = {Journal of Clinical Medicine},
  volume = {8},
  number = {9},
  publisher = {MDPI},
  doi = {10.3390/jcm8091393}
}

@article{juszczakReportingMultiArmParallelGroup2019,
  title = {Reporting of {{Multi-Arm Parallel-Group Randomized Trials}}: {{Extension}} of the {{CONSORT}} 2010 {{Statement}}},
  shorttitle = {Reporting of {{Multi-Arm Parallel-Group Randomized Trials}}},
  author = {Juszczak, Edmund and Altman, Douglas G. and Hopewell, Sally and Schulz, Kenneth},
  year = {2019},
  month = apr,
  journal = {JAMA},
  volume = {321},
  number = {16},
  pages = {1610--1620},
  issn = {0098-7484},
  doi = {10.1001/jama.2019.3087},
  urldate = {2024-08-26},
  abstract = {The quality of reporting of randomized clinical trials is suboptimal. In an era in which the need for greater research transparency is paramount, inadequate reporting hinders assessment of the reliability and validity of trial findings. The Consolidated Standards of Reporting Trials (CONSORT) 2010 Statement was developed to improve the reporting of randomized clinical trials, but the primary focus was on parallel-group trials with 2 groups. Multi-arm trials that use a parallel-group design (comparing treatments by concurrently randomizing participants to one of the treatment groups, usually with equal probability) but have 3 or more groups are relatively common. The quality of reporting of multi-arm trials varies substantially, making judgments and interpretation difficult. While the majority of the elements of the CONSORT 2010 Statement apply equally to multi-arm trials, some elements need adaptation, and, in some cases, additional issues need to be clarified.To present an extension to the CONSORT 2010 Statement for reporting multi-arm trials to facilitate the reporting of such trials.A guideline writing group, which included all authors, formed following the CONSORT group meeting in 2014. The authors met in person and by teleconference bimonthly between 2014 and 2018 to develop and revise the checklist and the accompanying text, with additional discussions by email. A draft manuscript was circulated to the wider CONSORT group of 36 individuals, plus 5 other selected individuals known for their specialist knowledge in clinical trials, for review. Extensive feedback was received from 14 individuals and, after detailed consideration of their comments, a final revised version of the extension was prepared.This CONSORT extension for multi-arm trials expands on 10 items of the CONSORT 2010 checklist and provides examples of good reporting and a rationale for the importance of each extension item. Key recommendations are that multi-arm trials should be identified as such and require clear objectives and hypotheses referring to all of the treatment groups. Primary treatment comparisons should be identified and authors should report the planned and unplanned comparisons resulting from multiple groups completely and transparently. If statistical adjustments for multiplicity are applied, the rationale and method used should be described.This extension of the CONSORT 2010 Statement provides specific guidance for the reporting of multi-arm parallel-group randomized clinical trials and should help provide greater transparency and accuracy in the reporting of such trials.},
  file = {/Users/cristian/Zotero/storage/XMCHLHKK/2731183.html}
}

@article{kadlecGreatPowerComes2023,
  title = {With {{Great Power Comes Great Responsibility}}: {{Common Errors}} in {{Meta-Analyses}} and {{Meta-Regressions}} in {{Strength}} \& {{Conditioning Research}}},
  shorttitle = {With {{Great Power Comes Great Responsibility}}},
  author = {Kadlec, Daniel and Sainani, Kristin L. and Nimphius, Sophia},
  year = {2023},
  month = feb,
  journal = {Sports Medicine},
  volume = {53},
  number = {2},
  pages = {313--325},
  issn = {1179-2035},
  doi = {10.1007/s40279-022-01766-0},
  urldate = {2023-02-06},
  abstract = {Meta-analysis and meta-regression are often highly cited and may influence practice. Unfortunately, statistical errors in meta-analyses are widespread and can lead to flawed conclusions. The purpose of this article was to review common statistical errors in meta-analyses and to document their frequency in highly cited meta-analyses from strength and conditioning research.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/VJIAPZRJ/Kadlec et al. - 2023 - With Great Power Comes Great Responsibility Commo.pdf}
}

@article{kahanEstimandsFrameworkPrimer2024,
  title = {The Estimands Framework: A Primer on the {{ICH E9}}({{R1}}) Addendum},
  shorttitle = {The Estimands Framework},
  author = {Kahan, Brennan C. and Hindley, Joanna and Edwards, Mark and Cro, Suzie and Morris, Tim P.},
  year = {2024},
  month = jan,
  journal = {BMJ},
  volume = {384},
  pages = {e076316},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-076316},
  urldate = {2025-03-20},
  abstract = {{$<$}p{$>$}Estimands can be used in studies of healthcare interventions to clarify the interpretation of treatment effects. The addendum to the ICH E9 harmonised guideline on statistical principles for clinical trials (ICH E9(R1)) describes a framework for using estimands as part of a study. This paper provides an overview of the estimands framework, as outlined in the addendum, with the aim of explaining why estimands are beneficial; clarifying the terminology being used; and providing practical guidance on using estimands to decide the appropriate study design, data collection, and estimation methods. This article illustrates how to use the estimands framework by applying it to an ongoing trial in emergency bowel surgery. Estimands can be a useful way of clarifying the exact research question being evaluated in a study, both to avoid misinterpretation and to ensure that study methods are aligned to the overall study objectives.{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {38262663}
}

@article{kaiserDirectionalStatisticalDecisions1960,
  title = {Directional Statistical Decisions},
  author = {Kaiser, Henry F.},
  year = {1960},
  journal = {Psychological Review},
  volume = {67},
  number = {3},
  pages = {160--167},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/h0047595},
  abstract = {Concerning the traditional nondirectional 2-sided test of significance, the author argues that "we cannot logically make a directional statistical decision or statement when the null hypothesis is rejected on the basis of the direction of the difference in the observed means." Thus, this test "should almost never be used." He proposes that "almost without exception the directional two-sided test should replace" it (18 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Significance},
  file = {/Users/cristian/Zotero/storage/9N5JGY3H/1961-01476-001.html}
}

@article{kaplanLikelihoodNullEffects2015a,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  year = {2015},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0132382},
  file = {/Users/cristian/Zotero/storage/9LS7JBZP/article.html}
}

@article{kardesRetractionsRehabilitationSport2020,
  title = {Retractions in {{Rehabilitation}} and {{Sport Sciences Journals}}: {{A Systematic Review}}},
  shorttitle = {Retractions in {{Rehabilitation}} and {{Sport Sciences Journals}}},
  author = {Karde{\c s}, Sinan and Levack, William and {\"O}zkuk, Ka{\u g}an and Atmaca Ayd{\i}n, Ebru and Seringe{\c c} Karabulut, Serap},
  year = {2020},
  month = nov,
  journal = {Archives of Physical Medicine and Rehabilitation},
  volume = {101},
  number = {11},
  pages = {1980--1990},
  issn = {0003-9993},
  doi = {10.1016/j.apmr.2020.03.010},
  urldate = {2021-02-07},
  abstract = {Objective To identify the characteristics of retracted publications in rehabilitation and sport sciences journals. Data Sources The Web of Science, PubMed, and Retraction Watch databases were searched from inception to August~2019. Study Selection Retracted publications published in rehabilitation or sport sciences journals, indexed in the Science Citation Index Expanded (SCIE) and Social Sciences Citation Index (SSCI) were included. Data Extraction One author extracted the data. Two other authors checked the data. Data Synthesis A total of 37 and 52 retracted publications and their retraction notices were identified for rehabilitation and sport sciences, respectively. The majority of retracted publications (68\% of all retracted papers in rehabilitation and 54\% of all retracted papers in sport sciences) were published in the past decade. Retracted publications in rehabilitation and sport sciences were published in 21 and 22 different journals and originated from 18 and 21 different countries, respectively. The full-text of the retracted publications was available with a retraction watermark or note for 59\% of cases in rehabilitation and 58\% in sport sciences. The reasons for the retractions were more often attributed to misconduct (79\% and 61\%) than to honest error (21\% and 39\%) in rehabilitation and sport sciences, respectively. However, a reason was not stated for 15\% of the publications. The median time interval between publication and retraction was 622 days in rehabilitation and 607 days in sport sciences publications. Conclusions The total number of retracted publications in rehabilitation and sport sciences journals was small. The retracted publications have been published in a variety of rehabilitation and sport sciences journals and came from different countries across the world. Several retracted publications and retraction notices failed to adhere to The Committee on Publication Ethics guidelines in the handling of full-text (retain with a watermark or note) or stating the underlying reasons for the retraction.},
  langid = {english},
  keywords = {Ethics,Physical and rehabilitation medicine,Plagiarism,Rehabilitation,Retracted publication,Retraction of publication,Scientific misconduct,Sports medicine},
  file = {/Users/cristian/Zotero/storage/FMRX7DQQ/S0003999320302082.html}
}

@article{karlasImpactControlledAttenuation2018,
  title = {Impact of Controlled Attenuation Parameter on Detecting Fibrosis Using Liver Stiffness Measurement},
  author = {Karlas, T. and Petroff, D. and Sasso, M. and Fan, J.-G. and Mi, Y.-Q. and {de L{\'e}dinghen}, V. and Kumar, M. and {Lupsor-Platon}, M. and Han, K.-H. and Cardoso, A.C. and Ferraioli, G. and Chan, W.-K. and Wong, V.W.-S. and Myers, R.P. and Chayama, K. and {Friedrich-Rust}, M. and Beaugrand, M. and Shen, F. and Hiriart, J.-B. and Sarin, S.K. and Badea, R. and Lee, H.W. and Marcellin, P. and Filice, C. and Mahadeva, S. and Wong, G.L.-H. and Crotty, P. and Masaki, K. and Bojunga, J. and Bedossa, P. and Keim, V. and Wiegand, J.},
  year = {2018},
  journal = {Alimentary Pharmacology and Therapeutics},
  volume = {47},
  number = {7},
  pages = {989--1000},
  publisher = {Blackwell Publishing Ltd},
  doi = {10.1111/apt.14529}
}

@article{kawadaREItalianBreakfast2021,
  ids = {kawadaREItalianBreakfast2021a},
  title = {{{RE}}: {{Italian}} Breakfast in Mind: {{The}} Effect of Caffeine, Carbohydrate and Protein on Physiological State, Mood and Cognitive Performance},
  author = {Kawada, T.},
  year = {2021},
  journal = {Physiology and Behavior},
  volume = {237},
  publisher = {Elsevier Inc.},
  doi = {10.1016/j.physbeh.2021.113452}
}

@article{keefeDefiningClinicallyMeaningful2013,
  title = {Defining a {{Clinically Meaningful Effect}} for the {{Design}} and {{Interpretation}} of {{Randomized Controlled Trials}}},
  author = {Keefe, Richard S. E. and Kraemer, Helena C. and Epstein, Robert S. and Frank, Ellen and Haynes, Ginger and Laughren, Thomas P. and Mcnulty, James and Reed, Shelby D. and Sanchez, Juan and Leon, Andrew C.},
  year = {2013},
  journal = {Innovations in Clinical Neuroscience},
  volume = {10},
  number = {5-6 Suppl A},
  pages = {4S-19S},
  issn = {2158-8333},
  urldate = {2024-07-24},
  abstract = {Objective: This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents., Design: Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials., Setting: The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them., Participants: The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD., Results: The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects., Conclusion: There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.},
  pmcid = {PMC3719483},
  pmid = {23882433},
  file = {/Users/cristian/Zotero/storage/TSGBWW6K/Keefe et al. - 2013 - Defining a Clinically Meaningful Effect for the De.pdf}
}

@article{kelleyEffectSize2012,
  title = {On Effect Size},
  author = {Kelley, Ken and Preacher, Kristopher J.},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  pages = {137--152},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/a0028086},
  abstract = {The call for researchers to report and interpret effect sizes and their corresponding confidence intervals has never been stronger. However, there is confusion in the literature on the definition of effect size, and consequently the term is used inconsistently. We propose a definition for effect size, discuss 3 facets of effect size (dimension, measure/index, and value), outline 10 corollaries that follow from our definition, and review ideal qualities of effect sizes. Our definition of effect size is general and subsumes many existing definitions of effect size. We define effect size as a quantitative reflection of the magnitude of some phenomenon that is used for the purpose of addressing a question of interest. Our definition of effect size is purposely more inclusive than the way many have defined and conceptualized effect size, and it is unique with regard to linking effect size to a question of interest. Additionally, we review some important developments in the effect size literature and discuss the importance of accompanying an effect size with an interval estimate that acknowledges the uncertainty with which the population value of the effect size has been estimated. We hope that this article will facilitate discussion and improve the practice of reporting and interpreting effect sizes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Effect Size (Statistical),Experimental Design},
  file = {/Users/cristian/Zotero/storage/PFTD5W64/2012-10789-001.html}
}

@article{kelleyMethodsBehavioralEducational2007,
  title = {Methods for the {{Behavioral}}, {{Educational}}, and {{Social Sciences}}: {{An R}} Package},
  shorttitle = {Methods for the {{Behavioral}}, {{Educational}}, and {{Social Sciences}}},
  author = {Kelley, Ken},
  year = {2007},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {4},
  pages = {979--984},
  issn = {1554-3528},
  doi = {10.3758/BF03192993},
  urldate = {2021-05-08},
  abstract = {Methods for the Behavioral, Educational, and Social Sciences (MBESS; Kelley, 2007b) is an open source package for R (R Development Core Team, 2007b), an open source statistical programming language and environment. MBESS implements methods that are not widely available elsewhere, yet are especially helpful for the idiosyncratic techniques used within the behavioral, educational, and social sciences. The major categories of functions are those that relate to confidence interval formation for noncentralt, F, and {$X$}2 parameters, confidence intervals for standardized effect sizes (which require noncentral distributions), and sample size planning issues from the power analytic and accuracy in parameter estimation perspectives. In addition, MBESS contains collections of other functions that should be helpful to substantive researchers and methodologists. MBESS is a long-term project that will continue to be updated and expanded so that important methods can continue to be made available to researchers in the behavioral, educational, and social sciences.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/6ZH7PKCS/Kelley - 2007 - Methods for the Behavioral, Educational, and Socia.pdf}
}

@article{kennyUnappreciatedHeterogeneityEffect2019,
  ids = {kennyUnappreciatedHeterogeneityEffect2019a},
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  year = {2019},
  month = oct,
  journal = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {578--589},
  issn = {1939-1463},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {30742474},
  keywords = {{Data Interpretation, Statistical},Humans,Meta-Analysis as Topic,Psychology,Reproducibility of Results,Research Design}
}

@article{kepesPublicationBiasOrganizational2012,
  title = {Publication {{Bias}} in the {{Organizational Sciences}}},
  author = {Kepes, Sven and Banks, George C. and McDaniel, Michael and Whetzel, Deborah L.},
  year = {2012},
  month = oct,
  journal = {Organizational Research Methods},
  volume = {15},
  number = {4},
  pages = {624--662},
  publisher = {SAGE Publications Inc},
  issn = {1094-4281},
  doi = {10.1177/1094428112452760},
  urldate = {2022-05-12},
  abstract = {Publication bias poses multiple threats to the accuracy of meta-analytically derived effect sizes and related statistics. Unfortunately, a review of the literature indicates that unlike meta-analytic reviews in medicine, research in the organizational sciences tends to pay little attention to this issue. In this article, the authors introduce advances in meta-analytic techniques from the medical and related sciences for a comprehensive assessment and evaluation of publication bias. The authors illustrate their use on a data set on employment interview validities. Using multiple methods, including contour-enhanced funnel plots, trim and fill, Egger's test of the intercept, Begg and Mazumdar's rank correlation, meta-regression, cumulative meta-analysis, and selection models, the authors find limited evidence of publication bias in the studied data.},
  langid = {english},
  keywords = {meta-analysis,missing data,publication bias,quantitative research},
  file = {/Users/cristian/Zotero/storage/M5DD6CJE/Kepes et al. - 2012 - Publication Bias in the Organizational Sciences.pdf}
}

@article{kepesQuestionableResearchPractices2022,
  title = {Questionable Research Practices among Researchers in the Most Research-Productive Management Programs},
  author = {Kepes, Sven and Keener, Sheila K. and McDaniel, Michael A. and Hartman, Nathan S.},
  year = {2022},
  journal = {Journal of Organizational Behavior},
  volume = {43},
  number = {7},
  issn = {1099-1379},
  doi = {10.1002/job.2623},
  urldate = {2022-05-02},
  abstract = {Questionable research practices (QRPs) among researchers have been a source of concern in many fields of study. QRPs are often used to enhance the probability of achieving statistical significance which affects the likelihood of a paper being published. Using a sample of researchers from 10 top research-productive management programs, we compared hypotheses tested in dissertations to those tested in journal articles derived from those dissertations to draw inferences concerning the extent of engagement in QRPs. Results indicated that QRPs related to changes in sample size and covariates were associated with unsupported dissertation hypotheses becoming supported in journal articles. Researchers also tended to exclude unsupported dissertation hypotheses from journal articles. Likewise, results suggested that many article hypotheses may have been created after the results were known (i.e., HARKed). Articles from prestigious journals contained a higher percentage of potentially HARKed hypotheses than those from less well-regarded journals. Finally, articles published in prestigious journals were associated with more QRP usage than less prestigious journals. QRPs increase in the percentage of supported hypotheses and result in effect sizes that likely overestimate population parameters. As such, results reported in articles published in our most prestigious journals may be less credible than previously believed.},
  langid = {english},
  keywords = {Chrysalis Effect,HARKing,questionable research practices,research integrity},
  file = {/Users/cristian/Zotero/storage/VCEIQ7V3/Kepes et al. - Questionable research practices among researchers .pdf;/Users/cristian/Zotero/storage/EG2R4ZEU/job.html}
}

@article{kerksickISSNExerciseSports2018,
  title = {{{ISSN}} Exercise \& Sports Nutrition Review Update: Research \& Recommendations},
  shorttitle = {{{ISSN}} Exercise \& Sports Nutrition Review Update},
  author = {Kerksick, Chad M. and Wilborn, Colin D. and Roberts, Michael D. and {Smith-Ryan}, Abbie and Kleiner, Susan M. and J{\"a}ger, Ralf and Collins, Rick and Cooke, Mathew and Davis, Jaci N. and Galvan, Elfego and Greenwood, Mike and Lowery, Lonnie M. and Wildman, Robert and Antonio, Jose and Kreider, Richard B.},
  year = {2018},
  month = aug,
  journal = {Journal of the International Society of Sports Nutrition},
  volume = {15},
  number = {1},
  pages = {38},
  issn = {1550-2783},
  doi = {10.1186/s12970-018-0242-y},
  urldate = {2022-11-11},
  abstract = {Sports nutrition is a constantly evolving field with hundreds of research papers published annually. In the year 2017 alone, 2082 articles were published under the key words `sport nutrition'. Consequently, staying current with the relevant literature is often difficult.},
  keywords = {Capacity,Dietary supplements,Double-blind,Efficacy,Ergogenic aids,Hypertrophy,Performance nutrition,Placebo-controlled,Position stand,Power,Randomized,Recommendations,Review,Sports nutrition,Strength,Weight gain},
  file = {/Users/cristian/Zotero/storage/23HLGIIY/Kerksick et al. - 2018 - ISSN exercise & sports nutrition review update re.pdf;/Users/cristian/Zotero/storage/Y2MRBXX9/s12970-018-0242-y.html}
}

@article{kerleyDietaryNitrateModulator2017,
  title = {Dietary Nitrate as Modulator of Physical Performance and Cardiovascular Health},
  author = {Kerley, C.P.},
  year = {2017},
  journal = {Current Opinion in Clinical Nutrition and Metabolic Care},
  volume = {20},
  number = {6},
  pages = {440--446},
  publisher = {{Lippincott Williams and Wilkins}},
  doi = {10.1097/MCO.0000000000000414}
}

@article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  file = {/Users/cristian/Zotero/storage/E6D55A8N/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf}
}

@article{khanCardioprotectiveRoleIschemic2014,
  title = {Cardioprotective Role of Ischemic Postconditioning in Acute Myocardial Infarction: {{A}} Systematic Review and Meta-Analysis},
  author = {Khan, A.R. and Binabdulhak, A.A. and Alastal, Y. and Khan, S. and {Faricy-Beredo}, B.M. and Luni, F.K. and Lee, W.M. and Khuder, S. and Tinkel, J.},
  year = {2014},
  journal = {American Heart Journal},
  volume = {168},
  number = {4},
  pages = {512-521.e4},
  publisher = {Mosby Inc.},
  doi = {10.1016/j.ahj.2014.06.021}
}

@article{khosravanPopulationPharmacokineticPharmacodynamic2016,
  title = {Population {{Pharmacokinetic}}/{{Pharmacodynamic Modeling}} of {{Sunitinib}} by {{Dosing Schedule}} in {{Patients}} with {{Advanced Renal Cell Carcinoma}} or {{Gastrointestinal Stromal Tumor}}},
  author = {Khosravan, R. and Motzer, R.J. and Fumagalli, E. and Rini, B.I.},
  year = {2016},
  journal = {Clinical Pharmacokinetics},
  volume = {55},
  number = {10},
  pages = {1251--1269},
  publisher = {Springer International Publishing},
  doi = {10.1007/s40262-016-0404-5}
}

@article{kimHeartBrainInterconnection2015,
  title = {Heart and Brain Interconnection - {{Clinical}} Implications of Changes in Brain Function during Heart Failure},
  author = {Kim, M.-S. and Kim, J.-J.},
  year = {2015},
  journal = {Circulation Journal},
  volume = {79},
  number = {5},
  pages = {942--947},
  publisher = {Japanese Circulation Society},
  doi = {10.1253/circj.CJ-15-0360}
}

@article{kirkPracticalSignificanceConcept1996,
  title = {Practical {{Significance}}: {{A Concept Whose Time Has Come}}},
  shorttitle = {Practical {{Significance}}},
  author = {Kirk, Roger E.},
  year = {1996},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {56},
  number = {5},
  pages = {746--759},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/0013164496056005002},
  urldate = {2021-03-10},
  abstract = {Statistical significance is concerned with whether a research result is due to chance or sampling variability; practical significance is concerned with whether the result is useful in the real world. A growing awareness of the limitations of null hypothesis significance tests has led to a search for ways to supplement these procedures. A variety of supplementary measures of effect magnitude have been proposed. The use of these procedures in four APA journals is examined, and an approach to assessing the practical significance of data is described.},
  langid = {english}
}

@article{kleinInvestigatingVariationReplicability2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = jan,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  doi = {htpps://doi.org/10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect -- imagined contact reducing prejudice -- showed weak support for replicability. And two effects -- flag priming influencing conservatism and currency priming influencing system justification -- did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  file = {/Users/cristian/Zotero/storage/Q8LD972K/Klein et al. - 2014 - Investigating Variation in Replicability.pdf;/Users/cristian/Zotero/storage/WP6Q28Z4/a000178.html}
}

@misc{kleinManyLabsFailure2019,
  title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
  shorttitle = {Many {{Labs}} 4},
  author = {Klein, Richard A. and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret and Cromar, Ryan and Vidamuerte, DeVere and Gardiner, Gwendolyn and Gosnell, Courtney and Grahe, Jon and Hall, Calvin and {Joy-Gaba}, Jennifer and Legg, Angela M. and Levitan, Carmel and Mancini, Anthony and Manfredi, Dylan and Miller, Jason Michael and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine and Storage, Daniel and Swanson, Trevor and van Swol, Lyn and Vaughn, Leigh Ann and Ratliff, Kate},
  year = {2019},
  month = dec,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/vef2c},
  urldate = {2021-02-16},
  abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 21 Labs and N = 2,220 participants) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not yet understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
  keywords = {many labs,Meta-science,metascience,mortality salience,psychology,replication,Social and Behavioral Sciences,Social and Personality Psychology,terror management theory},
  file = {/Users/cristian/Zotero/storage/LG643ZU9/Klein et al. - 2019 - Many Labs 4 Failure to Replicate Mortality Salien.pdf}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely highpowered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/CPP7GT3W/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{kleyCreatineTreatingMuscle2007,
  title = {Creatine for Treating Muscle Disorders},
  author = {Kley, R.A. and Vorgerd, M. and Tarnopolsky, M.A.},
  year = {2007},
  journal = {Cochrane Database of Systematic Reviews},
  number = {1},
  publisher = {{John Wiley and Sons Ltd}},
  doi = {10.1002/14651858.CD004760.pub2}
}

@article{knightNegativeResultsNull2003,
  title = {Negative Results: {{Null}} and Void},
  shorttitle = {Negative Results},
  author = {Knight, Jonathan},
  year = {2003},
  month = apr,
  journal = {Nature},
  volume = {422},
  number = {6932},
  pages = {554--555},
  issn = {0028-0836},
  doi = {10.1038/422554a},
  langid = {english},
  pmid = {12686968},
  keywords = {Periodicals as Topic,Publishing,Research},
  file = {/Users/cristian/Zotero/storage/VBRYYYMK/Knight - 2003 - Negative results Null and void.pdf}
}

@article{knudsonAuthorshipSamplingPractice2011,
  title = {Authorship and Sampling Practice in Selected Biomechanics and Sports Science Journals},
  author = {Knudson, Duane V.},
  year = {2011},
  month = jun,
  journal = {Perceptual and Motor Skills},
  volume = {112},
  number = {3},
  pages = {838--844},
  issn = {0031-5125},
  doi = {10.2466/17.PMS.112.3.838-844},
  abstract = {In some biomedical sciences, changes in patterns of collaboration and authorship have complicated the assignment of credit and responsibility for research. It is unclear if this problem of "promiscuous coauthorship" or "hyperauthorship" (defined as six or more authors) is also apparent in the applied research disciplines within sport and exercise science. This study documented the authorship and sampling of patterns of original research reports in three applied biomechanics (Clinical Biomechanics, Journal of Applied Biomechanics, and Sports Biomechanics) and five similar subdisciplinary journals within sport and exercise science (International Journal of Sports Physiology and Performance, Journal of Sport Rehabilitation, Journal of Teaching Physical Education, Measurement in Physical Education and Exercise Sciences, and Motor Control). Original research reports from the 2009 volumes of these biomechanics and sport and exercise journals were reviewed. Single authorship of papers was rare (2.6\%) in these journals, with the mean number of authors ranging from 2.7 to 4.5. Sample sizes and the ratio of sample to authors varied widely, and these variables tended not to be associated with number of authors. Original research reports published in these journals in 2009 tended to be published by small teams of collaborators, so currently there may be few problems with promiscuous coauthorship in these subdisciplines of sport and exercise science.},
  langid = {english},
  pmid = {21853773},
  keywords = {Authorship,Biomechanical Phenomena,Cooperative Behavior,Humans,Periodicals as Topic,Sample Size,Sports Medicine,United States}
}

@article{knudsonConfidenceCrisisResults2017,
  title = {Confidence Crisis of Results in Biomechanics Research},
  author = {Knudson, Duane V.},
  year = {2017},
  month = oct,
  journal = {Sports Biomechanics},
  volume = {16},
  number = {4},
  pages = {425--433},
  publisher = {Routledge},
  doi = {10.1080/14763141.2016.1246603},
  abstract = {Many biomechanics studies have small sample sizes and incorrect statistical analyses, so reporting of inaccurate inferences and inflated magnitude of effects are common in the field. This review examines these issues in biomechanics research and summarises potential solutions from research in other fields to increase the confidence in the experimental effects reported in biomechanics. Authors, reviewers and editors of biomechanics research reports are encouraged to improve sample sizes and the resulting statistical power, improve reporting transparency, improve the rigour of statistical analyses used, and increase the acceptance of replication studies to improve the validity of inferences from data in biomechanics research. The application of sports biomechanics research results would also improve if a larger percentage of unbiased effects and their uncertainty were reported in the literature.},
  pmid = {28632059},
  keywords = {Effect size,error,false positive,power,replicability},
  file = {/Users/cristian/Zotero/storage/BQXY27CR/14763141.2016.html}
}

@article{knudsonSatisticalReportingErros2005,
  title = {Satistical and {{Reporting Erros}} in {{Applied Biomechanics Research}}},
  author = {Knudson, Duane V.},
  year = {2005},
  journal = {ISBS - Conference Proceedings Archive},
  issn = {1999-4168},
  urldate = {2021-10-27},
  abstract = {Applied biomechanics research reports in the 2004 volumes of the ISBS Proceedings (n=94) and the Journal of Applied Biomechanics (n = 11) were analyzed for statistical errors. There was no significant difference in the distribution of ratings of quality based on statistical errors. The percentages of various statistical errors in reporting data were also quite similar. Both sources of applied biomechanics research had unacceptably large percentages of papers with errors in reporting statistical testing and results that limit readers' ability to interpret the findings. Improvements need to be made in the reporting and peer review of applied biomechanics papers in ISBS Proceedings in order to achieve the mission of ISBS.},
  copyright = {Copyright (c) 2015 ISBS - Conference Proceedings Archive},
  langid = {english},
  keywords = {effect size,reliability,sampling,type I error},
  file = {/Users/cristian/Zotero/storage/NRKJWUJW/Knudson - 2005 - STATISTICAL AND REPORTING ERRORS IN APPLIED BIOMEC.pdf}
}

@article{koEvidencebasedEvaluationPotential2014,
  title = {Evidence-Based Evaluation of Potential Benefits and Safety of Beta-Alanine Supplementation for Military Personnel},
  author = {Ko, R. and Low Dog, T. and Gorecki, D.K. and Cantilena, L.R. and Costello, R.B. and Evans, W.J. and Hardy, M.L. and Jordan, S.A. and Maughan, R.J. and Rankin, J.W. and {Smith-Ryan}, A.E. and Valerio, L.G. and Jones, D. and Deuster, P. and Giancaspro, G.I. and Sarma, N.D.},
  year = {2014},
  journal = {Nutrition Reviews},
  volume = {72},
  number = {3},
  pages = {217--225},
  publisher = {Blackwell Publishing Inc.},
  doi = {10.1111/nure.12087}
}

@article{konHormonalMetabolicResponses2015,
  title = {Hormonal and Metabolic Responses to Repeated Cycling Sprints under Different Hypoxic Conditions},
  author = {Kon, Michihiro and Nakagaki, Kohei and Ebi, Yoshiko and Nishiyama, Tetsunari and Russell, Aaron P.},
  year = {2015},
  month = jun,
  journal = {Growth Hormone \& IGF Research},
  volume = {25},
  number = {3},
  pages = {121--126},
  issn = {1096-6374},
  doi = {10.1016/j.ghir.2015.03.002},
  urldate = {2025-03-27},
  abstract = {Objective Sprint exercise and hypoxic stimulus during exercise are potent factors affecting hormonal and metabolic responses. However, the effects of different hypoxic levels on hormonal and metabolic responses during sprint exercise are not known. Here, we examined the effect of different hypoxic conditions on hormonal and metabolic responses during sprint exercise. Design Seven male subjects participated in three experimental trials: 1) sprint exercise under normoxia (NSE); 2) sprint exercise under moderate normobaric hypoxia (16.4\% oxygen) (HSE 16.4); and 3) sprint exercise under severe normobaric hypoxia (13.6\% oxygen) (HSE 13.6). The sprint exercise consisted of four 30s all-out cycling bouts with 4-min rest between bouts. Glucose, free fatty acids (FFA), blood lactate, growth hormone (GH), epinephrine (E), norepinephrine (NE), and insulin concentrations in the HSE trials were measured before exposure to hypoxia (pre 1), 15min after exposure to hypoxia (pre 2), and at 0, 15, 30, 60, 120, and 180min after the exercise performed in hypoxia. The blood samples in the NSE trial were obtained in normoxia at the same time points as the HSE trials. Results Circulating levels of glucose, FFA, lactate, GH, E, NE, and insulin significantly increased after all three exercise trials (P{$<$}0.05). The area under the curve (AUC) for GH was significantly higher in the HSE 13.6 trial than in the NSE and HSE 16.4 trials (P{$<$}0.05). A maximal increase in FFA concentration was observed at 180min after exercise and was not different between trials. Conclusion These findings suggest that severe hypoxia may be an important factor for the enhancement of GH response to all-out sprint exercise.},
  keywords = {Growth hormone,Lipolysis,Supramaximal exercise,Systemic hypoxia},
  file = {/Users/cristian/Zotero/storage/4JYN8E79/S1096637415000234.html}
}

@article{kovacsSampleSizePlannerToolEstimate2022,
  title = {{{SampleSizePlanner}}: {{A Tool}} to {{Estimate}} and {{Justify Sample Size}} for {{Two-Group Studies}}},
  shorttitle = {{{SampleSizePlanner}}},
  author = {Kovacs, Marton and {van Ravenzwaaij}, Don and Hoekstra, Rink and Aczel, Balazs},
  year = {2022},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {1},
  pages = {25152459211054059},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459211054059},
  urldate = {2023-07-13},
  abstract = {Planning sample size often requires researchers to identify a statistical technique and to make several choices during their calculations. Currently, there is a lack of clear guidelines for researchers to find and use the applicable procedure. In the present tutorial, we introduce a web app and R package that offer nine different procedures to determine and justify the sample size for independent two-group study designs. The application highlights the most important decision points for each procedure and suggests example justifications for them. The resulting sample-size report can serve as a template for preregistrations and manuscripts.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/XGTF8TGF/Kovacs et al. - 2022 - SampleSizePlanner A Tool to Estimate and Justify .pdf}
}

@article{kraftInterpretingEffectSizes2020,
  title = {Interpreting {{Effect Sizes}} of {{Education Interventions}}},
  author = {Kraft, Matthew A.},
  year = {2020},
  month = may,
  journal = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X20912798},
  urldate = {2023-10-18},
  abstract = {Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen's standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.},
  langid = {english}
}

@article{kramerStrongFieldGravityTests2021,
  title = {Strong-{{Field Gravity Tests}} with the {{Double Pulsar}}},
  author = {Kramer, M. and Stairs, I. H. and Manchester, R. N. and Wex, N. and Deller, A. T. and Coles, W. A. and Ali, M. and Burgay, M. and Camilo, F. and Cognard, I. and Damour, T. and Desvignes, G. and Ferdman, R. D. and Freire, P. C. C. and Grondin, S. and Guillemot, L. and Hobbs, G. B. and Janssen, G. and Karuppusamy, R. and Lorimer, D. R. and Lyne, A. G. and McKee, J. W. and McLaughlin, M. and M{\"u}nch, L. E. and Perera, B. B. P. and Pol, N. and Possenti, A. and Sarkissian, J. and Stappers, B. W. and Theureau, G.},
  year = {2021},
  month = dec,
  journal = {Physical Review X},
  volume = {11},
  number = {4},
  pages = {041050},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.11.041050},
  urldate = {2024-01-21},
  abstract = {Continued timing observations of the double pulsar PSR J0737--3039A/B, which consists of two active radio pulsars (A and B) that orbit each other with a period of 2.45 h in a mildly eccentric (e=0.088) binary system, have led to large improvements in the measurement of relativistic effects in this system. With a 16-yr data span, the results enable precision tests of theories of gravity for strongly self-gravitating bodies and also reveal new relativistic effects that have been expected but are now observed for the first time. These include effects of light propagation in strong gravitational fields which are currently not testable by any other method. In particular, we observe the effects of retardation and aberrational light bending that allow determination of the spin direction of the pulsar. In total, we detect seven post-Keplerian parameters in this system, more than for any other known binary pulsar. For some of these effects, the measurement precision is now so high that for the first time we have to take higher-order contributions into account. These include the contribution of the A pulsar's effective mass loss (due to spin-down) to the observed orbital period decay, a relativistic deformation of the orbit, and the effects of the equation of state of superdense matter on the observed post-Keplerian parameters via relativistic spin-orbit coupling. We discuss the implications of our findings, including those for the moment of inertia of neutron stars, and present the currently most precise test of general relativity's quadrupolar description of gravitational waves, validating the prediction of general relativity at a level of 1.3{\texttimes}10-4 with 95\% confidence. We demonstrate the utility of the double pulsar for tests of alternative theories of gravity by focusing on two specific examples and also discuss some implications of the observations for studies of the interstellar medium and models for the formation of the double pulsar system. Finally, we provide context to other types of related experiments and prospects for the future.},
  file = {/Users/cristian/Zotero/storage/4TYKC8TW/Kramer et al. - 2021 - Strong-Field Gravity Tests with the Double Pulsar.pdf;/Users/cristian/Zotero/storage/HFBGDMTZ/PhysRevX.11.html}
}

@article{krantzNullHypothesisTesting1999,
  title = {The {{Null Hypothesis Testing Controversy}} in {{Psychology}}},
  author = {Krantz, David H.},
  year = {1999},
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {448},
  eprint = {2669949},
  eprinttype = {jstor},
  pages = {1372--1381},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2669949},
  urldate = {2023-07-10},
  abstract = {A controversy concerning the usefulness of "null" hypothesis tests in scientific inference has continued in articles within psychology since 1960 and has recently come to a head, with serious proposals offered for a test ban or something close to it. This article sketches some of the views of statistical theory and practice among different groups of psychologists, reviews a recent book offering multiple perspectives on null hypothesis tests, and argues that the debate within psychology is a symptom of serious incompleteness in the foundations of statistics.},
  file = {/Users/cristian/Zotero/storage/YP9PUFU8/Krantz - 1999 - The Null Hypothesis Testing Controversy in Psychol.pdf}
}

@article{kruschkeBayesianAnalysisReporting2021,
  title = {Bayesian {{Analysis Reporting Guidelines}}},
  author = {Kruschke, John K.},
  year = {2021},
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {10},
  pages = {1282--1291},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01177-7},
  urldate = {2023-07-17},
  abstract = {Previous surveys of the literature have shown that reports of statistical analyses often lack important information, causing lack of transparency and failure of reproducibility. Editors and authors agree that guidelines for reporting should be encouraged. This Review presents a set of Bayesian analysis reporting guidelines (BARG). The BARG encompass the features of previous guidelines, while including many additional details for contemporary Bayesian analyses, with explanations. An extensive example of applying the BARG is presented. The BARG should be useful to researchers, authors, reviewers, editors, educators and students. Utilization, endorsement and promotion of the BARG may improve the quality, transparency and reproducibility of Bayesian analyses.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Medical research,Psychology},
  file = {/Users/cristian/Zotero/storage/774UMJXP/Kruschke - 2021 - Bayesian Analysis Reporting Guidelines.pdf}
}

@article{kudoEffectRamucirumabALBI2021,
  title = {Effect of Ramucirumab on {{ALBI}} Grade in Patients with Advanced {{HCC}}: {{Results}} from {{REACH}} and {{REACH-2}}.},
  author = {Kudo, Masatoshi and Galle, Peter R. and Brandi, Giovanni and Kang, Yoon-Koo and Yen, Chia-Jui and Finn, Richard S. and Llovet, Josep M. and Assenat, Eric and Merle, Philippe and Chan, Stephen L. and Palmer, Daniel H. and Ikeda, Masafumi and Yamashita, Tatsuya and Vogel, Arndt and Huang, Yi-Hsiang and Abada, Paolo B. and Yoshikawa, Reigetsu and Shinozaki, Kenta and Wang, Chunxiao and Widau, Ryan C. and Zhu, Andrew X.},
  year = {2021},
  month = apr,
  journal = {JHEP reports : innovation in hepatology},
  volume = {3},
  number = {2},
  pages = {100215},
  address = {Netherlands},
  issn = {2589-5559},
  doi = {10.1016/j.jhepr.2020.100215},
  abstract = {BACKGROUND \& AIMS: The albumin-bilirubin (ALBI) grade/score is derived from a validated nomogram to objectively assess prognosis and liver function in patients  with hepatocellular carcinoma (HCC). In this post hoc analysis, we assessed  prognosis in terms of survival by baseline ALBI grade and monitored liver  function during treatment with ramucirumab or placebo using the ALBI score in  patients with advanced HCC. METHODS: Patients with advanced HCC, Child-Pugh class  A with prior sorafenib treatment were randomised in REACH trials to receive  ramucirumab 8 mg/kg or placebo every 2 weeks. Data were analysed by trial and as  a meta-analysis of individual patient-level data (pooled population) from REACH  (alpha-fetoprotein {$\geq$}400 ng/ml) and REACH-2. Patients from REACH with Child-Pugh  class B were analysed as a separate cohort. The ALBI grades and scores were  calculated at baseline and before each treatment cycle. RESULTS: Baseline  characteristics by ALBI grade were balanced between treatment arms among patients  in the pooled population (ALBI-1, n~= 231; ALBI-2, n~= 296; ALBI-3, n~= 7).  Baseline ALBI grade was prognostic for overall survival (OS; ALBI grade 2 vs. 1;  hazard ratio [HR]: 1.38 [1.13-1.69]), after adjusting for other significant  prognostic factors. Mean ALBI scores remained stable in both treatment arms  compared with baseline and were unaffected by baseline ALBI grade, macrovascular  invasion, tumour response, geographical region, or prior locoregional therapy.  Baseline ALBI grades 2 and 3 were associated with increased incidence of  liver-specific adverse events and discontinuation rates in both treatments.  Ramucirumab improved OS in patients with baseline ALBI grade 1 (HR 0.605  [0.445-0.824]) and ALBI grade 2 (HR 0.814 [0.630-1.051]). CONCLUSIONS: Compared  with placebo, ramucirumab did not negatively impact liver function and improved  survival irrespective of baseline ALBI grade. LAY SUMMARY: Hepatocellular  carcinoma is the third leading cause of cancer-related death worldwide. Prognosis  is affected by many clinical factors including liver function both before and  during anticancer treatment. Here we have used a validated approach to assess  liver function using 2 laboratory parameters, serum albumin and bilirubin (ALBI),  both before and during treatment with ramucirumab in 2 phase III  placebo-controlled studies. We confirm the practicality of using this more  simplistic approach in assessing liver function prior to and during anticancer  therapy, and demonstrate ramucirumab did not impair liver function when compared  with placebo.},
  copyright = {{\copyright} 2020 The Authors.},
  langid = {english},
  pmcid = {PMC7772786},
  pmid = {33392490},
  keywords = {{AE, adverse event},{AESI, adverse event of special interest},{AFP, alpha-fetoprotein},{ALBI, albumin--bilirubin},{ALT, alanine aminotransferase},{AST, aspartate aminotransferase},{BCLC, Barcelona Clinic Liver Cancer},{BOR, best overall response},{BSC, best supportive care},{CP, Child-Pugh},{CR, complete response},{ECOG PS, Eastern Cooperative Oncology Group performance status},{EoT, end of treatment},{GGT, gamma-glutamyltransferase},{HCC, hepatocellular carcinoma},{HR, hazard ratio},{IQR, inter-quartile range},{ITT, intent-to-treat},{MVI, macrovascular invasion},{OS, overall survival},{PD, progressive disease},{PR, partial response},{Ram, ramucirumab},{SD, stable disease},{TACE, transarterial chemoembolisation},{VEGF, vascular endothelial growth factor},{VEGFRs, vascular endothelial growth factor receptors},ALBI,Liver function,Prognosis,Safety,Survival,Tumour response}
}

@article{kuhbergerPublicationBiasPsychology2014,
  title = {Publication {{Bias}} in {{Psychology}}: {{A Diagnosis Based}} on the {{Correlation}} between {{Effect Size}} and {{Sample Size}}},
  shorttitle = {Publication {{Bias}} in {{Psychology}}},
  author = {K{\"u}hberger, Anton and Fritz, Astrid and Scherndl, Thomas},
  year = {2014},
  month = sep,
  journal = {PLOS ONE},
  volume = {9},
  number = {9},
  pages = {e105825},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0105825},
  urldate = {2022-10-10},
  abstract = {Background The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias. Methods We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values. Results We found a negative correlation of r = -.45 [95\% CI: -.53; -.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings. Conclusion The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology.},
  langid = {english},
  keywords = {Clinical psychology,Decision trees,Psychology,Publication ethics,Scientific publishing,Social psychology,Statistical data,Test statistics},
  file = {/Users/cristian/Zotero/storage/6P4SNBIQ/Khberger et al. - 2014 - Publication Bias in Psychology A Diagnosis Based .pdf;/Users/cristian/Zotero/storage/XQ4I545U/article.html}
}

@article{kundaCaseMotivatedReasoning1990,
  title = {The Case for Motivated Reasoning},
  author = {Kunda, Z.},
  year = {1990},
  month = nov,
  journal = {Psychological Bulletin},
  volume = {108},
  number = {3},
  pages = {480--498},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.108.3.480},
  abstract = {It is proposed that motivation may affect reasoning through reliance on a biased set of cognitive processes--that is, strategies for accessing, constructing, and evaluating beliefs. The motivation to be accurate enhances use of those beliefs and strategies that are considered most appropriate, whereas the motivation to arrive at particular conclusions enhances use of those that are considered most likely to yield the desired conclusion. There is considerable evidence that people are more likely to arrive at conclusions that they want to arrive at, but their ability to do so is constrained by their ability to construct seemingly reasonable justifications for these conclusions. These ideas can account for a wide variety of research concerned with motivated reasoning.},
  langid = {english},
  pmid = {2270237},
  keywords = {Attitude,Concept Formation,Humans,Motivation,Problem Solving,Social Perception}
}

@article{kurmisUnderstandingLimitationsJournal2004,
  title = {Understanding the Limitations of the Journal Impact Factor. {{J Bone Joint Surg Am}} 85-{{A}}(12):2449-2454},
  shorttitle = {Understanding the Limitations of the Journal Impact Factor. {{J Bone Joint Surg Am}} 85-{{A}}(12)},
  author = {Kurmis, Andrew},
  year = {2004},
  month = jan,
  journal = {The Journal of bone and joint surgery. American volume},
  volume = {85-A},
  pages = {2449--54},
  abstract = {The impact factor, a simple mathematical formula reflecting the number of citations of a journal's material divided by the number of citable materials published by that same journal, has evolved to become one of the most influential tools in modern research and academia. The impact factor can be influenced and biased (intentionally or otherwise) by many factors. Extension of the impact factor to the assessment of journal quality or individual authors is inappropriate. Extension of the impact factor to cross-discipline journal comparison is also inappropriate. Those who choose to use the impact factor as a comparative tool should be aware of the nature and premise of its derivation and also of its inherent flaws and practical limitations.}
}

@article{kvarven_2020,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Str{\o}mland, Eirik and Johannesson, Magnus},
  year = {2020},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {4},
  pages = {423--434},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15\,meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15\,meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
  langid = {english},
  pmid = {31873200},
  keywords = {{Data Interpretation, Statistical},Behavioral Research,Bias,Humans,Meta-Analysis as Topic,Reproducibility of Results,Statistics as Topic}
}

@incollection{lakatosFalsificationMethodologyScientific1970,
  title = {Falsification and the {{Methodology}} of {{Scientific Research Programmes}}},
  booktitle = {Criticism and the {{Growth}} of {{Knowledge}}: {{Proceedings}} of the {{International Colloquium}} in the {{Philosophy}} of {{Science}}, {{London}}, 1965},
  author = {Lakatos, I.},
  editor = {Musgrave, Alan and Lakatos, Imre},
  year = {1970},
  volume = {4},
  pages = {91--196},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139171434.009},
  urldate = {2022-09-08},
  isbn = {978-0-521-09623-2},
  file = {/Users/cristian/Zotero/storage/VMNZDE8H/B1AAD974814D6E7BF35E6449691AA58F.html}
}

@article{lakatosSciencePseudoscience1978,
  title = {Science and Pseudoscience},
  author = {Lakatos, Imre},
  year = {1978},
  journal = {Philosophical papers},
  volume = {1},
  pages = {1--7},
  file = {/Users/cristian/Zotero/storage/3SQAHHZ3/Lakatos - 1978 - Science and pseudoscience.pdf}
}

@article{lakens_calculating_es,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  author = {Lakens, Daniel},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  pages = {863},
  doi = {10.3389/fpsyg.2013.00863},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning},
  file = {/Users/cristian/Zotero/storage/XYTWS9B9/Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english},
  keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods},
  file = {/Users/cristian/Zotero/storage/PZC95BMI/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@article{lakens_equivalence_2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  urldate = {2022-04-12},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  keywords = {equivalence testing,falsification,frequentist,null hypothesis,null-hypothesis significance test,open materials,power,TOST},
  file = {/Users/cristian/Zotero/storage/9KEMZ76Y/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@article{lakens_followup_bias,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta^2$}, {$\omega^2$} and {$\varepsilon^2$}) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta^2$} in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Sample Size,Statistical Analysis,Statistical Data,Statistical Estimation},
  file = {/Users/cristian/Zotero/storage/9T9W5BWQ/Albers and Lakens - 2018 - When power analyses based on pilot data are biased.pdf;/Users/cristian/Zotero/storage/SJGVTWPN/Albers and Lakens - 2018 - When power analyses based on pilot data are biased.pdf;/Users/cristian/Zotero/storage/BMVBTDZB/2017-53838-020.html;/Users/cristian/Zotero/storage/SPK9IJ8I/2017-53838-020.html}
}

@article{lakens_justification_2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  editor = {van Ravenzwaaij, Don},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2025-06-10},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/cristian/Zotero/storage/JBVP3ZHF/Lakens - 2022 - Sample Size Justification.pdf}
}

@article{lakens_sample_justification,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2022-10-17},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/cristian/Zotero/storage/ICZW3DHQ/Lakens - 2022 - Sample Size Justification.pdf;/Users/cristian/Zotero/storage/WHLG9TBD/Sample-Size-Justification.html}
}

@article{lakens_sequential_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2023},
  urldate = {2025-07-11},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/WQK9XWCF/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf}
}

@article{lakens_value_preregistration,
  ids = {lakensValuePreregistrationPsychological2019c},
  title = {The Value of Preregistration for Psychological Science: {{A}} Conceptual Analysis},
  shorttitle = {The Value of Preregistration for Psychological Science},
  author = {Lakens, Daniel},
  year = {2019},
  journal = {},
  volume = {62},
  number = {3},
  pages = {221--230},
  publisher = {},
  file = {/Users/cristian/Zotero/storage/K3I7ZFVR/Lakens - 2019 - The value of preregistration for psychological sci.pdf;/Users/cristian/Zotero/storage/IP4HASAJ/ja.html}
}

@article{lakens_when_2024,
  title = {When and How to Deviate from a Preregistration},
  author = {Lakens, Dani{\"e}l},
  year = {2024},
  month = may,
  journal = {Collabra: Psychology},
  volume = {10},
  number = {1},
  pages = {117094},
  issn = {2474-7394},
  doi = {10.1525/collabra.117094}
}

@article{lakensBenefitsPreregistrationRegistered2024,
  title = {The Benefits of Preregistration and {{Registered Reports}}},
  author = {Lakens, Dani{\"e}l and Mesquida, Cristian and Rasti, Sajedeh and Ditroilo, Massimiliano},
  year = {2024},
  month = dec,
  journal = {Evidence-Based Toxicology},
  volume = {2},
  number = {1},
  pages = {2376046},
  issn = {2833-373X},
  doi = {10.1080/2833373X.2024.2376046},
  urldate = {2025-02-17},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/IQDT6CXN/Lakens et al. - 2024 - The benefits of preregistration and Registered Rep.pdf}
}

@article{lakensChallengesDrawingConclusions2015,
  title = {On the Challenges of Drawing Conclusions from P-Values Just below 0.05},
  author = {Lakens, Dani{\"e}l},
  year = {2015},
  month = jul,
  journal = {PeerJ},
  volume = {3},
  issn = {2167-8359},
  doi = {10.7717/peerj.1142},
  urldate = {2021-02-07},
  abstract = {In recent years, researchers have attempted to provide an indication of the prevalence of inflated Type 1 error rates by analyzing the distribution of p-values in the published literature.  analyzed the distribution (and its change over time) of a large number of p-values automatically extracted from abstracts in the scientific literature. They concluded there is a `surge of p-values between 0.041--0.049 in recent decades' which `suggests (but does not prove) questionable research practices have increased over the past 25 years.' I show the changes in the ratio of fractions of p-values between 0.041--0.049 over the years are better explained by assuming the average power has decreased over time. Furthermore, I propose that their observation that p-values just below 0.05 increase more strongly than p-values above 0.05 can be explained by an increase in publication bias (or the file drawer effect) over the years (cf. ; , which has led to a relative decrease of `marginally significant' p-values in abstracts in the literature (instead of an increase in p-values just below 0.05). I explain why researchers analyzing large numbers of p-values need to relate their assumptions to a model of p-value distributions that takes into account the average power of the performed studies, the ratio of true positives to false positives in the literature, the effects of publication bias, and the Type 1 error rate (and possible mechanisms through which it has inflated). Finally, I discuss why publication bias and underpowered studies might be a bigger problem for science than inflated Type 1 error rates, and explain the challenges when attempting to draw conclusions about inflated Type 1 error rates from a large heterogeneous set of p-values.},
  pmcid = {PMC4525697},
  pmid = {26246976},
  file = {/Users/cristian/Zotero/storage/8LY3AI9R/Lakens - 2015 - On the challenges of drawing conclusions from p-va.pdf}
}

@article{lakensExaminingReproducibilityMetaanalyses2017,
  title = {Examining the Reproducibility of Meta-Analyses in Psychology: {{A}} Preliminary Report},
  shorttitle = {Examining the Reproducibility of Meta-Analyses in Psychology},
  author = {Lakens, Daniel and {Page-Gould}, Elizabeth and van Assen, Marcel A. L. M. and Spellman, Bobbie and Sch{\"o}nbrodt, Felix D. and Hasselman, Fred and Corker, Katherine S. and Grange, James A. and Sharples, Amanda and Cavender, Corinne and Augusteijn, Hilde Elisabeth Maria and Gerger, Heike and Locher, Cosima and Miller, Ian Dennis and Anvari, Farid and Scheel, Anne M.},
  year = {2017},
  journal = {Examining the reproducibility of meta-analyses in psychology},
  publisher = {MetaArXiv Preprints},
  doi = {10.31222/osf.io/xfbjf},
  urldate = {2025-05-18},
  abstract = {Meta-analyses are an important tool to evaluate the literature. It is essential that meta-analyses can easily be reproduced to allow researchers to evaluate the impact of subjective choices on meta-analytic effect sizes, but also to update meta-analyses as new data comes in, or as novel statistical techniques (for example to correct for publication bias) are developed. Research in medicine has revealed meta-analyses often cannot be reproduced. In this project, we examined the reproducibility of meta-analyses in psychology by reproducing twenty published meta-analyses. Reproducing published meta-analyses was surprisingly difficult. 96\% of meta-analyses published in 2013-2014 did not adhere to reporting guidelines. A third of these meta-analyses did not contain a table specifying all individual effect sizes. Five of the 20 randomly selected meta-analyses we attempted to reproduce could not be reproduced at all due to lack of access to raw data, no details about the effect sizes extracted from each study, or a lack of information about how effect sizes were coded. In the remaining meta-analyses, differences between the reported and reproduced effect size or sample size were common. We discuss a range of possible improvements, such as more clearly indicating which data were used to calculate an effect size, specifying all individual effect sizes, adding detailed information about equations that are used, and how multiple effect size estimates from the same study are combined, but also sharing raw data retrieved from original authors, or unpublished research reports. This project clearly illustrates there is a lot of room for improvement when it comes to the transparency and reproducibility of published meta-analyses.},
  file = {/Users/cristian/Zotero/storage/NRCQG8ZY/Lakens et al. - 2017 - Examining the reproducibility of meta-analyses in .pdf}
}

@article{lakensImprovingInferencesNull2020,
  title = {Improving {{Inferences About Null Effects With Bayes Factors}} and {{Equivalence Tests}}},
  author = {Lakens, Dani{\"e}l and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
  year = {2020},
  month = jan,
  journal = {The Journals of Gerontology: Series B},
  volume = {75},
  number = {1},
  pages = {45--57},
  issn = {1079-5014},
  doi = {10.1093/geronb/gby065},
  urldate = {2023-04-28},
  abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.},
  file = {/Users/cristian/Zotero/storage/5YAZUMJN/Lakens et al. - 2020 - Improving Inferences About Null Effects With Bayes.pdf;/Users/cristian/Zotero/storage/SZ8GW2T4/5033832.html}
}

@article{lakensImprovingTransparencyFalsifiability2021a,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine-Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa M.},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  urldate = {2022-09-08},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  langid = {english},
  keywords = {hypothesis testing,machine readability,metadata,scholarly communication}
}

@article{lakensPerformingHighpoweredStudies2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2023},
  urldate = {2023-07-13},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/KJ4QK66Y/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf;/Users/cristian/Zotero/storage/IQKNTY3S/ejsp.html}
}

@article{lakensPracticalAlternativeValue2021,
  title = {The {{Practical Alternative}} to the p {{Value Is}} the {{Correctly Used}} p {{Value}}},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {3},
  pages = {639--648},
  issn = {1745-6916},
  doi = {10.1177/1745691620958012},
  urldate = {2023-03-08},
  abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ``statistician's fallacy,'' whereby they declare which statistic researchers really ``want to know.'' Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  pmcid = {PMC8114329},
  pmid = {33560174},
  file = {/Users/cristian/Zotero/storage/STU4MLXV/Lakens - 2021 - The Practical Alternative to the p Value Is the Co.pdf}
}

@article{lakensProfessorsAreNot2017c,
  title = {Professors {{Are Not Elderly}}: {{Evaluating}} the {{Evidential Value}} of {{Two Social Priming Effects}} through {{P-Curve Analyses}}},
  shorttitle = {Professors {{Are Not Elderly}}},
  author = {Lakens, Daniel},
  year = {2017},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/3m5y9},
  urldate = {2022-11-16},
  abstract = {It is possible that the number of false positives in the literature is much greater than is desirable due to a combination of low statistical power, publication bias, and flexibility when analyzing data. Recently, some researchers have argued the replicability crisis social priming research is greatly exaggerated (Dijksterhuis, 2014; Stroebe \& Strack, 2014). To quantify the extent to which researcher degrees of freedom are a real problem, I present two p-curve analyses that examine the evidential value of research lines on professor priming and elderly priming. The results indicate studies examining elderly priming are p-hacked, while studies examining professor priming contain evidential value. I believe a polarized discussion about whether social priming is true or not, whether direct replications or conceptual replications are preferable, or whether methodological rigor or theory development is needed is unlikely to lead to scientific progress. Instead, we have to meta-analytically evaluate individual effects based on their evidential value, and collaboratively examine what is likely to be true.},
  langid = {american},
  keywords = {Social and Behavioral Sciences,Social and Personality Psychology}
}

@article{lakensReproducibilityMetaanalysesSix2016,
  title = {On the Reproducibility of Meta-Analyses: {{Six}} Practical Recommendations},
  author = {Lakens, Dani{\"e}l and Hilgard, Joe and Staaks, Janneke},
  year = {2016},
  month = may,
  journal = {BMC Psychology},
  volume = {4},
  number = {1},
  pages = {24},
  doi = {10.1186/s40359-016-0126-3},
  abstract = {Background Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. DiscussionThe present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including quotes from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel `registered report' formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. SummaryThe recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of ``future-proofing'' meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge.},
  file = {/Users/cristian/Zotero/storage/5YTLY8MK/Lakens et al. - 2016 - On the reproducibility of meta-analyses Six pract.pdf}
}

@misc{lakensRethinkingTypeErrors2025,
  title = {Rethinking {{Type S}} and {{M Errors}}},
  author = {Lakens, Daniel and Cristian and {Xavier-Quintais}, Gabriela and Rasti, Sajedeh and Toffalini, Enrico and Alto{\`e}, Gianmarco},
  year = {2025},
  month = aug,
  publisher = {OSF},
  doi = {10.31234/osf.io/2phzb_v1},
  urldate = {2025-08-12},
  abstract = {Gelman and Carlin (2014) introduced Type S (sign) and Type M (magnitude) errors to highlight the possibility that statistically significant results in published articles are misleading. While these concepts have been proposed to be useful both when designing a study (prospective) and when evaluating results (retroactive), we argue that these statistics do not facilitate the proper design of studies, nor the meaningful interpretation of results. Type S errors are a response to the criticism of testing against a point null of exactly zero in contexts where true zero effects are implausible. Testing against a minimum-effect, while controlling the Type 1 error rate, provides a more coherent and practically useful alternative. Type M errors warn against effect size inflation after selectively reporting significant results, but we argue that statistical indices such as the critical effect size or bias adjusted effect size are preferable approaches. We do believe that Type S and M errors can be valuable in statistics education where the principles of error control are explained, and in the discussion section of studies that fail to follow good research practices. Overall, we argue their use-cases are more limited than is currently recognized, and alternative solutions deserve greater attention.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {Bias correction,Error control,Hypothesis Testing,Type M error,Type S error},
  file = {/Users/cristian/Zotero/storage/53CFYTAC/Lakens et al. - 2025 - Rethinking Type S and M Errors.pdf}
}

@article{lakensSailingSeasChaos2014,
  title = {Sailing {{From}} the {{Seas}} of {{Chaos Into}} the {{Corridor}} of {{Stability}}: {{Practical Recommendations}} to {{Increase}} the {{Informational Value}} of {{Studies}}},
  shorttitle = {Sailing {{From}} the {{Seas}} of {{Chaos Into}} the {{Corridor}} of {{Stability}}},
  author = {Lakens, Dani{\"e}l and Evers, Ellen R. K.},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {9},
  number = {3},
  pages = {278--292},
  issn = {1745-6924},
  doi = {10.1177/1745691614528520},
  abstract = {Recent events have led psychologists to acknowledge that the inherent uncertainty encapsulated in an inductive science is amplified by problematic research practices. In this article, we provide a practical introduction to recently developed statistical tools that can be used to deal with these uncertainties when performing and evaluating research. In Part 1, we discuss the importance of accurate and stable effect size estimates as well as how to design studies to reach a corridor of stability around effect size estimates. In Part 2, we explain how, given uncertain effect size estimates, well-powered studies can be designed with sequential analyses. In Part 3, we (a) explain what p values convey about the likelihood that an effect is true, (b) illustrate how the v statistic can be used to evaluate the accuracy of individual studies, and (c) show how the evidential value of multiple studies can be examined with a p-curve analysis. We end by discussing the consequences of incorporating our recommendations in terms of a reduced quantity, but increased quality, of the research output. We hope that the practical recommendations discussed in this article will provide researchers with the tools to make important steps toward a psychological science that allows researchers to differentiate among all possible truths on the basis of their likelihood.},
  langid = {english},
  pmid = {26173264},
  keywords = {confidence intervals,induction,p-curve analysis,sequential analyses,v statistic},
  file = {/Users/cristian/Zotero/storage/F2M5ZULQ/Lakens and Evers - 2014 - Sailing From the Seas of Chaos Into the Corridor o.pdf}
}

@article{lakensSimulationbasedPowerAnalysis2021,
  ids = {lakensSimulationBasedPowerAnalysis2021a},
  title = {Simulation-Based Power Analysis for Factorial Analysis of Variance Designs},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  publisher = {Sage Publications},
  address = {US},
  issn = {2515-2467},
  doi = {10.1177/2515245920951503},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need {$\eta$}p{$^2$} or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Analysis of Variance,Computer Simulation,Hypothesis Testing,Open Data,Sample Size,Statistical Power},
  file = {/Users/cristian/Zotero/storage/GVSNUR5Q/Lakens and Caldwell - 2021 - Simulation-based power analysis for factorial anal.pdf;/Users/cristian/Zotero/storage/LXLB3CZ2/2021-33483-001.html}
}

@article{lakensSimulationBasedPowerAnalysis2021a,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920951503},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920951503},
  urldate = {2025-07-02},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need {$\eta$}2p or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/JIJN63FA/Lakens and Caldwell - 2021 - Simulation-Based Power Analysis for Factorial Anal.pdf}
}

@article{lakensWhatPhackingReally2015,
  title = {What P-Hacking Really Looks like: {{A}} Comment on {{Masicampo}} and {{LaLande}} (2012)},
  author = {Lakens, Dani{\"e}l},
  year = {2015},
  month = apr,
  journal = {The Quarterly Journal of Experimental Psychology},
  volume = {68},
  number = {4},
  pages = {829--832},
  publisher = {Routledge},
  doi = {10.1080/17470218.2014.982664},
  pmid = {25484109},
  file = {/Users/cristian/Zotero/storage/WJMNIIHV/Lakens - 2015 - What p-hacking really looks like A comment on Mas.pdf;/Users/cristian/Zotero/storage/X3Q2IWNN/17470218.2014.html}
}

@article{lakensWhenHowDeviate2024,
  title = {When and {{How}} to {{Deviate From}} a {{Preregistration}}},
  author = {Lakens, Dani{\"e}l},
  editor = {van Ravenzwaaij, Don},
  year = {2024},
  month = may,
  journal = {Collabra: Psychology},
  volume = {10},
  number = {1},
  pages = {117094},
  issn = {2474-7394},
  doi = {10.1525/collabra.117094},
  urldate = {2025-03-13},
  abstract = {As the practice of preregistration becomes more common, researchers need guidance in how to report deviations from their preregistered statistical analysis plan. A principled approach to the use of preregistration should not treat all deviations as problematic. Deviations from a preregistered analysis plan can both reduce and increase the severity of a test, as well as increase the validity of inferences. I provide examples of how researchers can present deviations from preregistrations and evaluate the consequences of the deviation when encountering 1) unforeseen events, 2) errors in the preregistration, 3) missing information, 4) violations of untested assumptions, and 5) falsification of auxiliary hypotheses. The current manuscript aims to provide a principled approach to deciding when to deviate from a preregistration and how to report deviations from an error-statistical philosophy grounded in methodological falsificationism. The goal is to help researchers reflect on the consequence of deviations from preregistrations by evaluating the test's severity and the validity of the inference.},
  file = {/Users/cristian/Zotero/storage/3XCYXMLW/Lakens - 2024 - When and How to Deviate From a Preregistration.pdf;/Users/cristian/Zotero/storage/XZI78KZS/When-and-How-to-Deviate-From-a-Preregistration.html}
}

@article{lakensWhyValuesAre2022,
  title = {Why {{P}} Values Are Not Measures of Evidence},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Trends in Ecology \& Evolution},
  volume = {37},
  number = {4},
  pages = {289--290},
  publisher = {Elsevier},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2021.12.006},
  urldate = {2023-03-04},
  langid = {english},
  pmid = {35027226},
  file = {/Users/cristian/Zotero/storage/H4P9XR5S/Lakens - 2022 - Why P values are not measures of evidence.pdf}
}

@article{lakicevicEffectsRapidWeight2020,
  title = {Effects of Rapid Weight Loss on Judo Athletes: {{A}} Systematic Review},
  author = {Lakicevic, N. and Roklicer, R. and Bianco, A. and Mani, D. and Paoli, A. and Trivic, T. and Ostojic, S.M. and Milovancev, A. and Maksimovic, N. and Drid, P.},
  year = {2020},
  journal = {Nutrients},
  volume = {12},
  number = {5},
  publisher = {MDPI AG},
  doi = {10.3390/nu12051220}
}

@article{langenberg_tutorial,
  title = {A Tutorial on Using the Paired t Test for Power Calculations in Repeated Measures {{ANOVA}} with Interactions},
  author = {Langenberg, Benedikt and Janczyk, Markus and Koob, Valentin and Kliegl, Reinhold and Mayer, Axel},
  year = {2023},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {55},
  number = {5},
  pages = {2467--2484},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-01902-8},
  urldate = {2024-09-30},
  abstract = {The a priori calculation of statistical power has become common practice in behavioral and social sciences to calculate the necessary sample size for detecting an expected effect size with a certain probability (i.e., power). In multi-factorial repeated measures ANOVA, these calculations can sometimes be cumbersome, especially for higher-order interactions. For designs that only involve factors with two levels each, the paired t test can be used for power calculations, but some pitfalls need to be avoided. In this tutorial, we provide practical advice on how to express main and interaction effects in repeated measures ANOVA as single difference variables. In particular, we demonstrate how to calculate the effect size Cohen's d of this difference variable either based on means, variances, and covariances of conditions or by transforming \$\$\{{\textbackslash}eta \_\{p\}{\textasciicircum}\{2\}\}\$\$or \$\$\{{\textbackslash}omega \_\{p\}{\textasciicircum}\{2\}\}\$\$from the ANOVA framework into d. With the effect size correctly specified, we then show how to use the t test for sample size considerations by means of an empirical example. The relevant R code is provided in an online repository for all example calculations covered in this article.},
  langid = {english},
  keywords = {Effect sizes,Interactions,Power,Repeated measures ANOVA},
  file = {/Users/cristian/Zotero/storage/HR48KSL7/Langenberg et al. - 2023 - A tutorial on using the paired t test for power ca.pdf}
}

@article{laplaneWhyScienceNeeds2019,
  title = {Why Science Needs Philosophy},
  author = {Laplane, Lucie and Mantovani, Paolo and Adolphs, Ralph and Chang, Hasok and Mantovani, Alberto and {McFall-Ngai}, Margaret and Rovelli, Carlo and Sober, Elliott and Pradeu, Thomas},
  year = {2019},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {10},
  pages = {3948--3952},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1900357116},
  urldate = {2025-06-05},
  file = {/Users/cristian/Zotero/storage/RMCD5ACA/Laplane et al. - 2019 - Why science needs philosophy.pdf}
}

@article{larsenRateGrowthScientific2010,
  title = {The Rate of Growth in Scientific Publication and the Decline in Coverage Provided by {{Science Citation Index}}},
  author = {Larsen, Peder Olesen and {von Ins}, Markus},
  year = {2010},
  journal = {Scientometrics},
  volume = {84},
  number = {3},
  pages = {575--603},
  issn = {0138-9130},
  doi = {10.1007/s11192-010-0202-z},
  urldate = {2022-03-16},
  abstract = {The growth rate of scientific publication has been studied from 1907 to 2007 using available data from a number of literature databases, including Science Citation Index (SCI) and Social Sciences Citation Index (SSCI). Traditional scientific publishing, that is publication in peer-reviewed journals, is still increasing although there are big differences between fields. There are no indications that the growth rate has decreased in the last 50~years. At the same time publication using new channels, for example conference proceedings, open archives and home pages, is growing fast. The growth rate for SCI up to 2007 is smaller than for comparable databases. This means that SCI was covering a decreasing part of the traditional scientific literature. There are also clear indications that the coverage by SCI is especially low in some of the scientific areas with the highest growth rate, including computer science and engineering sciences. The role of conference proceedings, open access archives and publications published on the net is increasing, especially in scientific fields with high growth rates, but this has only partially been reflected in the databases. The new publication channels challenge the use of the big databases in measurements of scientific productivity or output and of the growth rate of science. Because of the declining coverage and this challenge it is problematic that SCI has been used and is used as the dominant source for science indicators based on publication and citation numbers. The limited data available for social sciences show that the growth rate in SSCI was remarkably low and indicate that the coverage by SSCI was declining over time. National Science Indicators from Thomson Reuters is based solely on SCI, SSCI and Arts and Humanities Citation Index (AHCI). Therefore the declining coverage of the citation databases problematizes the use of this source.},
  pmcid = {PMC2909426},
  pmid = {20700371},
  file = {/Users/cristian/Zotero/storage/C64375AZ/Larsen and von Ins - 2010 - The rate of growth in scientific publication and t.pdf}
}

@article{latanCrossingRedLine2021,
  title = {Crossing the {{Red Line}}? {{Empirical Evidence}} and {{Useful Recommendations}} on {{Questionable Research Practices}} among {{Business Scholars}}},
  shorttitle = {Crossing the {{Red Line}}?},
  author = {Latan, Hengky and Chiappetta Jabbour, Charbel Jose and {Lopes de Sousa Jabbour}, Ana Beatriz and Ali, Murad},
  year = {2021},
  month = nov,
  journal = {Journal of Business Ethics},
  issn = {1573-0697},
  doi = {10.1007/s10551-021-04961-7},
  urldate = {2023-03-02},
  abstract = {Academic leaders in management from all over the world---including recent calls by the Academy of Management Shaw (Academy of Management Journal 60(3): 819--822, 2017)---have urged further research into the extent and use of questionable research practices~(QRPs). In order to provide empirical evidence on the topic of QRPs, this work presents two linked studies. Study 1 determines the level of use of QRPs based on self-admission rates and estimated prevalence among business scholars in Indonesia. It was determined that if the level of QRP use identified in Study 1 was quite high, Study 2 would be conducted to follow-up on this result, and this was indeed the case. Study 2 examines the factors that encourage and discourage the use of QRPs in the sample analyzed. The main research findings are as follows: (a) in Study 1, we found the self-admission rates and estimated prevalence of business scholars' involvement in QRPs to be quite high when compared with studies conducted in other countries and (b) in Study 2, we found pressure for publication from universities, fear of rejection of manuscripts, meeting the expectations of reviewers, and available rewards to be the main reasons for the use of QRPs in Indonesia, whereas (c) formal sanctions and prevention efforts are factors that discourage QRPs. Recommendations for stakeholders (in this case, reviewers, editors, funders, supervisors, chancellors and others) are also provided in order to reduce the use of QRPs.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UNARM567/Latan et al. - 2021 - Crossing the Red Line Empirical Evidence and Usef.pdf}
}

@article{lawlerMisalignmentResearchHypotheses2021,
  title = {Misalignment {{Between Research Hypotheses}} and {{Statistical Hypotheses}}: {{A Threat}} to {{Evidence-Based Medicine}}?},
  shorttitle = {Misalignment {{Between Research Hypotheses}} and {{Statistical Hypotheses}}},
  author = {Lawler, Insa and Zimmermann, Georg},
  year = {2021},
  month = apr,
  journal = {Topoi},
  volume = {40},
  number = {2},
  pages = {307--318},
  issn = {1572-8749},
  doi = {10.1007/s11245-019-09667-0},
  urldate = {2023-08-09},
  abstract = {Evidence-based medicine frequently~uses statistical hypothesis testing. In this paradigm, data can only disconfirm a research hypothesis' competitors: One tests the negation of a statistical hypothesis that is supposed to correspond to the research hypothesis. In practice, these hypotheses are often misaligned. For instance, directional research hypotheses are often paired with non-directional statistical hypotheses. Prima facie, one cannot gain proper evidence for one's research hypothesis employing a misaligned statistical hypothesis. This paper sheds lights on the nature of and the reasons for such misalignments and it provides a thorough analysis of whether they pose a threat to evidence-based medicine. The upshots are that the misalignments are often hidden for clinicians and that although some cases of misalignments can be partially counterbalanced, the overall threat is non-negligible. The counterbalances either lead to methodological inadequacy (in addition to the misalignment), loss of statistical power, or involve a (potential) lack of information that could be crucial for decision making. This result casts doubt on various findings of medical studies in addition to issues associated with under-powered studies or the replication crisis.},
  langid = {english},
  keywords = {Clinical decision making,Evidence-based medicine,Null hypotheses,Research hypotheses,Statistical hypothesis testing},
  file = {/Users/cristian/Zotero/storage/R7VEGEEY/Lawler and Zimmermann - 2021 - Misalignment Between Research Hypotheses and Stati.pdf}
}

@article{lawranceWhatEstimandHow2020,
  ids = {lawranceWhatEstimandHow2020a},
  title = {What Is an Estimand \& How Does It Relate to Quantifying the Effect of Treatment on Patient-Reported Quality of Life Outcomes in Clinical Trials?},
  author = {Lawrance, Rachael and Degtyarev, Evgeny and Griffiths, Philip and Trask, Peter and Lau, Helen and D'Alessio, Denise and Griebsch, Ingolf and Wallenstein, Gudrun and Cocks, Kim and Rufibach, Kaspar},
  year = {2020},
  month = aug,
  journal = {Journal of Patient-Reported Outcomes},
  volume = {4},
  number = {1},
  pages = {68},
  issn = {2509-8020},
  doi = {10.1186/s41687-020-00218-5},
  urldate = {2024-08-29},
  abstract = {Published in 2019, a new addendum to the ICH E9 guideline presents the estimand framework as a systematic approach to ensure alignment among clinical trial objectives, trial execution/conduct, statistical analyses, and interpretation of results. The use of the estimand framework for describing clinical trial objectives has yet to be extensively considered in the context of patient-reported outcomes (PROs). We discuss the application of the estimand framework to PRO objectives when designing clinical trials in the future, with a focus on PRO outcomes in oncology trial settings as our example.},
  keywords = {Clinical trial,Design,Estimand,HRQoL,ICH,Objective,Patient,PRO,Treatment effect},
  file = {/Users/cristian/Zotero/storage/QUILXSF7/Lawrance et al. - 2020 - What is an estimand & how does it relate to quanti.pdf;/Users/cristian/Zotero/storage/QFLPMCKD/s41687-020-00218-5.html}
}

@article{lebelBriefGuideEvaluate2018,
  title = {A {{Brief Guide}} to {{Evaluate Replications}}},
  author = {LeBel, Etienne P. and Vanpaemel, Wolf and Cheung, Irene and Campbell, Lorne},
  year = {2018},
  month = jan,
  publisher = {OSF},
  doi = {None},
  urldate = {2021-02-01},
  abstract = {The importance of replication is becoming increasingly appreciated, however, considerably less consensus exists about how to evaluate the design and results of replications. We make concrete recommendations on how to evaluate replications with more nuance than what is typically done currently in the literature. We highlight six study characteristics that are crucial for evaluating replications: replication method similarity, replication differences, investigator independence, method/data transparency, analytic result reproducibility, and auxiliary hypotheses' plausibility evidence. We also recommend a more nuanced approach to statistically interpret replication results at the individual-study and meta-analytic levels, and propose clearer language to communicate replication results.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/MUZE7S5A/bku9j.html}
}

@article{lebelFalsifiabilityNotOptional2017,
  title = {Falsifiability Is Not Optional},
  author = {LeBel, Etienne P. and Berger, Derek and Campbell, Lorne and Loving, Timothy J.},
  year = {2017},
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {2},
  pages = {254--261},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315},
  doi = {10.1037/pspi0000106},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 113(5) of Journal of Personality and Social Psychology (see record 2017-45261-001). In the reply, there were two errors in the References list. The publishing year for the 14th and 21st articles was cited incorrectly as 2016. The in-text acronym associated with these citations should read instead as FER2017 and LCL2017. The correct References list citations should read as follows, respectively: Finkel, E. J., Eastwick, P. W., \& Reis, H. T. (2017). Replicability and other features of a high-quality science: Toward a balanced and empirical approach. Journal of Personality and Social Psychology, 113, 244--253. http://dx.doi.org/10.1037/pspi0000075 LeBel, E. P., Campbell, L., \& Loving, T. J. (2017). Benefits of open and high-powered research outweigh costs. Journal of Personality and Social Psychology, 113, 230--243. http://dx.doi.org/10 .1037/pspi0000049. The online version of this article has been corrected.] Finkel, Eastwick, and Reis (2016; FER2016) argued the post-2011 methodological reform movement has focused narrowly on replicability, neglecting other essential goals of research. We agree multiple scientific goals are essential, but argue, however, a more fine-grained language, conceptualization, and approach to replication is needed to accomplish these goals. Replication is the general empirical mechanism for testing and falsifying theory. Sufficiently methodologically similar replications, also known as direct replications, test the basic existence of phenomena and ensure cumulative progress is possible a priori. In contrast, increasingly methodologically dissimilar replications, also known as conceptual replications, test the relevance of auxiliary hypotheses (e.g., manipulation and measurement issues, contextual factors) required to productively investigate validity and generalizability. Without prioritizing replicability, a field is not empirically falsifiable. We also disagree with FER2016's position that ``bigger samples are generally better, but . . . that very large samples could have the downside of commandeering resources that would have been better invested in other studies'' (abstract). We identify problematic assumptions involved in FER2016's modifications of our original research-economic model, and present an improved model that quantifies when (and whether) it is reasonable to worry that increasing statistical power will engender potential trade-offs. Sufficiently powering studies (i.e., {$>$}80\%) maximizes both research efficiency and confidence in the literature (research quality). Given that we are in agreement with FER2016 on all key open science points, we are eager to start seeing the accelerated rate of cumulative knowledge development of social psychological phenomena such a sufficiently transparent, powered, and falsifiable approach will generate. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Experimental Design,Experimental Methods,Experimental Replication,Social Psychology,Statistical Power,Test Validity},
  file = {/Users/cristian/Zotero/storage/A6S4HIED/LeBel et al. - 2017 - Falsifiability is not optional.pdf;/Users/cristian/Zotero/storage/BX94HTFI/2017-30567-003.html}
}

@article{lebelUnifiedFrameworkQuantify2018,
  title = {A {{Unified Framework}} to {{Quantify}} the {{Credibility}} of {{Scientific Findings}}},
  author = {LeBel, Etienne P. and McCarthy, Randy J. and Earp, Brian D. and Elson, Malte and Vanpaemel, Wolf},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {389--402},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918787489},
  urldate = {2022-09-06},
  abstract = {Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.},
  langid = {english},
  keywords = {analytic reproducibility,analytic robustness,open science,replicability,research credibility,transparency},
  file = {/Users/cristian/Zotero/storage/VCUEJ77W/LeBel et al. - 2018 - A Unified Framework to Quantify the Credibility of.pdf}
}

@article{leeReviewFeedingSupplementary2014,
  title = {A Review of Feeding Supplementary Nitrate to Ruminant Animals: {{Nitrate}} Toxicity, Methane Emissions, and Production Performance},
  shorttitle = {Une Revue de l'ajout de Nitrate Dans l'alimentation Des Ruminants : {{Toxicit{\'e}}} Aux Nitrates, {\'E}missions de M{\'e}thane et Performance de Production},
  author = {Lee, C. and Beauchemin, K.A.},
  year = {2014},
  journal = {Canadian Journal of Animal Science},
  volume = {94},
  number = {4},
  pages = {557--570},
  publisher = {Agricultural Institute of Canada},
  doi = {10.4141/CJAS-2014-069}
}

@article{leggettLifeJustSignificant2013,
  title = {The {{Life}} of p: ``{{Just Significant}}'' {{Results}} Are on the {{Rise}}},
  shorttitle = {The {{Life}} of p},
  author = {Leggett, Nathan C. and Thomas, Nicole A. and Loetscher, Tobias and Nicholls, Michael E. R.},
  year = {2013},
  month = dec,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {12},
  pages = {2303--2309},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1080/17470218.2013.863371},
  urldate = {2023-03-01},
  abstract = {Null hypothesis significance testing uses the seemingly arbitrary probability of .05 as a means of objectively determining whether a tested effect is reliable. Within recent psychological articles, research has found an overrepresentation of p values around this cut-off. The present study examined whether this overrepresentation is a product of recent pressure to publish or whether it has existed throughout psychological research. Articles published in 1965 and 2005 from two prominent psychology journals were examined. Like previous research, the frequency of p values at and just below .05 was greater than expected compared to p frequencies in other ranges. While this overrepresentation was found for values published in both 1965 and 2005, it was much greater in 2005. Additionally, p values close to but over .05 were more likely to be rounded down to, or incorrectly reported as, significant in 2005 than in 1965. Modern statistical software and an increased pressure to publish may explain this pattern. The problem may be alleviated by reduced reliance on p values and increased reporting of confidence intervals and effect sizes.},
  langid = {english}
}

@article{lehmannFisherNeymanPearsonTheories1993,
  title = {The {{Fisher}}, {{Neyman-Pearson Theories}} of {{Testing Hypotheses}}: {{One Theory}} or {{Two}}?},
  shorttitle = {The {{Fisher}}, {{Neyman-Pearson Theories}} of {{Testing Hypotheses}}},
  author = {Lehmann, E. L.},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {424},
  eprint = {2291263},
  eprinttype = {jstor},
  pages = {1242--1249},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2291263},
  urldate = {2022-03-29},
  abstract = {The Fisher and Neyman-Pearson approaches to testing statistical hypotheses are compared with respect to their attitudes to the interpretation of the outcome, to power, to conditioning, and to the use of fixed significance levels. It is argued that despite basic philosophical differences, in their main practical aspects the two theories are complementary rather than contradictory and that a unified approach is possible that combines the best features of both. As applications, the controversies about the Behrens-Fisher problem and the comparison of two binomials (2 {\texttimes} 2 tables) are considered from the present point of view.}
}

@article{leisingTenStepsBetter2022,
  title = {Ten {{Steps Toward}} a {{Better Personality Science}} -- {{How Quality May Be Rewarded More}} in {{Research Evaluation}}},
  author = {Leising, Daniel and Thielmann, Isabel and Gl{\"o}ckner, Andreas and G{\"a}rtner, Anne and Sch{\"o}nbrodt, Felix},
  year = {2022},
  month = may,
  journal = {Personality Science},
  volume = {3},
  pages = {1--44},
  doi = {10.5964/ps.6029}
}

@misc{lenhardComputationEffectSizes2017,
  title = {Computation of {{Effect Sizes}}},
  author = {Lenhard, Wolfgang and Lenhard, Alexandra},
  year = {2017},
  doi = {10.13140/RG.2.2.17823.92329},
  urldate = {2022-10-11},
  howpublished = {http://rgdoi.net/10.13140/RG.2.2.17823.92329},
  langid = {english}
}

@article{lenth_posthocpower_2007,
  title = {Statistical Power Calculations},
  author = {Lenth, R},
  year = {2007},
  month = apr,
  journal = {Journal of animal science},
  volume = {85},
  pages = {E24-9},
  doi = {10.2527/jas.2006-449}
}

@misc{LenthRussell2007,
  title = {Lenth, {{Russell V}}. 2007. ``{{Statistical Power Calculations}}.'' {{Journal}} of {{Animal Science}} 85 (Suppl\_13): {{E24}}--29. - {{Google Search}}},
  urldate = {2025-07-02},
  howpublished = {https://www.google.com/search?client=safari\&rls=en\&q=Lenth\%2C+Russell+V.+2007.+\%E2\%80\%9CStatistical+Power+Calculations.\%E2\%80\%9D+Journal+of+Animal+Science+85+(suppl\_13)\%3A+E24\%E2\%80\%9329.\&ie=UTF-8\&oe=UTF-8},
  file = {/Users/cristian/Zotero/storage/5NQAJZEL/search.html}
}

@article{leonardi-beeCommonMethodologicalPitfalls2019,
  title = {Common Methodological Pitfalls and New Developments in Systematic Review Meta-analyses},
  author = {Leonardi-Bee, J. and Flohr, C. and {van Zuuren}, E. J. and Le Cleach, L. and Hollestein, L.M.},
  year = {2019},
  month = oct,
  journal = {British Journal of Dermatology},
  volume = {181},
  number = {4},
  pages = {649--651},
  issn = {0007-0963},
  doi = {10.1111/bjd.18336},
  urldate = {2025-06-12},
  abstract = {Systematic reviews have become an increasingly popular tool to assess best evidence and have an established place at the top of the evidence pyramid, above individual randomized controlled trials (RCTs). Most systematic reviews include a meta-analysis. Over recent years, the methodologies employed have become more complex, and we therefore review common methodological pitfalls and new developments in this editorial.Meta-analyses are a `statistical analysis that combines or integrates the results of several independent clinical trials considered by the analyst to be combinable'1 and provides a statistical summary estimate of the effectiveness (called an effect size) of one intervention/treatment vs. another, for a given population.2 However, methods have been advanced to pool results from other study designs, for example, comparative observational studies for estimating an association between a risk factor and disease, or studies for estimating the prevalence of a disease. Meta-analyses are useful as they provide estimates of effect from more than one study and therefore will have increased statistical power (through larger sample size) and have improved estimates of precision (through smaller standard errors), thus yielding narrower confidence intervals (CIs) for the effect estimate; therefore, the more certain we are that the effect estimate reflects the true effect size. Meta-analyses also help to overcome controversies resulting from disparate results across studies and can generate new hypotheses to be tested in future studies. However, there are many decisions which need to be made before a meta-analysis is conducted as part of a systematic review. We describe some of the common pitfalls that authors may face below.},
  file = {/Users/cristian/Zotero/storage/86Z5RTWJ/LeonardiBee et al. - 2019 - Common methodological pitfalls and new development.pdf;/Users/cristian/Zotero/storage/QBZ4ZMLB/6602464.html}
}

@article{lianPioglitazoneNAFLDPatients2021,
  title = {Pioglitazone for {{NAFLD Patients With Prediabetes}} or {{Type}} 2 {{Diabetes Mellitus}}: {{A Meta-Analysis}}},
  author = {Lian, J. and Fu, J.},
  year = {2021},
  journal = {Frontiers in Endocrinology},
  volume = {12},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fendo.2021.615409}
}

@article{liIntroductionMultiplicityIssues2016,
  title = {An Introduction to Multiplicity Issues in Clinical Trials: The What, Why, When and How},
  shorttitle = {An Introduction to Multiplicity Issues in Clinical Trials},
  author = {Li, Guowei and Taljaard, Monica and Van Den Heuvel, Edwin R. and Levine, Mitchell Ah. and Cook, Deborah J. and Wells, George A. and Devereaux, Philip J. and Thabane, Lehana},
  year = {2016},
  month = dec,
  journal = {International Journal of Epidemiology},
  pages = {dyw320},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyw320},
  urldate = {2025-04-15},
  abstract = {In clinical trials it is not uncommon to face a multiple testing problem which can have an impact on both type I and type II error rates, leading to inappropriate interpretation of trial results. Multiplicity issues may need to be considered at the design, analysis and interpretation stages of a trial. The proportion of trial reports not adequately correcting for multiple testing remains substantial. The purpose of this article is to provide an introduction to multiple testing issues in clinical trials, and to reduce confusion around the need for multiplicity adjustments. We use a tutorial, question-and-answer approach to address the key issues of why, when and how to consider multiplicity adjustments in trials. We summarize the relevant circumstances under which multiplicity adjustments ought to be considered, as well as options for carrying out multiplicity adjustments in terms of trial design factors including Population, Intervention/Comparison, Outcome, Time frame and Analysis (PICOTA). Results are presented in an easy-to-use table and flow diagrams. Confusion about multiplicity issues can be reduced or avoided by considering the potential impact of multiplicity on type I and II errors and, if necessary pre-specifying statistical approaches to either avoid or adjust for multiplicity in the trial protocol or analysis plan.},
  langid = {english}
}

@misc{LimitationsOurUnderstanding,
  title = {The Limitations to Our Understanding of Peer Review {\textbar} {{Research Integrity}} and {{Peer Review}}},
  urldate = {2025-07-02},
  howpublished = {https://link.springer.com/article/10.1186/s41073-020-00092-1},
  file = {/Users/cristian/Zotero/storage/4E4C9P9F/s41073-020-00092-1.html}
}

@misc{linAltmetaAlternativeMetaAnalysis2022,
  title = {Altmeta: {{Alternative Meta-Analysis Methods}}},
  shorttitle = {Altmeta},
  author = {Lin, Lifeng and Rosenberger, Kristine J. and Shi, Linyu and Wang, Yipeng and Chu, Haitao},
  year = {2022},
  month = aug,
  urldate = {2022-09-24},
  abstract = {Provides alternative statistical methods for meta-analysis, including: - bivariate generalized linear mixed models for synthesizing odds ratios, relative risks, and risk differences (Chu et al., 2012 {$<$}doi:10.1177/0962280210393712{$>$}) - heterogeneity tests and measures and penalization methods that are robust to outliers (Lin et al., 2017 {$<$}doi:10.1111/biom.12543{$>$}; Wang et al., 2022 {$<$}doi:10.1002/sim.9261{$>$}); - measures, tests, and visualization tools for publication bias or small-study effects (Lin and Chu, 2018 {$<$}doi:10.1111/biom.12817{$>$}; Lin, 2019 {$<$}doi:10.1002/jrsm.1340{$>$}; Lin, 2020 {$<$}doi:10.1177/0962280220910172{$>$}; Shi et al., 2020 {$<$}doi:10.1002/jrsm.1415{$>$}); - meta-analysis of diagnostic tests for synthesizing sensitivities, specificities, etc. (Reitsma et al., 2005 {$<$}doi:10.1016/j.jclinepi.2005.02.022{$>$}; Chu and Cole, 2006 {$<$}doi:10.1016/j.jclinepi.2006.06.011{$>$}); - meta-analysis methods for synthesizing proportions (Lin and Chu, 2020 {$<$}doi:10.1097/ede.0000000000001232{$>$}); - models for multivariate meta-analysis, measures of inconsistency degrees of freedom in Bayesian network meta-analysis, and predictive P-score (Lin and Chu, 2018 {$<$}doi:10.1002/jrsm.1293{$>$}; Lin, 2020 {$<$}doi:10.1080/10543406.2020.1852247{$>$}; Rosenberger et al., 2021 {$<$}doi:10.1186/s12874-021-01397-5{$>$}).},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {MetaAnalysis}
}

@article{lindenHeterogeneityResearchResults2021,
  title = {Heterogeneity of {{Research Results}}: {{A New Perspective From Which}} to {{Assess}} and {{Promote Progress}} in {{Psychological Science}}},
  shorttitle = {Heterogeneity of {{Research Results}}},
  author = {Linden, Audrey Helen and H{\"o}nekopp, Johannes},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {16},
  number = {2},
  pages = {358--376},
  issn = {1745-6924},
  doi = {10.1177/1745691620964193},
  abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
  langid = {english},
  pmcid = {PMC7961629},
  pmid = {33400613},
  keywords = {Behavioral Research,Female,heterogeneity,Humans,Male,meta-analysis,Meta-Analysis as Topic,philosophy of science,psychological research,Psychology,replication,statistical power},
  file = {/Users/cristian/Zotero/storage/N7BJYZB7/Linden and Hnekopp - 2021 - Heterogeneity of Research Results A New Perspecti.pdf}
}

@article{lindsayReplicationPsychologicalScience2015,
  title = {Replication in {{Psychological Science}}},
  author = {Lindsay, D. Stephen},
  year = {2015},
  month = dec,
  journal = {Psychological Science},
  volume = {26},
  number = {12},
  pages = {1827--1832},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797615616374},
  urldate = {2023-02-20},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/8LSYPNBE/Lindsay - 2015 - Replication in Psychological Science.pdf}
}

@article{lindsaySevenStepsTransparency2020a,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science},
  author = {Lindsay, D. Stephen},
  year = {2020},
  journal = {Canadian Psychology / Psychologie canadienne},
  volume = {61},
  pages = {310--317},
  publisher = {Educational Publishing Foundation},
  address = {US},
  issn = {1878-7304},
  doi = {10.1037/cap0000222},
  abstract = {Psychological scientists strive to advance understanding of how and why we animals do and think and feel as we do. This is difficult, in part because flukes of chance and measurement error obscure researchers' perceptions. Many psychologists use inferential statistical tests to peer through the murk of chance and discern relationships between variables. Those tests are powerful tools, but they must be wielded with skill. Moreover, research reports must convey to readers a detailed and accurate understanding of how the data were obtained and analyzed. Research psychologists often fall short in those regards. This paper attempts to motivate and explain ways to enhance the transparency and replicability of psychological science. Specifically, I speak to how publication bias and p hacking contribute to effect-size exaggeration in the published literature and how effect-size exaggeration contributes, in turn, to replication failures. Then I present seven steps toward addressing these problems: telling the truth; upgrading statistical knowledge; standardizing aspects of research practices; documenting laboratory procedures in a laboratory manual; making materials, data, and analysis scripts transparent; addressing constraints on generality; and collaborating. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Behavioral Sciences,Experimental Replication,Methodology,Procedural Knowledge,Psychologists,Reporting Standards,Statistical Tests},
  file = {/Users/cristian/Zotero/storage/X2EIHXUB/Lindsay - 2020 - Seven steps toward transparency and replicability .pdf;/Users/cristian/Zotero/storage/P6Z4GIWX/2020-59196-001.html}
}

@article{linQuantifyingPublicationBias2018,
  title = {Quantifying Publication Bias in Meta-Analysis},
  author = {Lin, Lifeng and Chu, Haitao},
  year = {2018},
  month = sep,
  journal = {Biometrics},
  volume = {74},
  number = {3},
  pages = {785--794},
  issn = {1541-0420},
  doi = {10.1111/biom.12817},
  abstract = {Publication bias is a serious problem in systematic reviews and meta-analyses, which can affect the validity and generalization of conclusions. Currently, approaches to dealing with publication bias can be distinguished into two classes: selection models and funnel-plot-based methods. Selection models use weight functions to adjust the overall effect size estimate and are usually employed as sensitivity analyses to assess the potential impact of publication bias. Funnel-plot-based methods include visual examination of a funnel plot, regression and rank tests, and the nonparametric trim and fill method. Although these approaches have been widely used in applications, measures for quantifying publication bias are seldom studied in the literature. Such measures can be used as a characteristic of a meta-analysis; also, they permit comparisons of publication biases between different meta-analyses. Egger's regression intercept may be considered as a candidate measure, but it lacks an intuitive interpretation. This article introduces a new measure, the skewness of the standardized deviates, to quantify publication bias. This measure describes the asymmetry of the collected studies' distribution. In addition, a new test for publication bias is derived based on the skewness. Large sample properties of the new measure are studied, and its performance is illustrated using simulations and three case studies.},
  langid = {english},
  pmcid = {PMC5953768},
  pmid = {29141096},
  keywords = {{Models, Statistical},Animals,Computer Simulation,Heterogeneity,Humans,Meta-analysis,Meta-Analysis as Topic,Ocular Motility Disorders,Publication bias,Publication Bias,Reference Standards,Regression Analysis,Sample Size,Skewness,Standardized deviate,Statistical power},
  file = {/Users/cristian/Zotero/storage/8L3RDFXP/Lin and Chu - 2018 - Quantifying publication bias in meta-analysis.pdf}
}

@article{lipseyEfficacyPsychologicalEducational1993,
  title = {The Efficacy of Psychological, Educational, and Behavioral Treatment: {{Confirmation}} from Meta-Analysis},
  shorttitle = {The Efficacy of Psychological, Educational, and Behavioral Treatment},
  author = {Lipsey, Mark W. and Wilson, David B.},
  year = {1993},
  journal = {American Psychologist},
  volume = {48},
  number = {12},
  pages = {1181--1209},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.48.12.1181},
  abstract = {Conventional reviews of research on the efficacy of psychological, educational, and behavioral treatments often find considerable variation in outcome among studies and, as a consequence, fail to reach firm conclusions about the overall effectiveness of the interventions in question. In contrast, meta-analysis reviews show a strong, dramatic pattern of positive overall effects that cannot readily be explained as artifacts of meta-analytic technique or generalized placebo effects. Moreover, the effects are not so small that they can be dismissed as lacking practical or clinical significance. Although meta-analysis has limitations, there are good reasons to believe that its results are more credible than those of conventional reviews and to conclude that well-developed psychological, educational, and behavioral treatment is generally efficacious. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Behavior Therapy,Education,Meta Analysis,Psychotherapy,Treatment Effectiveness Evaluation},
  file = {/Users/cristian/Zotero/storage/LJ9H6XI7/1994-18340-001.html}
}

@article{liRiskScoreSystem2018,
  title = {Risk Score System for the Prediction of Hepatocellular Carcinoma in Patients with Type 2 Diabetes: {{Taiwan Diabetes Study}}},
  author = {Li, T.-C. and Li, C.-I. and Liu, C.-S. and Lin, W.-Y. and Lin, C.-H. and Yang, S.-Y. and Lin, C.-C.},
  year = {2018},
  journal = {Seminars in Oncology},
  volume = {45},
  number = {5-6},
  pages = {264--274},
  publisher = {W.B. Saunders},
  doi = {10.1053/j.seminoncol.2018.07.006}
}

@article{liuEfficacyDanshenClass2019,
  title = {Efficacy of {{Danshen Class Injection}} in the {{Treatment}} of {{Acute Cerebral Infarction}}: {{A Bayesian Network Meta-Analysis}} of {{Randomized Controlled Trials}}},
  author = {Liu, S. and Wang, K. and Duan, X. and Wu, J. and Zhang, D. and Liu, X. and Zhao, Y.},
  year = {2019},
  journal = {Evidence-based Complementary and Alternative Medicine},
  volume = {2019},
  publisher = {Hindawi Limited},
  doi = {10.1155/2019/5814749}
}

@article{liuFeedingGlycerolenrichedYeast2014,
  title = {Feeding Glycerol-Enriched Yeast Culture Improves Performance, Energy Status, and Heat Shock Protein Gene Expression of Lactating {{Holstein}} Cows under Heat Stress},
  author = {Liu, J. and Ye, G. and Zhou, Y. and Liu, Y. and Zhao, L. and Liu, Y. and Chen, X. and Huang, D. and Liao, S.F. and Huang, K.},
  year = {2014},
  journal = {Journal of Animal Science},
  volume = {92},
  number = {6},
  pages = {2494--2502},
  publisher = {American Society of Animal Science},
  doi = {10.2527/jas.2013-7152}
}

@misc{LoadMuscleGroup,
  ids = {LoadMuscleGroupa},
  title = {Load and Muscle Group Size Influence the Ergogenic Effect of Acute Caffeine Intake in Muscular Strength, Power and Endurance {\textbar} {{SpringerLink}}},
  urldate = {2023-07-20},
  howpublished = {https://link.springer.com/article/10.1007/s00394-023-03109-9},
  file = {/Users/cristian/Zotero/storage/WBAXDC9P/s00394-023-03109-9.html}
}

@article{longchampPredictorsComplicationsLiver2021,
  title = {Predictors of Complications after Liver Surgery: A Systematic Review of the Literature},
  author = {Longchamp, G. and Labgaa, I. and Demartines, N. and Joliat, G.-R.},
  year = {2021},
  journal = {HPB},
  volume = {23},
  number = {5},
  pages = {645--655},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.hpb.2020.12.009}
}

@article{longMetabolomicsguidedGlobalPathway2021,
  title = {Metabolomics-Guided Global Pathway Analysis Reveals Better Insights into the Metabolic Alterations of Breast Cancer},
  author = {Long, N.P. and Heo, D. and Kim, H.-Y. and Kim, T.H. and Shin, J.-G. and Lee, A. and Kim, D.-H.},
  year = {2021},
  journal = {Journal of Pharmaceutical and Biomedical Analysis},
  volume = {202},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.jpba.2021.114134}
}

@article{lopez-nicolasMetareviewTransparencyReproducibilityrelated2022,
  ids = {lopez-nicolasMetareviewTransparencyReproducibilityrelated2022a},
  title = {A Meta-Review of Transparency and Reproducibility-Related Reporting Practices in Published Meta-Analyses on Clinical Psychological Interventions (2000-2020)},
  author = {{L{\'o}pez-Nicol{\'a}s}, Rub{\'e}n and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Rubio-Aparicio}, Mar{\'i}a and {S{\'a}nchez-Meca}, Julio},
  year = {2022},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {54},
  number = {1},
  pages = {334--349},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01644-z},
  abstract = {Meta-analysis is a powerful and important tool to synthesize the literature about a research topic. Like other kinds of research, meta-analyses must be reproducible to be compliant with the principles of the scientific method. Furthermore, reproducible meta-analyses can be easily updated with new data and reanalysed applying new and more refined analysis techniques. We attempted to empirically assess the prevalence of transparency and reproducibility-related reporting practices in published meta-analyses from clinical psychology by examining a random sample of 100 meta-analyses. Our purpose was to identify the key points that could be improved, with the aim of providing some recommendations for carrying out reproducible meta-analyses. We conducted a meta-review of meta-analyses of psychological interventions published between 2000 and 2020. We searched PubMed, PsycInfo and Web of Science databases. A structured coding form to assess transparency indicators was created based on previous studies and existing meta-analysis guidelines. We found major issues concerning: completely reproducible search procedures report, specification of the exact method to compute effect sizes, choice of weighting factors and estimators, lack of availability of the raw statistics used to compute the effect size and of interoperability of available data, and practically total absence of analysis script code sharing. Based on our findings, we conclude with recommendations intended to improve the transparency, openness, and reproducibility-related reporting practices of meta-analyses in clinical psychology and related areas.},
  langid = {english},
  pmcid = {PMC8863703},
  pmid = {34173943},
  keywords = {Data sharing,Humans,Meta-analysis,Meta-Analysis as Topic,Meta-science,Prevalence,Psychosocial Intervention,Reproducibility,Reproducibility of Results,Research Design,Transparency and openness practices},
  file = {/Users/cristian/Zotero/storage/8YTZ5IFG/Lpez-Nicols et al. - 2022 - A meta-review of transparency and reproducibility-.pdf}
}

@article{lopez-nicolasReproducibilityPublishedMetaAnalyses2024,
  title = {Reproducibility of {{Published Meta-Analyses}} on {{Clinical-Psychological Interventions}}},
  author = {{L{\'o}pez-Nicol{\'a}s}, Rub{\'e}n and Lakens, Daniel and {L{\'o}pez-L{\'o}pez}, Jose A. and {Rubio-Aparicio}, Maria and {Sandoval-Lentisco}, Alejandro and {L{\'o}pez-Ib{\'a}{\~n}ez}, Carmen and {Bl{\'a}zquez-Rinc{\'o}n}, Desir{\'e}e and {S{\'a}nchez-Meca}, Julio},
  year = {2024},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {1},
  pages = {25152459231202929},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231202929},
  urldate = {2025-06-12},
  abstract = {Meta-analysis is one of the most useful research approaches, the relevance of which relies on its credibility. Reproducibility of scientific results could be considered as the minimal threshold of this credibility. We assessed the reproducibility of a sample of meta-analyses published between 2000 and 2020. From a random sample of 100 articles reporting results of meta-analyses of interventions in clinical psychology, 217 meta-analyses were selected. We first tried to retrieve the original data by recovering a data file, recoding the data from document files, or requesting it from original authors. Second, through a multistage workflow, we tried to reproduce the main results of each meta-analysis. The original data were retrieved for 67\% (146/217) of meta-analyses. Although this rate showed an improvement over the years, in only 5\% of these cases was it possible to retrieve a data file ready for reuse. Of these 146, 52 showed a discrepancy larger than 5\% in the main results in the first stage. For 10 meta-analyses, this discrepancy was solved after fixing a coding error of our data-retrieval process, and for 15 of them, it was considered approximately reproduced in a qualitative assessment. In the remaining meta-analyses (18\%, 27/146), different issues were identified in an in-depth review, such as reporting inconsistencies, lack of data, or transcription errors. Nevertheless, the numerical discrepancies were mostly minor and had little or no impact on the conclusions. Overall, one of the biggest threats to the reproducibility of meta-analysis is related to data availability and current data-sharing practices in meta-analysis.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/X4HJJXW7/Lpez-Nicols et al. - 2024 - Reproducibility of Published Meta-Analyses on Clin.pdf}
}

@article{lopez-samanesEffectsAcuteCaffeine2021,
  title = {Effects of Acute Caffeine Ingestion on Futsal Performance in Sub-Elite Players},
  author = {{L{\'o}pez-Samanes}, {\'A}lvaro and {Moreno-P{\'e}rez}, V{\'i}ctor and Travassos, Bruno and Del Coso, Juan},
  year = {2021},
  month = dec,
  journal = {European Journal of Nutrition},
  volume = {60},
  number = {8},
  pages = {4531--4540},
  issn = {1436-6215},
  doi = {10.1007/s00394-021-02617-w},
  urldate = {2023-08-15},
  abstract = {To date, no previous investigation has studied the effect of acute caffeine ingestion on futsal performance during futsal-specific testing and during a simulated match. Therefore, the aim of this investigation was to establish the effects of acute caffeine intake on futsal-specific tests and match-play running performance in male futsal players.},
  langid = {english},
  keywords = {External load,Sports nutrition,Sports performance,Supplement,Team sports},
  file = {/Users/cristian/Zotero/storage/QLGDXIEX/Lpez-Samanes et al. - 2021 - Effects of acute caffeine ingestion on futsal perf.pdf}
}

@article{lorenzocalvoCaffeineCognitiveFunctions2021,
  ids = {calvoCaffeineCognitiveFunctions2021,calvoCaffeineCognitiveFunctions2021a},
  title = {Caffeine and {{Cognitive Functions}} in {{Sports}}: {{A Systematic Review}} and {{Meta-Analysis}}.},
  author = {Lorenzo Calvo, Jorge and Fei, Xueyin and Dom{\'i}nguez, Ra{\'u}l and {Pareja-Galeano}, Helios},
  year = {2021},
  month = mar,
  journal = {Nutrients},
  volume = {13},
  number = {3},
  publisher = {MDPI AG},
  address = {Switzerland},
  issn = {2072-6643},
  doi = {10.3390/nu13030868},
  abstract = {Cognitive functions are essential in any form of exercise. Recently, interest has mounted in addressing the relationship between caffeine intake and cognitive  performance during sports practice. This review examines this relationship  through a structured search of the databases Medline/PubMed and Web of Science  for relevant articles published in English from August 1999 to March 2020. The  study followed PRISMA guidelines. Inclusion criteria were defined according to  the PICOS model. The identified records reported on randomized cross-over studies  in which caffeine intake (as drinks, capsules, energy bars, or gum) was compared  to an identical placebo situation. There were no filters on participants'  training level, gender, or age. For the systematic review, 13 studies examining  the impacts of caffeine on objective measures of cognitive performance or  self-reported cognitive performance were selected. Five of these studies were  also subjected to meta-analysis. After pooling data in the meta-analysis, the  significant impacts of caffeine only emerged on attention, accuracy, and speed.  The results of the 13 studies, nevertheless, suggest that the intake of a  low/moderate dose of caffeine before and/or during exercise can improve  self-reported energy, mood, and cognitive functions, such as attention; it may  also improve simple reaction time, choice reaction time, memory, or fatigue,  however, this may depend on the research protocols.},
  langid = {english},
  pmcid = {PMC8000732},
  pmid = {33800853},
  keywords = {Adult,Athletes/*psychology,Athletic Performance/*psychology,caffeine,Caffeine/*pharmacology,Cognition/*drug effects,cognitive function,ergogenic drinks,Female,Humans,Male,Performance-Enhancing Substances/*pharmacology,Randomized Controlled Trials as Topic,sport}
}

@article{LowPrevalenceofAPrioriPowerAnalysesinMotorBehaviorResearch,
  title = {Low Prevalence of a Priori Power Analyses in Motor Behavior Research},
  author = {McKay, Brad and Corson, Abbey and Vinh, Mary-Anne and Jeyarajan, Gianna and Tandon, Chitrini and Brooks, Hugh and Hubley, Julie and Carter, Michael J.},
  year = {2023},
  journal = {Journal of Motor Learning and Development},
  volume = {11},
  number = {1},
  pages = {15--28},
  publisher = {Human Kinetics},
  address = {Champaign IL, USA},
  doi = {10.1123/jmld.2022-0042}
}

@misc{ludeckeEscEffectSize2018,
  title = {Esc: {{Effect Size Computation For Meta Analysis}}},
  shorttitle = {Esc},
  author = {L{\"u}decke, Daniel},
  year = {2018},
  month = may,
  doi = {10.5281/ZENODO.1249218},
  urldate = {2023-02-02},
  abstract = {Bug fixes 	Fix issue with wrong computation of confidence intervals when converting effect sizes from {$<$}em{$>$}d{$<$}/em{$>$} to {$<$}em{$>$}f{$<$}/em{$>$} ({$<$}code{$>$}esc\_d2f(){$<$}/code{$>$}).},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@article{lundbergWhatYourEstimand2021,
  ids = {lundbergWhatYourEstimand2021a},
  title = {What {{Is Your Estimand}}? {{Defining}} the {{Target Quantity Connects Statistical Evidence}} to {{Theory}}},
  shorttitle = {What {{Is Your Estimand}}?},
  author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M.},
  year = {2021},
  month = jun,
  journal = {American Sociological Review},
  volume = {86},
  number = {3},
  pages = {532--565},
  publisher = {SAGE Publications Inc},
  issn = {0003-1224},
  doi = {10.1177/00031224211004187},
  urldate = {2023-07-06},
  abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity---the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis---the theoretical estimand---in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/E5RG9FUW/Lundberg et al. - 2021 - What Is Your Estimand Defining the Target Quantit.pdf}
}

@article{luoDiagnosticPerformanceTransient2022,
  title = {Diagnostic {{Performance}} of {{Transient Elastography Versus Two-Dimensional Shear Wave Elastography}} for {{Liver Fibrosis}} in {{Chronic Viral Hepatitis}}: {{Direct Comparison}} and a {{Meta-Analysis}}},
  author = {Luo, Q.-T. and Zhu, Q. and Zong, X.-D. and Li, M.-K. and Yu, H.-S. and Jiang, C.-Y. and Liao, X.},
  year = {2022},
  journal = {BioMed Research International},
  volume = {2022},
  publisher = {Hindawi Limited},
  doi = {10.1155/2022/1960244}
}

@article{lvEfficacySafetyIbrutinib2021,
  title = {Efficacy and {{Safety}} of {{Ibrutinib}} in {{Central Nervous System Lymphoma}}: {{A PRISMA-Compliant Single-Arm Meta-Analysis}}},
  author = {Lv, L. and Sun, X. and Wu, Y. and Cui, Q. and Chen, Y. and Liu, Y.},
  year = {2021},
  journal = {Frontiers in Oncology},
  volume = {11},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fonc.2021.707285}
}

@article{maassenReproducibilityIndividualEffect2020,
  title = {Reproducibility of Individual Effect Sizes in Meta-Analyses in Psychology},
  author = {Maassen, Esther and van Assen, Marcel A. L. M. and Nuijten, Mich{\`e}le B. and {Olsson-Collentine}, Anton and Wicherts, Jelte M.},
  year = {2020},
  month = may,
  journal = {PLOS ONE},
  volume = {15},
  number = {5},
  pages = {e0233107},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0233107},
  urldate = {2023-02-13},
  abstract = {To determine the reproducibility of psychological meta-analyses, we investigated whether we could reproduce 500 primary study effect sizes drawn from 33 published meta-analyses based on the information given in the meta-analyses, and whether recomputations of primary study effect sizes altered the overall results of the meta-analysis. Results showed that almost half (k = 224) of all sampled primary effect sizes could not be reproduced based on the reported information in the meta-analysis, mostly because of incomplete or missing information on how effect sizes from primary studies were selected and computed. Overall, this led to small discrepancies in the computation of mean effect sizes, confidence intervals and heterogeneity estimates in 13 out of 33 meta-analyses. We provide recommendations to improve transparency in the reporting of the entire meta-analytic process, including the use of preregistration, data and workflow sharing, and explicit coding practices.},
  langid = {english},
  keywords = {Clinical psychology,Metaanalysis,Peer review,Psychology,Publication ethics,Reproducibility,Research reporting guidelines,Systematic reviews},
  file = {/Users/cristian/Zotero/storage/YCX3U8GP/Maassen et al. - 2020 - Reproducibility of individual effect sizes in meta.pdf}
}

@article{machadoPhototherapyManagementCreatine2020,
  title = {Phototherapy on {{Management}} of {{Creatine Kinase Activity}} in {{General Versus Localized Exercise}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  author = {Machado, A.F. and Micheletti, J.K. and Lopes, J.S.S. and Vanderlei, F.M. and {Leal-Junior}, E.C.P. and Netto Junior, J. and Pastre, C.M.},
  year = {2020},
  journal = {Clinical journal of sport medicine : official journal of the Canadian Academy of Sport Medicine},
  volume = {30},
  number = {3},
  pages = {267--274},
  publisher = {NLM (Medline)},
  doi = {10.1097/JSM.0000000000000606}
}

@article{macuhEffectsNitrateSupplementation2021,
  title = {Effects of Nitrate Supplementation on Exercise Performance in Humans: {{A}} Narrative Review},
  author = {Macuh, M. and Knap, B.},
  year = {2021},
  journal = {Nutrients},
  volume = {13},
  number = {9},
  publisher = {MDPI},
  doi = {10.3390/nu13093183}
}

@article{mahoneyPublicationPrejudicesExperimental1977,
  title = {Publication Prejudices: {{An}} Experimental Study of Confirmatory Bias in the Peer Review System},
  author = {Mahoney, Michael J.},
  year = {1977},
  month = jun,
  journal = {Cognitive Therapy and Research},
  volume = {1},
  pages = {161--175},
  doi = {10.1007/BF01173636},
  abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
  langid = {english}
}

@article{maierJustifyYourAlpha2022,
  ids = {maierJustifyYourAlpha2021},
  title = {Justify {{Your Alpha}}: {{A Primer}} on {{Two Practical Approaches}}},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459221080396},
  urldate = {2023-10-12},
  abstract = {The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {english},
  keywords = {Hypothesis Testing,Meta-science,Quantitative Methods,Social and Behavioral Sciences,Statistical Power,Type 1 Error,Type 2 Error},
  file = {/Users/cristian/Zotero/storage/2NTLASBK/Maier and Lakens - 2021 - Justify Your Alpha A Primer on Two Practical Appr.pdf;/Users/cristian/Zotero/storage/KDK43S2Z/Maier and Lakens - 2022 - Justify Your Alpha A Primer on Two Practical Appr.pdf}
}

@article{maierNoEvidenceNudging2022,
  title = {No Evidence for Nudging after Adjusting for Publication Bias},
  author = {Maier, Maximilian and Barto{\v s}, Franti{\v s}ek and Stanley, T. D. and Shanks, David R. and Harris, Adam J. L. and Wagenmakers, Eric-Jan},
  year = {2022},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {31},
  pages = {e2200300119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2200300119},
  urldate = {2023-02-28},
  file = {/Users/cristian/Zotero/storage/IS46F2NS/Maier et al. - 2022 - No evidence for nudging after adjusting for public.pdf}
}

@article{maierRobustBayesianMetaanalysis2023,
  title = {Robust {{Bayesian}} Meta-Analysis: {{Addressing}} Publication Bias with Model-Averaging},
  shorttitle = {Robust {{Bayesian}} Meta-Analysis},
  author = {Maier, Maximilian and Barto{\v s}, Franti{\v s}ek and Wagenmakers, Eric-Jan},
  year = {2023},
  journal = {Psychological Methods},
  volume = {28},
  number = {1},
  pages = {107--122},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000405},
  abstract = {Meta-analysis is an important quantitative tool for cumulative science, but its application is frustrated by publication bias. In order to test and adjust for publication bias, we extend model-averaged Bayesian meta-analysis with selection models. The resulting robust Bayesian meta-analysis (RoBMA) methodology does not require all-or-none decisions about the presence of publication bias, can quantify evidence in favor of the absence of publication bias, and performs well under high heterogeneity. By model-averaging over a set of 12 models, RoBMA is relatively robust to model misspecification and simulations show that it outperforms existing methods. We demonstrate that RoBMA finds evidence for the absence of publication bias in Registered Replication Reports and reliably avoids false positives. We provide an implementation in R so that researchers can easily use the new methodology in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Adjustment,Bayesian Analysis,Errors,Meta Analysis,Models,Publication Bias,Sciences,Simulation,Statistical Probability},
  file = {/Users/cristian/Zotero/storage/CGKS6NYY/Maier et al. - 2023 - Robust Bayesian meta-analysis Addressing publicat.pdf}
}

@article{maierUsingSelectionModels2022,
  title = {Using Selection Models to Assess Sensitivity to Publication Bias: {{A}} Tutorial and Call for More Routine Use},
  shorttitle = {Using Selection Models to Assess Sensitivity to Publication Bias},
  author = {Maier, Maximilian and VanderWeele, Tyler J. and Mathur, Maya B.},
  year = {2022},
  month = jul,
  journal = {Campbell Systematic Reviews},
  volume = {18},
  number = {3},
  pages = {e1256},
  issn = {1891-1803},
  doi = {10.1002/cl2.1256},
  urldate = {2025-05-06},
  abstract = {In meta-analyses, it is critical to assess the extent to which publication bias might have compromised the results. Classical methods based on the funnel plot, including Egger's test and Trim-and-Fill, have become the de facto default methods to do so, with a large majority of recent meta-analyses in top medical journals (85\%) assessing for publication bias exclusively using these methods. However, these classical funnel plot methods have important limitations when used as the sole means of assessing publication bias: they essentially assume that the publication process favors large point estimates for small studies and does not affect the largest studies, and they can perform poorly when effects are heterogeneous. In light of these limitations, we recommend that meta-analyses routinely apply other publication bias methods in addition to or instead of classical funnel plot methods. To this end, we describe how to use and interpret selection models. These methods make the often more realistic assumption that publication bias favors ``statistically significant'' results, and the methods also directly accommodate effect heterogeneity. Selection models have been established for decades in the statistics literature and are supported by user-friendly software, yet remain rarely reported in many disciplines. We use a previously published meta-analysis to demonstrate that selection models can yield insights that extend beyond those provided by funnel plot methods, suggesting the importance of establishing more comprehensive reporting practices for publication bias assessment.},
  pmcid = {PMC9247867},
  pmid = {36909879},
  file = {/Users/cristian/Zotero/storage/RCEFRGKT/Maier et al. - 2022 - Using selection models to assess sensitivity to pu.pdf}
}

@article{makelBothQuestionableOpen2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = nov,
  journal = {Educational Researcher},
  volume = {50},
  number = {8},
  pages = {493--504},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X211001356},
  urldate = {2023-03-02},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UI9R3FJ5/Makel et al. - 2021 - Both Questionable and Open Research Practices Are .pdf}
}

@article{makinTenCommonStatistical2019,
  title = {Ten Common Statistical Mistakes to Watch out for When Writing or Reviewing a Manuscript},
  author = {Makin, Tamar R and {Orban de Xivry}, Jean-Jacques},
  editor = {Rodgers, Peter and Parsons, Nick and Holmes, Nick},
  year = {2019},
  month = oct,
  journal = {eLife},
  volume = {8},
  pages = {e48175},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.48175},
  urldate = {2023-08-11},
  abstract = {Inspired by broader efforts to make the conclusions of scientific research more robust, we have compiled a list of some of the most common statistical mistakes that appear in the scientific literature. The mistakes have their origins in ineffective experimental designs, inappropriate analyses and/or flawed reasoning. We provide advice on how authors, reviewers and readers can identify and resolve these mistakes and, we hope, avoid them in the future.},
  keywords = {analysis,causality,null results,p-hacking,power,statistics},
  file = {/Users/cristian/Zotero/storage/9EE27GIK/Makin and Orban de Xivry - 2019 - Ten common statistical mistakes to watch out for w.pdf}
}

@article{malickiTransparencyConductingReporting2023,
  title = {Transparency in Conducting and Reporting Research: {{A}} Survey of Authors, Reviewers, and Editors across Scholarly Disciplines},
  shorttitle = {Transparency in Conducting and Reporting Research},
  author = {Mali{\v c}ki, Mario and Aalbersberg, IJsbrand Jan and Bouter, Lex and Mulligan, Adrian and ter Riet, Gerben},
  year = {2023},
  month = mar,
  journal = {PLOS ONE},
  volume = {18},
  number = {3},
  pages = {e0270054},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0270054},
  urldate = {2023-03-10},
  abstract = {Calls have been made for improving transparency in conducting and reporting research, improving work climates, and preventing detrimental research practices. To assess attitudes and practices regarding these topics, we sent a survey to authors, reviewers, and editors. We received 3,659 (4.9\%) responses out of 74,749 delivered emails. We found no significant differences between authors', reviewers', and editors' attitudes towards transparency in conducting and reporting research, or towards their perceptions of work climates. Undeserved authorship was perceived by all groups as the most prevalent detrimental research practice, while fabrication, falsification, plagiarism, and not citing prior relevant research, were seen as more prevalent by editors than authors or reviewers. Overall, 20\% of respondents admitted sacrificing the quality of their publications for quantity, and 14\% reported that funders interfered in their study design or reporting. While survey respondents came from 126 different countries, due to the survey's overall low response rate our results might not necessarily be generalizable. Nevertheless, results indicate that greater involvement of all stakeholders is needed to align actual practices with current recommendations.},
  langid = {english},
  keywords = {Medicine and health sciences,Peer review,Psychological attitudes,Regression analysis,Research reporting guidelines,Scientific misconduct,Survey research,Surveys},
  file = {/Users/cristian/Zotero/storage/UTFDZU6K/Maliki et al. - 2023 - Transparency in conducting and reporting research.pdf}
}

@article{marotoCabozantinibTreatmentSolid2022,
  title = {Cabozantinib for the Treatment of Solid Tumors: A Systematic Review},
  author = {Maroto, P. and Porta, C. and Capdevila, J. and Apolo, A.B. and Viteri, S. and {Rodriguez-Antona}, C. and Martin, L. and Castellano, D.},
  year = {2022},
  journal = {Therapeutic Advances in Medical Oncology},
  volume = {14},
  publisher = {SAGE Publications Inc.},
  doi = {10.1177/17588359221107112}
}

@article{martin-canteroFactorsInfluencingEfficacy2021,
  title = {Factors Influencing the Efficacy of Nutritional Interventions on Muscle Mass in Older Adults: {{A}} Systematic Review and Meta-Analysis},
  author = {{Martin-Cantero}, A. and Reijnierse, E.M. and Gill, B.M.T. and Maier, A.B.},
  year = {2021},
  journal = {Nutrition Reviews},
  volume = {79},
  number = {3},
  pages = {315--330},
  publisher = {Oxford University Press},
  doi = {10.1093/nutrit/nuaa064}
}

@article{marurFDAAnalysesSurvival2018,
  title = {{{FDA}} Analyses of Survival in Older Adults with Metastatic Non--Small Cell Lung Cancer in Controlled Trials of {{PD-1}}/{{PD-L1}} Blocking Antibodies},
  author = {Marur, S. and Singh, H. and {Mishra-Kalyani}, P. and Larkins, E. and Keegan, P. and Sridhara, R. and Blumenthal, G.M. and Pazdur, R.},
  year = {2018},
  journal = {Seminars in Oncology},
  volume = {45},
  number = {4},
  pages = {220--225},
  publisher = {W.B. Saunders},
  doi = {10.1053/j.seminoncol.2018.08.007}
}

@article{mathurMethodsAddressConfounding2022,
  title = {Methods to {{Address Confounding}} and {{Other Biases}} in {{Meta-Analyses}}: {{Review}} and {{Recommendations}}},
  shorttitle = {Methods to {{Address Confounding}} and {{Other Biases}} in {{Meta-Analyses}}},
  author = {Mathur, Maya B. and VanderWeele, Tyler J.},
  year = {2022},
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {43},
  number = {Volume 43, 2022},
  pages = {19--35},
  publisher = {Annual Reviews},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-051920-114020},
  urldate = {2025-06-12},
  abstract = {Meta-analyses contribute critically to cumulative science, but they can produce misleading conclusions if their constituent primary studies are biased, for example by unmeasured confounding in nonrandomized studies. We provide practical guidance on how meta-analysts can address confounding and other biases that affect studies' internal validity, focusing primarily on sensitivity analyses that help quantify how biased the meta-analysis estimates might be. We review a number of sensitivity analysis methods to do so, especially recent developments that are straightforward to implement and interpret and that use somewhat less stringent statistical assumptions than do earlier methods. We give recommendations for how these newer methods could be applied in practice and illustrate using a previously published meta-analysis. Sensitivity analyses can provide informative quantitative summaries of evidence strength, and we suggest reporting them routinely in meta-analyses of potentially biased studies. This recommendation in no way diminishes the importance of defining study eligibility criteria that reduce bias and of characterizing studies' risks of bias qualitatively.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/W6UHTWUW/Mathur and VanderWeele - 2022 - Methods to Address Confounding and Other Biases in.pdf;/Users/cristian/Zotero/storage/8WP6UJGQ/annurev-publhealth-051920-114020.html}
}

@article{matosinNegativityNegativeResults2014,
  title = {Negativity towards Negative Results: A Discussion of the Disconnect between Scientific Worth and Scientific Culture},
  shorttitle = {Negativity towards Negative Results},
  author = {Matosin, Natalie and Frank, Elisabeth and Engel, Martin and Lum, Jeremy S. and Newell, Kelly A.},
  year = {2014},
  month = feb,
  journal = {Disease Models \& Mechanisms},
  volume = {7},
  number = {2},
  pages = {171--173},
  issn = {1754-8411},
  doi = {10.1242/dmm.015123},
  abstract = {Science is often romanticised as a flawless system of knowledge building, where scientists work together to systematically find answers. In reality, this is not always the case. Dissemination of results are straightforward when the findings are positive, but what happens when you obtain results that support the null hypothesis, or do not fit with the current scientific thinking? In this Editorial, we discuss the issues surrounding publication bias and the difficulty in communicating negative results. Negative findings are a valuable component of the scientific literature because they force us to critically evaluate and validate our current thinking, and fundamentally move us towards unabridged science.},
  langid = {english},
  pmcid = {PMC3917235},
  pmid = {24713271},
  keywords = {Attitude,Humans,Negative findings,Negative results,Null hypothesis,Organizational Culture,Psychiatry,Research,Science},
  file = {/Users/cristian/Zotero/storage/5IVZSKTQ/Matosin et al. - 2014 - Negativity towards negative results a discussion .pdf}
}

@article{matsumuraErgogenicEffectsVery2023,
  title = {Ergogenic {{Effects}} of {{Very Low}} to {{Moderate Doses}} of {{Caffeine}} on {{Vertical Jump Performance}}},
  author = {Matsumura, Teppei and Takamura, Yuki and Fukuzawa, Kazushi and Nakagawa, Kazuya and Nonoyama, Shunya and Tomoo, Keigo and Tsukamoto, Hayato and Shinohara, Yasushi and Iemitsu, Motoyuki and Nagano, Akinori and Isaka, Tadao and Hashimoto, Takeshi},
  year = {2023},
  month = sep,
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {33},
  number = {5},
  pages = {275--281},
  issn = {1543-2742},
  doi = {10.1123/ijsnem.2023-0061},
  abstract = {Although the ergogenic effects of 3-6~mg/kg caffeine are widely accepted, the efficacy of low doses of caffeine has been discussed. However, it is unclear whether the ergogenic effects of caffeine on jump performance are dose responsive in a wide range of doses. This study aimed to examine the effect of very low (1~mg/kg) to moderate doses of caffeine, including commonly utilized ergogenic doses (i.e.,~3 and 6~mg/kg), on vertical jump performance. A total of 32 well-trained collegiate sprinters and jumpers performed countermovement jumps and squat jumps three times each in a double-blind, counterbalanced, randomized, crossover design. Participants ingested a placebo or 1, 3, or 6~mg/kg caffeine 60~min before jumping. Compared with the placebo, 6~mg/kg caffeine significantly enhanced countermovement jump (p {$<$} .001) and squat jump (p = .012) heights; furthermore, 1 and 3~mg/kg of caffeine also significantly increased countermovement jump height (1~mg/kg: p = .002, 3~mg/kg: p {$<$} .001) but not squat jump height (1~mg/kg: p = .436, 3~mg/kg: p = .054). There were no significant differences among all caffeine doses in both jumps (all p {$>$} .05). In conclusion, even at a dose as low as 1~mg/kg, caffeine improved vertical jump performance in a dose-independent manner. This study provides new insight into the applicability and feasibility of 1~mg/kg caffeine as a safe and effective ergogenic strategy for jump performance.},
  langid = {english},
  pmid = {37414404},
  keywords = {athlete,Athletic Performance,Caffeine,countermovement jump,Cross-Over Studies,Double-Blind Method,ergogenic aid,Humans,Performance-Enhancing Substances,squat jump}
}

@article{maughanIOCConsensusStatement2018,
  title = {{{IOC}} Consensus Statement: Dietary Supplements and the High-Performance Athlete},
  shorttitle = {{{IOC}} Consensus Statement},
  author = {Maughan, Ronald J. and Burke, Louise M. and Dvorak, Jiri and {Larson-Meyer}, D. Enette and Peeling, Peter and Phillips, Stuart M. and Rawson, Eric S. and Walsh, Neil P. and Garthe, Ina and Geyer, Hans and Meeusen, Romain and {van Loon}, Lucas J. C. and Shirreffs, Susan M. and Spriet, Lawrence L. and Stuart, Mark and Vernec, Alan and Currell, Kevin and Ali, Vidya M. and Budgett, Richard Gm and Ljungqvist, Arne and Mountjoy, Margo and Pitsiladis, Yannis P. and Soligard, Torbj{\o}rn and Erdener, U{\u g}ur and Engebretsen, Lars},
  year = {2018},
  month = apr,
  journal = {British Journal of Sports Medicine},
  volume = {52},
  number = {7},
  pages = {439--455},
  issn = {1473-0480},
  doi = {10.1136/bjsports-2018-099027},
  abstract = {Nutrition usually makes a small but potentially valuable contribution to successful performance in elite athletes, and dietary supplements can make a minor contribution to this nutrition programme. Nonetheless, supplement use is widespread at all levels of sport. Products described as supplements target different issues, including (1) the management of micronutrient deficiencies, (2) supply of convenient forms of energy and macronutrients, and (3) provision of direct benefits to performance or (4) indirect benefits such as supporting intense training regimens. The appropriate use of some supplements can benefit the athlete, but others may harm the athlete's health, performance, and/or livelihood and reputation (if an antidoping rule violation results). A complete nutritional assessment should be undertaken before decisions regarding supplement use are made. Supplements claiming to directly or indirectly enhance performance are typically the largest group of products marketed to athletes, but only a few (including caffeine, creatine, specific buffering agents and nitrate) have good evidence of benefits. However, responses are affected by the scenario of use and may vary widely between individuals because of factors that include genetics, the microbiome and habitual diet. Supplements intended to enhance performance should be thoroughly trialled in training or simulated competition before being used in competition. Inadvertent ingestion of substances prohibited under the antidoping codes that govern elite sport is a known risk of taking some supplements. Protection of the athlete's health and awareness of the potential for harm must be paramount; expert professional opinion and assistance is strongly advised before an athlete embarks on supplement use.},
  langid = {english},
  pmcid = {PMC5867441},
  pmid = {29540367},
  keywords = {Athletes,Athletic Performance,Consensus,diet,Diet,Dietary Supplements,Humans,performance,Sports Nutritional Physiological Phenomena},
  file = {/Users/cristian/Zotero/storage/JA76K795/Maughan et al. - 2018 - IOC consensus statement dietary supplements and t.pdf}
}

@article{maxwellPersistenceUnderpoweredStudies2004,
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}},
  author = {Maxwell, Scott E.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {147--163},
  doi = {10.1037/1082-989X.9.2.147},
  abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The "curse of multiplicities" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Consequence,Effect Size (Statistical),Hypothesis Testing,Methodology,Psychology,Statistical Power,Type I Errors},
  file = {/Users/cristian/Zotero/storage/ZIE2UQD5/Maxwell - 2004 - The Persistence of Underpowered Studies in Psychol.pdf;/Users/cristian/Zotero/storage/P35K8BIW/2004-14114-001.html}
}

@article{maxwellPsychologySufferingReplication2015,
  title = {Is Psychology Suffering from a Replication Crisis? {{What}} Does "Failure to Replicate" Really Mean?},
  shorttitle = {Is Psychology Suffering from a Replication Crisis?},
  author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
  year = {2015},
  month = sep,
  journal = {The American Psychologist},
  volume = {70},
  number = {6},
  pages = {487--498},
  issn = {1935-990X},
  doi = {10.1037/a0039400},
  abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and meta-analysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology's alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.},
  langid = {english},
  pmid = {26348332},
  keywords = {Bayes Theorem,Humans,Psychology,Reproducibility of Results,Research Design,Sample Size}
}

@article{maxwellSampleSizePlanning2008,
  title = {Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  journal = {Annual Review of Psychology},
  volume = {59},
  pages = {537--563},
  doi = {10.1146/annurev.psych.59.103006.093735},
  abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
  langid = {english},
  keywords = {{Models, Psychological},Confidence Intervals,Humans,Linear Models,Psychology,Sampling Studies},
  file = {/Users/cristian/Zotero/storage/DUJZ3ZXX/Maxwell et al. - 2008 - Sample size planning for statistical power and acc.pdf;/Users/cristian/Zotero/storage/YRVUXT89/annurev.psych.59.103006.html}
}

@article{mayoSevereTestingBasic2006,
  title = {Severe {{Testing}} as a {{Basic Concept}} in a {{Neyman-Pearson Philosophy}} of {{Induction}}},
  author = {Mayo, Deborah G. and Spanos, Aris},
  year = {2006},
  journal = {The British Journal for the Philosophy of Science},
  volume = {57},
  number = {2},
  eprint = {3873470},
  eprinttype = {jstor},
  pages = {323--357},
  publisher = {[Oxford University Press, The British Society for the Philosophy of Science]},
  issn = {0007-0882},
  urldate = {2022-03-30},
  abstract = {Despite the widespread use of key concepts of the Neyman-Pearson (N-P) statistical paradigm-type I and II errors, significance levels, power, confidence levels-they have been the subject of philosophical controversy and debate for over 60 years. Both current and long-standing problems of N-P tests stem from unclarity and confusion, even among N-P adherents, as to how a test's (pre-data) error probabilities are to be used for (post-data) inductive inference as opposed to inductive behavior. We argue that the relevance of error probabilities is to ensure that only statistical hypotheses that have passed severe or probative tests are inferred from the data. The severity criterion supplies a meta-statistical principle for evaluating proposed statistical inferences, avoiding classic fallacies from tests that are overly sensitive, as well as those not sensitive enough to particular errors and discrepancies.}
}

@article{mayoSevereTestsArguing1997,
  title = {Severe {{Tests}}, {{Arguing}} from {{Error}}, and {{Methodological Underdetermination}}},
  author = {Mayo, Deborah G.},
  year = {1997},
  journal = {Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition},
  volume = {86},
  number = {3},
  eprint = {4320757},
  eprinttype = {jstor},
  pages = {243--266},
  publisher = {Springer},
  issn = {0031-8116},
  urldate = {2023-09-07}
}

@article{mazzolariMythsMethodologiesUse2022,
  title = {Myths and Methodologies: {{The}} Use of Equivalence and Non-Inferiority Tests for Interventional Studies in Exercise Physiology and Sport Science},
  shorttitle = {Myths and Methodologies},
  author = {Mazzolari, Raffaele and Porcelli, Simone and Bishop, David J. and Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Experimental Physiology},
  volume = {107},
  number = {3},
  pages = {201--212},
  issn = {1469-445X},
  doi = {10.1113/EP090171},
  abstract = {Exercise physiology and sport science have traditionally made use of the null hypothesis of no difference to make decisions about experimental interventions. In this article, we aim to review current statistical approaches typically used by exercise physiologists and sport scientists for the design and analysis of experimental interventions and to highlight the importance of including equivalence and non-inferiority studies, which address different research questions from deciding whether an effect is present. Initially, we briefly describe the most common approaches, along with their rationale, to investigate the effects of different interventions. We then discuss the main steps involved in the design and analysis of equivalence and non-inferiority studies, commonly performed in other research fields, with worked examples from exercise physiology and sport science scenarios. Finally, we provide recommendations to exercise physiologists and sport scientists who would like to apply the different approaches in future research. We hope this work will promote the correct use of equivalence and non-inferiority designs in exercise physiology and sport science whenever the research context, conditions, applications, researchers' interests or reasonable beliefs justify these approaches.},
  langid = {english},
  pmid = {35041233},
  keywords = {Exercise,Humans,intervention efficacy,methodology,Research Design,Sports,statistical review}
}

@article{mccarthyAcuteKetoneMonoester2023,
  title = {Acute {{Ketone Monoester Supplementation Impairs}} 20-Min {{Time-Trial Performance}} in {{Trained Cyclists}}: {{A Randomized}}, {{Crossover Trial}}},
  shorttitle = {Acute {{Ketone Monoester Supplementation Impairs}} 20-Min {{Time-Trial Performance}} in {{Trained Cyclists}}},
  author = {McCarthy, Devin G. and Bone, Jack and Fong, Matthew and Pinckaers, Phillippe J. M. and Bostad, William and Richards, Douglas L. and van Loon, Luc J. C. and Gibala, Martin J.},
  year = {2023},
  month = apr,
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {33},
  number = {4},
  pages = {181--188},
  publisher = {Human Kinetics},
  issn = {1543-2742, 1526-484X},
  doi = {10.1123/ijsnem.2022-0255},
  urldate = {2023-07-20},
  abstract = {Acute ketone monoester (KE) supplementation can alter exercise responses, but the performance effect is unclear. The limited and equivocal data to date are likely related to factors including the KE dose, test conditions, and caliber of athletes studied. We tested the hypothesis that mean power output during a 20-min cycling time trial (TT) would be different after KE ingestion compared to a placebo (PL). A sample size of 22 was estimated to provide 80\% power to detect an effect size dz of 0.63 at an alpha level of .05 with a two-tailed paired t test. This determination considered 2.0\% as the minimal important difference in performance. Twenty-three trained cyclists (N\,=\,23; peak oxygen uptake: 65\,{\textpm}\,12 ml{$\cdot$}kg-1 min-1; M\,{\textpm}\,SD), who were regularly cycling {$>$}5 hr/week, completed a familiarization trial followed by two experimental trials. Participants self-selected and replicated their diet and exercise for {$\sim$}24 hr before each trial. Participants ingested either 0.35 g/kg body mass of (R)-3-hydroxybutyl (R)-3-hydroxybutyrate KE or a flavor-matched PL 30 min before exercise in a randomized, triple-blind, crossover manner. Exercise involved a 15-min warm-up followed by the 20-min TT on a cycle ergometer. The only feedback provided was time elapsed. Preexercise venous [{$\beta$}-hydroxybutyrate] was higher after KE versus PL (2.0\,{\textpm}\,0.6 vs. 0.2\,{\textpm}\,0.1 mM, p\,{$<$}\,.0001). Mean TT power output was 2.4\% (0.6\% to 4.1\%; mean [95\% confidence interval]) lower after KE versus PL (255\,{\textpm}\,54 vs. 261\,{\textpm}\,54 W, p\,{$<$}\,.01; dz\,=\,0.60). The mechanistic basis for the impaired TT performance after KE ingestion under the present study conditions remains to be determined.},
  chapter = {International Journal of Sport Nutrition and Exercise Metabolism},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/RNFP5TU7/McCarthy et al. - 2023 - Acute Ketone Monoester Supplementation Impairs 20-.pdf}
}

@article{mckayCombinationReportingBias2023,
  ids = {mckayCombinationReportingBias},
  title = {The Combination of Reporting Bias and Underpowered Study Designs Has Substantially Exaggerated the Motor Learning Benefits of Self-Controlled Practice and Enhanced Expectancies: A Meta-Analysis},
  shorttitle = {The Combination of Reporting Bias and Underpowered Study Designs Has Substantially Exaggerated the Motor Learning Benefits of Self-Controlled Practice and Enhanced Expectancies},
  author = {McKay, Brad and Bacelar, Mariane F. B. and Parma, Juliana O. and Miller, Matthew W. and Carter, Michael J.},
  year = {2023},
  month = may,
  journal = {International Review of Sport and Exercise Psychology},
  pages = {1--21},
  publisher = {Routledge},
  issn = {1750-984X, 1750-9858},
  doi = {10.1080/1750984X.2023.2207255},
  urldate = {2025-03-05},
  langid = {english},
  keywords = {motivation,OPTIMAL theory,publication bias,roBMA,Z-curve},
  file = {/Users/cristian/Zotero/storage/HQ34GEN4/McKay et al. - 2023 - The combination of reporting bias and underpowered.pdf}
}

@article{mckayDefiningTrainingPerformance2022,
  title = {Defining {{Training}} and {{Performance Caliber}}: {{A Participant Classification Framework}}},
  shorttitle = {Defining {{Training}} and {{Performance Caliber}}},
  author = {McKay, Alannah K. A. and Stellingwerff, Trent and Smith, Ella S. and Martin, David T. and Mujika, I{\~n}igo and {Goosey-Tolfrey}, Vicky L. and Sheppard, Jeremy and Burke, Louise M.},
  year = {2022},
  month = feb,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {17},
  number = {2},
  pages = {317--331},
  issn = {1555-0273},
  doi = {10.1123/ijspp.2021-0451},
  abstract = {Throughout the sport-science and sports-medicine literature, the term "elite" subjects might be one of the most overused and ill-defined terms. Currently, there is no common perspective or terminology to characterize the caliber and training status of an individual or cohort. This paper presents a 6-tiered Participant Classification Framework whereby all individuals across a spectrum of exercise backgrounds and athletic abilities can be classified. The Participant Classification Framework uses training volume and performance metrics to classify a participant to one of the following: Tier 0: Sedentary; Tier 1: Recreationally Active; Tier 2: Trained/Developmental; Tier 3: Highly Trained/National Level; Tier 4: Elite/International Level; or Tier 5: World Class. We suggest the Participant Classification Framework can be used to classify participants both prospectively (as part of study participant recruitment) and retrospectively (during systematic reviews and/or meta-analyses). Discussion around how the Participant Classification Framework can be tailored toward different sports, athletes, and/or events has occurred, and sport-specific examples provided. Additional nuances such as depth of sport participation, nationality differences, and gender parity within a sport are all discussed. Finally, chronological age with reference to the junior and masters athlete, as well as the Paralympic athlete, and their inclusion within the Participant Classification Framework has also been considered. It is our intention that this framework be widely implemented to systematically classify participants in research featuring exercise, sport, performance, health, and/or fitness outcomes going forward, providing the much-needed uniformity to classification practices.},
  langid = {english},
  pmid = {34965513},
  keywords = {athlete,Athletes,Athletic Performance,classification system,elite,exercise,Exercise,Humans,recreationally active,Retrospective Studies,Sports,Sports Medicine,terminology},
  file = {/Users/cristian/Zotero/storage/UVPBFVYT/McKay et al. - 2022 - Defining Training and Performance Caliber A Parti.pdf}
}

@article{mckayMetaanalysisReducedRelative2022,
  title = {Meta-Analysis of the Reduced Relative Feedback Frequency Effect on Motor Learning and Performance},
  author = {McKay, Brad and Hussien, Julia and Vinh, Mary-Anne and {Mir-Orefice}, Alexandre and Brooks, Hugh and {Ste-Marie}, Diane M.},
  year = {2022},
  journal = {Psychology of Sport and Exercise},
  volume = {61},
  pages = {102165},
  publisher = {Elsevier},
  urldate = {2025-03-05},
  file = {/Users/cristian/Zotero/storage/ZXTMN9DE/S1469029222000334.html}
}

@article{mckayMetaanalyticFindingsSelfcontrolled2022,
  title = {Meta-Analytic Findings of the Self-Controlled Motor Learning Literature: {{Underpowered}}, Biased, and Lacking Evidential Value},
  shorttitle = {Meta-Analytic Findings of the Self-Controlled Motor Learning Literature},
  author = {McKay, Brad and Yantha, Zachary and Hussien, Julia and Carter, Michael and {Ste-Marie}, Diane},
  year = {2022},
  journal = {Meta-Psychology},
  volume = {6},
  urldate = {2025-03-05},
  file = {/Users/cristian/Zotero/storage/DPYZP2ZU/McKay et al. - 2022 - Meta-analytic findings of the self-controlled moto.pdf}
}

@article{mckayReportingBiasNot2024,
  title = {Reporting Bias, Not External Focus: {{A}} Robust {{Bayesian}} Meta-Analysis and Systematic Review of the External Focus of Attention Literature},
  shorttitle = {Reporting Bias, Not External Focus},
  author = {McKay, Brad and Corson, Abbey E. and Seedu, Jeswende and De Faveri, Celeste S. and Hasan, Hibaa and Arnold, Kristen and Adams, Faith C. and Carter, Michael J.},
  year = {2024},
  journal = {Psychological Bulletin},
  volume = {150},
  number = {11},
  pages = {1347--1362},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/bul0000451},
  abstract = {Evidence has ostensibly been accumulating over the past 2 decades suggesting that an external focus on the intended movement effect (e.g., on the golf club during a swing) is superior to an internal focus on body movements (e.g., on your arms during a swing) for skill acquisition. Seven previous meta-studies have all reported evidence of external focus superiority. The most comprehensive of these concluded that an external focus enhances motor skill retention, transfer, and performance and leads to reduced eletromyographic activity during performance and that more distal external foci are superior to proximal external foci for performance. Here, we reanalyzed these data using robust Bayesian meta-analyses that included several plausible models of publication bias. We found moderate to strong evidence of publication bias for all analyses. After correcting for publication bias, estimated mean effects were negligible: g = 0.01 (performance), g = 0.15 (retention), g = 0.09 (transfer), g = 0.06 (electromyography), and g = -0.01 (distance effect). Bayes factors indicated data favored the null for each analysis, ranging from BF01 = 1.3 (retention) to 5.75 (performance). We found clear evidence of heterogeneity in each analysis, suggesting the impact of attentional focus depends on yet unknown contextual factors. Our results contradict the existing consensus that an external focus is always more effective than an internal focus. Instead, focus of attention appears to have a variety of effects that we cannot account for, and, on average, those effects are small to nil. These results parallel previous metascience suggesting publication bias has obfuscated the motor learning literature. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Focused Attention,Motor Development,Publication Bias,Skill Learning},
  file = {/Users/cristian/Zotero/storage/7SKBMHLT/2025-40542-004.html}
}

@article{mckiernanUseJournalImpact2019,
  title = {Use of the {{Journal Impact Factor}} in Academic Review, Promotion, and Tenure Evaluations},
  author = {McKiernan, Erin C and Schimanski, Lesley A and Mu{\~n}oz Nieves, Carol and Matthias, Lisa and Niles, Meredith T and Alperin, Juan P},
  editor = {Pewsey, Emma and Rodgers, Peter and Brembs, Bj{\"o}rn},
  year = {2019},
  month = jul,
  journal = {eLife},
  volume = {8},
  pages = {e47338},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.47338},
  urldate = {2021-09-01},
  abstract = {We analyzed how often and in what ways the Journal Impact Factor (JIF) is currently used in review, promotion, and tenure (RPT) documents of a representative sample of universities from the United States and Canada. 40\% of research-intensive institutions and 18\% of master's institutions mentioned the JIF, or closely related terms. Of the institutions that mentioned the JIF, 87\% supported its use in at least one of their RPT documents, 13\% expressed caution about its use, and none heavily criticized it or prohibited its use. Furthermore, 63\% of institutions that mentioned the JIF associated the metric with quality, 40\% with impact, importance, or significance, and 20\% with prestige, reputation, or status. We conclude that use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and that there is work to be done to avoid the potential misuse of metrics like the JIF.},
  keywords = {academic careers,higher education,impact factor,institutional policy,scholarly communications},
  file = {/Users/cristian/Zotero/storage/4JYMWWPI/McKiernan et al. - 2019 - Use of the Journal Impact Factor in academic revie.pdf}
}

@article{mcmorrisCognitiveFatigueEffects2018,
  title = {Cognitive Fatigue Effects on Physical Performance: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Cognitive Fatigue Effects on Physical Performance},
  author = {McMorris, Terry and Barwood, Martin and Hale, Beverley J. and Dicks, Matt and Corbett, Jo},
  year = {2018},
  month = may,
  journal = {Physiology \& Behavior},
  volume = {188},
  pages = {103--107},
  issn = {0031-9384},
  doi = {10.1016/j.physbeh.2018.01.029},
  urldate = {2022-10-03},
  abstract = {Recent research has examined the effect that undertaking a cognitively fatiguing task for {$\leq$}90\,min has on subsequent physical performance. Cognitive fatigue is claimed to affect subsequent physical performance by inducing energy depletion in the brain, depletion of brain catecholamine neurotransmitters or changes in motivation. Observation of the psychophysiology and neurochemistry literature questions the ability of 90\,min' cognitive activity to deplete energy or catecholamine resources. The purpose of this study, therefore, was to examine the evidence for cognitive fatigue having an effect on subsequent physical performance. A systematic, meta-analytic review was undertaken. We found a small but significant pooled effect size based on comparison between physical performance post-cognitive fatigue compared to post-control (g\,=\,-0.27, SE\,=\,-0.12, 95\% CI -0.49 to -0.04, Z(10)\,=\,-2.283, p\,{$<$}\,0.05). However, the results were not heterogenous (Q(10)\,=\,2.789, p\,{$>$}\,0.10, {$T$}2\,{$<$}\,0.001), suggesting that the pooled effect size does not amount to a real effect and differences are due to random error. No publication bias was evident (Kendall's {$\tau$}\,=\,-0.07, p\,{$>$}\,0.05). Thus, the results are somewhat contradictory. The pooled effect size shows a small but significant negative effect of cognitive fatigue, however tests of heterogeneity show that the results are due to random error. Future research should use neuroscientific tests to ensure that cognitive fatigue has been achieved.},
  langid = {english},
  keywords = {Central executive,Central fatigue,Motivation},
  file = {/Users/cristian/Zotero/storage/J8GS6Z36/McMorris et al. - 2018 - Cognitive fatigue effects on physical performance.pdf;/Users/cristian/Zotero/storage/ISCUPIX8/S0031938418300441.html}
}

@article{mcshaneAdjustingPublicationBias2016,
  title = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}: {{An Evaluation}} of {{Selection Methods}} and {{Some Cautionary Notes}}},
  shorttitle = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}},
  author = {McShane, Blakeley B. and B{\"o}ckenholt, Ulf and Hansen, Karsten T.},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {730--749},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691616662243},
  urldate = {2023-01-24},
  abstract = {We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis?that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.},
  file = {/Users/cristian/Zotero/storage/LCGRFIVA/McShane et al. - 2016 - Adjusting for Publication Bias in Meta-Analysis A.pdf}
}

@article{mcshaneLargeScaleReplicationProjects2019,
  title = {Large-{{Scale Replication Projects}} in {{Contemporary Psychological Research}}},
  author = {McShane, Blakeley B. and Tackett, Jennifer L. and B{\"o}ckenholt, Ulf and Gelman, Andrew},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {99--105},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1505655},
  urldate = {2023-01-27},
  abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next---an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous---even if the sign of the effect is consistent---the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
  keywords = {Between-study variation,Heterogeneity,Hierarchical,Meta-Analysis,Multilevel,Null hypothesis significance testing,p-value,Psychology,Replication},
  file = {/Users/cristian/Zotero/storage/QKNRAR46/McShane et al. - 2019 - Large-Scale Replication Projects in Contemporary P.pdf}
}

@article{mcshaneYouCannotStep2014,
  title = {You {{Cannot Step Into}} the {{Same River Twice}}: {{When Power Analyses Are Optimistic}}},
  shorttitle = {You {{Cannot Step Into}} the {{Same River Twice}}},
  author = {McShane, Blakeley B. and B{\"o}ckenholt, Ulf},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {9},
  number = {6},
  pages = {612--625},
  issn = {1745-6924},
  doi = {10.1177/1745691614548513},
  abstract = {Statistical power depends on the size of the effect of interest. However, effect sizes are rarely fixed in psychological research: Study design choices, such as the operationalization of the dependent variable or the treatment manipulation, the social context, the subject pool, or the time of day, typically cause systematic variation in the effect size. Ignoring this between-study variation, as standard power formulae do, results in assessments of power that are too optimistic. Consequently, when researchers attempting replication set sample sizes using these formulae, their studies will be underpowered and will thus fail at a greater than expected rate. We illustrate this with both hypothetical examples and data on several well-studied phenomena in psychology. We provide formulae that account for between-study variation and suggest that researchers set sample sizes with respect to our generally more conservative formulae. Our formulae generalize to settings in which there are multiple effects of interest. We also introduce an easy-to-use website that implements our approach to setting sample sizes. Finally, we conclude with recommendations for quantifying between-study variation.},
  langid = {english},
  pmid = {26186112},
  keywords = {between-study variation,effect size,heterogeneity,Humans,Internet,power,Psychology,sample size,Sample Size,statistical significance,Statistics as Topic}
}

@article{meehlAppraisingAmendingTheories1990,
  title = {Appraising and {{Amending Theories}}: {{The Strategy}} of {{Lakatosian Defense}} and {{Two Principles That Warrant It}}},
  shorttitle = {Appraising and {{Amending Theories}}},
  author = {Meehl, Paul E.},
  year = {1990},
  journal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  eprint = {1448768},
  eprinttype = {jstor},
  pages = {108--141},
  publisher = {Taylor \& Francis, Ltd.},
  issn = {1047-840X},
  urldate = {2023-07-24},
  abstract = {In social science, everything is somewhat correlated with everything ("crud factor"), so whether H\textsubscript{0} is refuted depends solely on statistical power. In psychology, the directional counternull of interest, H*, is not equivalent to the substantive theory T, there being many plausible alternative explanations of a mere directional trend (weak use of significance tests). Testing against a predicted point value (the strong use of significant tests) can discorroborate T by refuting H*. If used thus to abandon T forthwith, it is too strong, not allowing for theoretical verisimilitude as distinguished from truth. Defense and amendment of an apparently falsified T are appropriate strategies only when T has accumulated a good track record ("money in the bank") by making successful or near-miss predictions of low prior probability (Salmon's "damn strange coincidences"). Two rough indexes are proposed for numerifying the track record, by considering jointly how intolerant (risky) and how close (accurate) are its predictions.}
}

@article{meehlTheoreticalRisksTabular1978,
  title = {Theoretical Risks and Tabular Asterisks: {{Sir Karl}}, {{Sir Ronald}}, and the Slow Progress of Soft Psychology},
  shorttitle = {Theoretical Risks and Tabular Asterisks},
  author = {Meehl, Paul E.},
  year = {1978},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {46},
  number = {4},
  pages = {806--834},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2117},
  doi = {10.1037/0022-006X.46.4.806},
  abstract = {Theories in "soft" areas of psychology (e.g., clinical, counseling, social, personality, school, and community) lack the cumulative character of scientific knowledge because they tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest. Even though intrinsic subject matter difficulties (20 are listed) contribute to this, the excessive reliance on significance testing is partly responsible (Ronald A. Fisher). Karl Popper's approach, with modifications, would be prophylactic. Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of "significant differences" are little more than complex, causally uninterpretable outcomes of statistical power functions. Multiple paths to estimating numerical point values ("consistency tests") are better, even if approximate with rough tolerances; and lacking this, ranges, orderings, 2nd-order differences, curve peaks and valleys, and function forms should be used. Such methods are usual in developed sciences that seldom report statistical significance. Consistency tests of a conjectural taxometric model yielded 94\% success with no false negatives. (3 p ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Clinical Psychology,Community Psychology,Counseling Psychology,Experimental Design,Methodology,Personality,School Psychology,Social Psychology,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/J6MCF7TS/1979-25042-001.html}
}

@article{meehlTheoryTestingPsychologyPhysics1967,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  year = {1967},
  month = jun,
  journal = {Philosophy of Science},
  volume = {34},
  number = {2},
  pages = {103--115},
  publisher = {The University of Chicago Press},
  issn = {0031-8248},
  doi = {10.1086/288135},
  urldate = {2022-03-31},
  abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation.}
}

@article{meehlWhySummariesResearch1990,
  title = {Why {{Summaries}} of {{Research}} on {{Psychological Theories}} Are {{Often Uninterpretable}}},
  author = {Meehl, Paul E.},
  year = {1990},
  month = feb,
  journal = {Psychological Reports},
  volume = {66},
  number = {1},
  pages = {195--244},
  publisher = {SAGE Publications Inc},
  issn = {0033-2941},
  doi = {10.2466/pr0.1990.66.1.195},
  urldate = {2023-09-05},
  abstract = {Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/KL4BGQMM/Meehl - 1990 - Why Summaries of Research on Psychological Theorie.pdf}
}

@article{mehlerAppreciatingSignificanceNonsignificant2019,
  title = {Appreciating the {{Significance}} of {{Non-significant Findings}} in {{Psychology}}},
  author = {Mehler, David and Edelsbrunner, Peter and Mati{\'c}, Karla},
  year = {2019},
  month = jul,
  journal = {Journal of European Psychology Students},
  volume = {10},
  number = {4},
  pages = {1--7},
  publisher = {EFPSA},
  issn = {2222-6931},
  doi = {10.5334/e2019a},
  urldate = {2021-02-08},
  abstract = {The Journal of European Psychology Students (JEPS) is an open-access, double-blind, peer-reviewed journal for psychology students worldwide. JEPS is run by highly motivated European psychology students and has been publishing since 2009. By ensuring that authors are always provided with extensive feedback, JEPS gives psychology students the chance to gain experience in publishing and to improve their scientific skills. Furthermore, JEPS provides students with the opportunity to share their research and to take a first step toward a scientific career.Submissions are welcomed at any time.Register to submit your work online.Visit our blog - JEPS Bulletin - for tips on writing and more.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or  of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/FCFMJUEG/Mehler et al. - 2019 - Appreciating the Significance of Non-significant F.pdf;/Users/cristian/Zotero/storage/8A3HI9FI/e2019a.html}
}

@article{meiIntegrationNonrandomizedStudies2024,
  title = {Integration of Non-Randomized Studies with Randomized Controlled Trials in Meta-Analyses of Clinical Studies: A Meta-Epidemiological Study on Effect Estimation of Interventions},
  shorttitle = {Integration of Non-Randomized Studies with Randomized Controlled Trials in Meta-Analyses of Clinical Studies},
  author = {Mei, Fan and Yao, Minghong and Wang, Yuning and Huan, Jiayidaer and Ma, Yu and Li, Guowei and Zou, Kang and Li, Ling and Sun, Xin},
  year = {2024},
  month = dec,
  journal = {BMC Medicine},
  volume = {22},
  number = {1},
  pages = {571},
  issn = {1741-7015},
  doi = {10.1186/s12916-024-03778-1},
  urldate = {2025-06-19},
  abstract = {Syntheses of non-randomized studies of interventions (NRSIs) and randomized controlled trials (RCTs) are increasingly used in decision-making. This study aimed to summarize when NRSIs are included in evidence syntheses of RCTs, with a particular focus on the methodological issues associated with combining NRSIs and RCTs.},
  langid = {english},
  keywords = {Clinical Trial Design,Expertise Studies,Meta-analysis,Meta-epidemiology,Mixed Methods,Non-randomized studies of interventions,Outcomes research,Randomized Controlled Clinical Trials,Randomized controlled trials,Systematic review,Translational Research},
  file = {/Users/cristian/Zotero/storage/JNG5I434/Mei et al. - 2024 - Integration of non-randomized studies with randomi.pdf}
}

@article{meletiSalivaryBiomarkersDiagnosis2020,
  title = {Salivary Biomarkers for Diagnosis of Systemic Diseases and Malignant Tumors. {{A}} Systematic Review},
  author = {Meleti, M. and Cassi, D. and Vescovi, P. and Setti, G. and Pertinhez, T.A. and Pezzi, M.E.},
  year = {2020},
  journal = {Medicina Oral Patologia Oral y Cirugia Bucal},
  volume = {25},
  number = {2},
  pages = {e299-e310},
  publisher = {Medicina Oral S.L.},
  doi = {10.4317/medoral.23355}
}

@article{mentisHypotheticoDeductiveInductiveApproaches1988,
  title = {Hypothetico-{{Deductive}} and {{Inductive Approaches}} in {{Ecology}}},
  author = {Mentis, M. T.},
  year = {1988},
  journal = {Functional Ecology},
  volume = {2},
  number = {1},
  eprint = {2389454},
  eprinttype = {jstor},
  pages = {5--14},
  publisher = {[British Ecological Society, Wiley]},
  issn = {0269-8463},
  doi = {10.2307/2389454},
  urldate = {2021-03-11},
  file = {/Users/cristian/Zotero/storage/Z3DVDLJR/Mentis - 1988 - Hypothetico-Deductive and Inductive Approaches in .pdf}
}

@book{mertonSociologyScienceTheoretical1973,
  title = {The {{Sociology}} of {{Science}}: {{Theoretical}} and {{Empirical Investigations}}},
  shorttitle = {The {{Sociology}} of {{Science}}},
  author = {Merton, Robert K.},
  year = {1973},
  publisher = {University of Chicago Press},
  abstract = {"The exploration of the social conditions that facilitate or retard the search for scientific knowledge has been the major theme of Robert K. Merton's work for forty years. This collection of papers [is] a fascinating overview of this sustained inquiry. . . . There are very few other books in sociology . . . with such meticulous scholarship, or so elegant a style. This collection of papers is, and is likely to remain for a long time, one of the most important books in sociology."---Joseph Ben-David, New York Times Book Review  "The novelty of the approach, the erudition and elegance, and the unusual breadth of vision make this volume one of the most important contributions to sociology in general and to the sociology of science in particular. . . . Merton's Sociology of Science is a magisterial summary of the field."---Yehuda Elkana, American Journal of Sociology  "Merton's work provides a rich feast for any scientist concerned for a genuine understanding of his own professional self. And Merton's industry, integrity, and humility are permanent witnesses to that ethos which he has done so much to define and support."---J. R. Ravetz, American Scientist  "The essays not only exhibit a diverse and penetrating analysis and a deal of historical and contemporary examples, with concrete numerical data, but also make genuinely good reading because of the wit, the liveliness and the rich learning with which Merton writes."---Philip Morrison, Scientific American  "Merton's impact on sociology as a whole has been large, and his impact on the sociology of science has been so momentous that the title of the book is apt, because Merton's writings represent modern sociology of science more than any other single writer."---Richard McClintock, Contemporary Sociology},
  googlebooks = {zPvcHuUMEMwC},
  isbn = {978-0-226-52092-6},
  langid = {english},
  keywords = {Social Science / General,Social Science / Sociology / General}
}

@article{mesquidaEffectLargeEnough2024,
  title = {Is the Effect Large Enough to Matter? {{Why}} Exercise Physiologists Should Interpret Effect Sizes Meaningfully: A Reply to {{Williams}} et~al. (2023)},
  shorttitle = {Is the Effect Large Enough to Matter?},
  author = {Mesquida, Cristian and Lakens, Dani{\"e}l},
  year = {2024},
  month = jan,
  journal = {The Journal of Physiology},
  volume = {602},
  number = {1},
  pages = {241--242},
  issn = {1469-7793},
  doi = {10.1113/JP285901},
  langid = {english},
  pmid = {37983637},
  keywords = {{Data Interpretation, Statistical},effect size,Exercise,interpretation,null-hypothesis significance testing,smallest effect size of interest}
}

@misc{mesquidaPrevalenceReportingPractices2025,
  title = {The Prevalence, Reporting Practices, and Methodological Quality of a Priori Power Analyses in Sports and Exercise Science Research},
  author = {Mesquida, Cristian and Murphy, Jennifer and Warne, Joe and Lakens, Dani{\"e}l},
  year = {2025},
  month = jul,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.575},
  urldate = {2025-07-03},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {reproducibility,sample size,statistical power},
  file = {/Users/cristian/Zotero/storage/N9K58KDV/Mesquida et al. - 2025 - The prevalence, reporting practices, and methodolo.pdf}
}

@article{mesquidaPublicationBiasStatistical2023,
  title = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}: Potential Barriers to Replicability},
  shorttitle = {Publication Bias, Statistical Power and Reporting Practices in the {{Journal}} of {{Sports Sciences}}},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = {2023},
  month = sep,
  journal = {Journal of Sports Sciences},
  volume = {41},
  number = {16},
  pages = {1507--1517},
  issn = {1466-447X},
  doi = {10.1080/02640414.2023.2269357},
  abstract = {Two factors that decrease the replicability of studies in the scientific literature are publication bias and studies with underpowered desgins. One way to ensure that studies have adequate statistical power to detect the effect size of interest is by conducting a-priori power analyses. Yet, a previous editorial published in the Journal of Sports Sciences reported a median sample size of 19 and the scarce usage of a-priori power analyses. We meta-analysed 89 studies from the same journal to assess the presence and extent of publication bias, as well as the average statistical power, by conducting a z-curve analysis. In a larger sample of 174 studies, we also examined a) the usage, reporting practices and reproducibility of a-priori power analyses; and b) the prevalence of reporting practices of t-statistic or F-ratio, degrees of freedom, exact p-values, effect sizes and confidence intervals. Our results indicate that there was some indication of publication bias and the average observed power was low (53\% for significant and non-significant findings and 61\% for only significant findings). Finally, the usage and reporting practices of a-priori power analyses as well as statistical results including test statistics, effect sizes and confidence intervals were suboptimal.},
  langid = {english},
  pmid = {38018365},
  keywords = {Bias,Humans,publication bias,Publication Bias,Replicability,reporting practices,reproducibility,Reproducibility of Results,Research Design,Sample Size,statistical power},
  file = {/Users/cristian/Zotero/storage/WNA9JV57/Mesquida et al. - 2023 - Publication bias, statistical power and reporting .pdf}
}

@misc{mesquidaReplicabilitySportsExercise2025,
  title = {On the Replicability of Sports and Exercise Science Research: Assessing the Prevalence of Publication Bias and Studies with Underpowered Designs by a z-Curve Analysis},
  shorttitle = {On the Replicability of Sports and Exercise Science Research},
  author = {Mesquida, Cristian and Murphy, Jennifer and Warne, Joe and Lakens, Dani{\"e}l},
  year = {2025},
  month = apr,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.534},
  urldate = {2025-04-04},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {publication bias,replicability,statistical power},
  file = {/Users/cristian/Zotero/storage/TWSZ9W5Z/Mesquida et al. - 2025 - On the replicability of sports and exercise scienc.pdf}
}

@misc{mesquidaReplicationConcernsSports2022,
  title = {Replication Concerns in Sports Science: A Narrative Review of Selected Methodological Issues in the Field},
  shorttitle = {Replication Concerns in Sports Science},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = {2022},
  month = mar,
  institution = {SportRxiv},
  doi = {10.51224/SRXIV.127},
  urldate = {2022-11-28},
  langid = {english},
  keywords = {publication bias,questionable research practices,replicability,statistical power},
  file = {/Users/cristian/Zotero/storage/77BZQJ43/Mesquida et al. - 2022 - Replication concerns in sports science a narrativ.pdf}
}

@article{mesquidaReplicationConcernsSports2022a,
  title = {Replication Concerns in Sports and Exercise Science: A Narrative Review of Selected Methodological Issues in the Field},
  shorttitle = {Replication Concerns in Sports and Exercise Science},
  author = {Mesquida, Cristian and Murphy, Jennifer and Lakens, Dani{\"e}l and Warne, Joe},
  year = {2022},
  month = dec,
  journal = {Royal Society Open Science},
  volume = {9},
  number = {12},
  pages = {220946},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220946},
  urldate = {2025-03-29},
  abstract = {Known methodological issues such as publication bias, questionable research practices and studies with underpowered designs are known to decrease the replicability of study findings. The presence of such issues has been widely established across different research fields, especially in psychology. Their presence raised the first concerns that the replicability of study findings could be low and led researchers to conduct large replication projects. These replication projects revealed that a significant portion of original study findings could not be replicated, giving rise to the conceptualization of the replication crisis. Although previous research in the field of sports and exercise science has identified the first warning signs, such as an overwhelming proportion of significant findings, small sample sizes and lack of data availability, their possible consequences for the replicability of our field have been overlooked. We discuss the consequences of the above issues on the replicability of our field and offer potential solutions to improve replicability.},
  keywords = {hypothesis testing,Open Science practices,publication bias,questionable research practices,replicability,statistical power},
  file = {/Users/cristian/Zotero/storage/P72A75KI/Mesquida et al. - 2022 - Replication concerns in sports and exercise scienc.pdf}
}

@misc{mesquidaScopingReviewTransparency2025,
  title = {A Scoping Review of the Transparency, Reporting Practices and Methodological Rigor of Meta-Analyses Published in {{Sports Medicine}}},
  author = {Mesquida, Cristian and Warne, Joe and Lakens, Dani{\"e}l},
  year = {2025},
  month = jul,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.576},
  urldate = {2025-07-03},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {meta-analysis,meta-research,reproducibility,transparency},
  file = {/Users/cristian/Zotero/storage/5EWCFTMU/Mesquida et al. - 2025 - A scoping review of the transparency, reporting pr.pdf}
}

@misc{mesquidaWhatYourHypothesis2025,
  title = {What Is Your Hypothesis? {{On}} the Importance of Knowing Your Hypothesis before Conducting a Hypothesis Test},
  shorttitle = {What Is Your Hypothesis?},
  author = {Mesquida, Cristian and Warne, Joe and Lakens, Dani{\"e}l},
  year = {2025},
  month = jul,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.577},
  urldate = {2025-07-03},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {error control,meta-research,null hypothesis testing},
  file = {/Users/cristian/Zotero/storage/E42BFKAQ/Mesquida et al. - 2025 - What is your hypothesis  On the importance of kn.pdf}
}

@article{methaneethornPredictorsSirolimusPharmacokinetic2022,
  title = {Predictors of Sirolimus Pharmacokinetic Variability Identified Using a Nonlinear Mixed Effects Approach: A Systematic Review},
  author = {Methaneethorn, J. and {Art-Arsa}, P. and Kosiyaporn, R. and Leelakanok, N.},
  year = {2022},
  journal = {Journal of Population Therapeutics and Clinical Pharmacology},
  volume = {29},
  number = {4},
  pages = {11--29},
  publisher = {Codon Publications},
  doi = {10.47750/jptcp.2022.940}
}

@article{miAssessmentNonalcoholicFatty2016,
  title = {Assessment Nonalcoholic Fatty Liver Disease Fibrosis Score for Staging and Predicting Outcome},
  author = {Mi, X.-X. and Wang, L. and Xun, Y.-H. and Shi, J.-P.},
  year = {2016},
  journal = {International Journal of Clinical and Experimental Medicine},
  volume = {9},
  number = {8},
  pages = {16146--16156},
  publisher = {E-Century Publishing Corporation}
}

@article{micklewrightDevelopmentValidityRatingofFatigue2017,
  title = {Development and {{Validity}} of the {{Rating-of-Fatigue Scale}}},
  author = {Micklewright, D. and St Clair Gibson, A. and Gladwell, V. and Al Salman, A.},
  year = {2017},
  month = nov,
  journal = {Sports Medicine},
  volume = {47},
  number = {11},
  pages = {2375--2393},
  issn = {1179-2035},
  doi = {10.1007/s40279-017-0711-5},
  urldate = {2025-03-21},
  abstract = {The purpose of these experiments was to develop a rating-of-fatigue (ROF) scale capable of tracking the intensity of perceived fatigue in a variety of contexts.},
  langid = {english},
  keywords = {Electronic Supplementary Material Appendix,Likert Score,Respiratory Exchange Ratio,Ventilation Rate,Volitional Exhaustion},
  file = {/Users/cristian/Zotero/storage/Y5WQ7UDW/Micklewright et al. - 2017 - Development and Validity of the Rating-of-Fatigue .pdf}
}

@article{millerWhatProbabilityReplicating2009,
  title = {What Is the Probability of Replicating a Statistically Significant Effect?},
  author = {Miller, Jeff},
  year = {2009},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  pages = {617--640},
  doi = {10.3758/PBR.16.4.617},
  abstract = {If an initial experiment produces a statistically significant effect, what is the probability that this effect will be replicated in a follow-up experiment? I argue that this seemingly fundamental question can be interpreted in two very different ways and that its answer is, in practice, virtually unknowable under either interpretation. Although the data from an initial experiment can be used to estimate one type of replication probability, this estimate will rarely be precise enough to be of any use. The other type of replication probability is also unknowable, because it depends on unknown aspects of the research context. Thus, although it would be nice to know the probability of replicating a significant effect, researchers must accept the fact that they generally cannot determine this information, whichever type of replication probability they seek.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/SF3AKN3C/Miller - 2009 - What is the probability of replicating a statistic.pdf}
}

@article{milojevicQuantifyingCognitiveExtent2015,
  title = {Quantifying the Cognitive Extent of Science},
  author = {Milojevi{\'c}, Sta{\v s}a},
  year = {2015},
  month = oct,
  journal = {Journal of Informetrics},
  volume = {9},
  number = {4},
  pages = {962--973},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2015.10.005},
  urldate = {2022-03-16},
  abstract = {While the modern science is characterized by an exponential growth in scientific literature, the increase in publication volume clearly does not reflect the expansion of the cognitive boundaries of science. Nevertheless, most of the metrics for assessing the vitality of science or for making funding and policy decisions are based on productivity. Similarly, the increasing level of knowledge production by large science teams, whose results often enjoy greater visibility, does not necessarily mean that ``big science'' leads to cognitive expansion. Here we present a novel, big-data method to quantify the extents of cognitive domains of different bodies of scientific literature independently from publication volume, and apply it to 20 million articles published over 60--130 years in physics, astronomy, and biomedicine. The method is based on the lexical diversity of titles of fixed quotas of research articles. Owing to large size of quotas, the method overcomes the inherent stochasticity of article titles to achieve {$<$}1\% precision. We show that the periods of cognitive growth do not necessarily coincide with the trends in publication volume. Furthermore, we show that the articles produced by larger teams cover significantly smaller cognitive territory than (the same quota of) articles from smaller teams. Our findings provide a new perspective on the role of small teams and individual researchers in expanding the cognitive boundaries of science. The proposed method of quantifying the extent of the cognitive territory can also be applied to study many other aspects of `science of science.'},
  langid = {english},
  keywords = {Big science,Cognitive extent,Collaboration,Growth of science,Team science},
  file = {/Users/cristian/Zotero/storage/H4A7VZU8/Milojevi - 2015 - Quantifying the cognitive extent of science.pdf;/Users/cristian/Zotero/storage/FTDWLLBC/S175115771530081X.html}
}

@article{mingersEvaluatingJournalQuality2017,
  title = {Evaluating Journal Quality: {{A}} Review of Journal Citation Indicators and Ranking in Business and Management},
  shorttitle = {Evaluating Journal Quality},
  author = {Mingers, John and Yang, Liying},
  year = {2017},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {257},
  number = {1},
  pages = {323--337},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2016.07.058},
  urldate = {2021-04-20},
  abstract = {Evaluating the quality of academic journals is becoming increasing important within the context of research performance evaluation. Traditionally, journals have been ranked by peer review lists such as that of the Association of Business Schools (UK) or though their journal impact factor (JIF). However, several new indicators have been developed, such as the h-index, SJR, SNIP and the Eigenfactor which take into account different factors and therefore have their own particular biases. In this paper we evaluate these metrics both theoretically and also through an empirical study of a large set of business and management journals. We show that even though the indicators appear highly correlated in fact they lead to large differences in journal rankings. We contextualise our results in terms of the UK's large scale research assessment exercise (the RAE/REF) and particularly the ABS journal ranking list. We conclude that no one indicator is superior but that the h-index (which includes the productivity of a journal) and SNIP (which aims to normalise for field effects) may be the most effective at the moment.},
  langid = {english},
  keywords = {ABS journal list,H-index,Impact factor,SJR,SNIP},
  file = {/Users/cristian/Zotero/storage/CKHNXI7Y/Mingers and Yang - 2017 - Evaluating journal quality A review of journal ci.pdf;/Users/cristian/Zotero/storage/R8ZMKR7Y/S0377221716306130.html}
}

@article{mingxingPhosphocreatineCardiacSurgery2018,
  title = {Phosphocreatine in {{Cardiac Surgery Patients}}: {{A Meta-Analysis}} of {{Randomized Controlled Trials}}},
  author = {Mingxing, F. and Landoni, G. and Zangrillo, A. and Monaco, F. and Lomivorotov, V.V. and Hui, C. and Novikov, M. and Nepomniashchikh, V. and Fominskiy, E.},
  year = {2018},
  journal = {Journal of Cardiothoracic and Vascular Anesthesia},
  volume = {32},
  number = {2},
  pages = {762--770},
  publisher = {W.B. Saunders},
  doi = {10.1053/j.jvca.2017.07.024}
}

@article{mirandaComparisonShareDocuments2019,
  title = {Comparison of the Share of Documents and Citations from Different Quartile Journals in 25 Research Areas},
  author = {Miranda, Ruben and {Garcia-Carpintero}, Esther},
  year = {2019},
  month = oct,
  journal = {Scientometrics},
  volume = {121},
  number = {1},
  pages = {479--501},
  issn = {1588-2861},
  doi = {10.1007/s11192-019-03210-z},
  urldate = {2021-04-21},
  abstract = {The total number of publications and/or the share of total publications in a given quartile, usually first quartile (Q1), is increasingly used in performance-based funding of public research. However, the quality significance of publishing in Q1 journals is very different depending on the research areas. Both the expected probability to publish in Q1 journals, given by the number of papers published in each quartile, as well as the average citations received by Q1 publications compared to other quartiles, is largely dependent on the research area. This study analyzes the share of articles published in each quartile in the 25 largest research areas indexed by Science Citation Index-Expanded (Web of Science) and their main citation characteristics aiming to enrich the discussion about journal-based evaluation systems and specifically the number and/or the share of publications in Q1. It was found that the average share of documents published in Q1 was 45.7\% (38.4\% for articles and reviews), varying from 25.4 to 85.6\% (from 17.1 to 88.9\% for articles and reviews) depending on the area. Q1 publications were cited, on average, 2.07 times more than Q2 publications (2.41 times for articles plus reviews), however, depending on the area, this ratio varied from 0.9 to 6.1 (from 1.7 to 5.4 times for articles plus reviews). Q1 (total publications or articles plus reviews), received, on average, 65\% of total citations of the research area, but again this value varied from 46 to 98\% depending on the area.},
  langid = {english}
}

@article{moEffectivenessCreatineTreatment2017,
  title = {The Effectiveness of Creatine Treatment for {{Parkinson}}'s Disease: {{An}} Updated Meta-Analysis of Randomized Controlled Trials},
  author = {Mo, J.-J. and Liu, L.-Y. and Peng, W.-B. and Rao, J. and Liu, Z. and Cui, L.-L.},
  year = {2017},
  journal = {BMC Neurology},
  volume = {17},
  number = {1},
  publisher = {BioMed Central Ltd.},
  doi = {10.1186/s12883-017-0885-3}
}

@article{moeyaertMethodsDealingMultiple2017,
  ids = {moeyaertMethodsDealingMultiple2017a},
  title = {Methods for Dealing with Multiple Outcomes in Meta-Analysis: A Comparison between Averaging Effect Sizes, Robust Variance Estimation and Multilevel Meta-Analysis},
  shorttitle = {Methods for Dealing with Multiple Outcomes in Meta-Analysis},
  author = {Moeyaert, Mariola and Ugille, Maaike and Natasha Beretvas, S. and Ferron, John and Bunuan, Rommel and {Van den Noortgate}, Wim},
  year = {2017},
  month = nov,
  journal = {International Journal of Social Research Methodology},
  volume = {20},
  number = {6},
  pages = {559--572},
  publisher = {Routledge},
  address = {United Kingdom},
  issn = {1364-5579},
  doi = {10.1080/13645579.2016.1252189},
  urldate = {2024-06-10},
  abstract = {This study investigates three methods to handle dependency among effect size estimates in meta-analysis arising from studies reporting multiple outcome measures taken on the same sample. The three-level approach is compared with the method of robust variance estimation, and with averaging effects within studies. A simulation study is performed, and the fixed and random effect estimates of the three methods are compared with each other. Both the robust variance estimation and three-level approach result in unbiased estimates of the fixed effects, corresponding standard errors and variances. Averaging effect sizes results in overestimated standard errors when the effect sizes within studies are truly independent. Although the robust variance and three-level approach are more complicated to use, they have the advantage that they do not require an estimate of the correlation between outcomes, and they still result in unbiased parameter estimates.},
  keywords = {averaging effects,dependent effect sizes,Effect Size (Statistical),Meta Analysis,Meta-analysis,multilevel meta-analysis,robust variance estimation,Statistical Estimation},
  file = {/Users/cristian/Zotero/storage/U7JC78QH/2017-42149-003.html}
}

@article{molloyMultiplicityAdjustmentsParallelgroup2022,
  title = {Multiplicity Adjustments in Parallel-Group Multi-Arm Trials Sharing a Control Group: {{Clear}} Guidance Is Needed},
  shorttitle = {Multiplicity Adjustments in Parallel-Group Multi-Arm Trials Sharing a Control Group},
  author = {Molloy, S{\'i}le F. and White, Ian R. and Nunn, Andrew J. and Hayes, Richard and Wang, Duolao and Harrison, Thomas S.},
  year = {2022},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {113},
  pages = {106656},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2021.106656},
  urldate = {2025-04-12},
  abstract = {Multi-arm, parallel-group clinical trials are an efficient way of testing several new treatments, treatment regimens or doses. However, guidance on the requirement for statistical adjustment to control for multiple comparisons (type I error) using a shared control group is unclear. We argue, based on current evidence, that adjustment is not always necessary in such situations. We propose that adjustment should not be a requirement in multi-arm, parallel-group trials testing distinct treatments and sharing a control group, and we call for clearer guidance from stakeholders, such as regulators and scientific journals, on the appropriate settings for adjustment of multiplicity.},
  keywords = {{Multi-arm, parallel-group clinical trials},Family-wise error rate (FWER),Multiplicity,Type 1 error},
  file = {/Users/cristian/Zotero/storage/SZYSAQEI/Molloy et al. - 2022 - Multiplicity adjustments in parallel-group multi-a.pdf;/Users/cristian/Zotero/storage/M5QLIHTD/S155171442100392X.html}
}

@article{mooreImpactColdWaterImmersion2022,
  ids = {mooreImpactColdWaterImmersion2022a},
  title = {Impact of {{Cold-Water Immersion Compared}} with {{Passive Recovery Following}} a {{Single Bout}} of {{Strenuous Exercise}} on {{Athletic Performance}} in {{Physically Active}}  {{Participants}}: {{A Systematic Review}} with {{Meta-analysis}} and {{Meta-regression}}.},
  author = {Moore, Emma and Fuller, Joel T. and Buckley, Jonathan D. and Saunders, Siena and Halson, Shona L. and Broatch, James R. and Bellenger, Clint R.},
  year = {2022},
  month = jul,
  journal = {Sports medicine (Auckland, N.Z.)},
  volume = {52},
  number = {7},
  pages = {1667--1688},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  address = {New Zealand},
  issn = {1179-2035 0112-1642},
  doi = {10.1007/s40279-022-01644-9},
  abstract = {BACKGROUND: Studies investigating the effects of cold-water immersion (CWI) on the recovery of athletic performance, perceptual measures and creatine kinase  (CK) have reported mixed results in physically active populations. OBJECTIVES:  The purpose of this systematic review was to investigate the effects of CWI on  recovery of athletic performance, perceptual measures and CK following an acute  bout of exercise in physically active populations. STUDY DESIGN: Systematic  review with meta-analysis and meta-regression. METHODS: A systematic search was  conducted in September 2021 using Medline, SPORTDiscus, Scopus, Web of Science,  Cochrane Library, EmCare and Embase databases. Studies were included if they were  peer reviewed and published in English, included participants who were involved  in sport or deemed physically active, compared CWI with passive recovery methods  following an acute bout of strenuous exercise and included athletic performance,  athlete perception and CK outcome measures. Studies were divided into two  strenuous exercise subgroups: eccentric exercise and high-intensity exercise.  Random effects meta-analyses were used to determine standardised mean differences  (SMD) with 95\% confidence intervals. Meta-regression analyses were completed with  water temperature and exposure durations as continuous moderator variables.  RESULTS: Fifty-two studies were included in the meta-analyses. CWI improved the  recovery of muscular power 24~h after eccentric exercise (SMD 0.34 [95\% CI  0.06-0.62]) and after high-intensity exercise (SMD 0.22 [95\% CI 0.004-0.43]), and  reduced serum CK (SMD\,-\,0.85 [95\% CI\,-\,1.61 to\,-\,0.08]) 24~h after high-intensity  exercise. CWI also improved muscle soreness (SMD\,-\,0.89 [95\% CI\,-\,1.48  to\,-\,0.29]) and perceived feelings of recovery (SMD 0.66 [95\% CI 0.29-1.03]) 24~h  after high-intensity exercise. There was no significant influence on the recovery  of strength performance following either eccentric or high-intensity exercise.  Meta-regression indicated that shorter time and lower temperatures were related  to the largest beneficial effects on serum CK (duration and temperature dose  effects) and endurance performance (duration dose effects only) after  high-intensity exercise. CONCLUSION: CWI was an effective recovery tool after  high-intensity exercise, with positive outcomes occurring for muscular power,  muscle soreness, CK, and perceived recovery 24~h after exercise. However, after  eccentric exercise, CWI was only effective for positively influencing muscular  power 24~h after exercise. Dose-response relationships emerged for positively  influencing endurance performance and reducing serum CK, indicating that shorter  durations and lower temperatures may improve the efficacy of CWI if used after  high-intensity exercise. FUNDING: Emma Moore is supported by a Research Training  Program (Domestic) Scholarship from the Australian Commonwealth Department of  Education and Training. PROTOCOL REGISTRATION: Open Science Framework:  10.17605/OSF.IO/SRB9D.},
  copyright = {{\copyright} 2022. The Author(s).},
  langid = {english},
  pmcid = {PMC9213381},
  pmid = {35157264},
  keywords = {*Athletic Performance,*Myalgia/therapy,Cold Temperature,Creatine Kinase,Humans,Immersion,Water}
}

@article{morenoAssessmentRegressionbasedMethods2009,
  title = {Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study},
  author = {Moreno, Santiago G. and Sutton, Alex J. and Ades, {\relax AE} and Stanley, Tom D. and Abrams, Keith R. and Peters, Jaime L. and Cooper, Nicola J.},
  year = {2009},
  month = jan,
  journal = {BMC Medical Research Methodology},
  volume = {9},
  number = {1},
  pages = {2},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-9-2},
  urldate = {2023-02-20},
  abstract = {In meta-analysis, the presence of funnel plot asymmetry is attributed to publication or other small-study effects, which causes larger effects to be observed in the smaller studies. This issue potentially mean inappropriate conclusions are drawn from a meta-analysis. If meta-analysis is to be used to inform decision-making, a reliable way to adjust pooled estimates for potential funnel plot asymmetry is required.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/EG7QTSC2/Moreno et al. - 2009 - Assessment of regression-based methods to adjust f.pdf}
}

@article{moreyStatisticsAcceptingNull2018,
  title = {Beyond {{Statistics}}: {{Accepting}} the {{Null Hypothesis}} in {{Mature Sciences}}},
  shorttitle = {Beyond {{Statistics}}},
  author = {Morey, Richard D. and Homer, Saskia and Proulx, Travis},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {245--258},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918776023},
  urldate = {2023-09-06},
  abstract = {Scientific theories explain phenomena using simplifying assumptions?for instance, that the speed of light does not depend on the direction in which the light is moving, or that the shape of a pea plant?s seeds depends on a small number of alleles randomly obtained from its parents. These simplifying assumptions often take the form of statistical null hypotheses; hence, supporting these simplifying assumptions with statistical evidence is crucial to scientific progress, though it might involve ?accepting? a null hypothesis. We review two historical examples in which statistical evidence was used to accept a simplifying assumption (that there is no luminiferous ether and that genetic traits are passed on in discrete forms) and one in which the null hypothesis was not accepted despite repeated failures (gravitational waves), drawing lessons from each. We emphasize the role of the scientific context in acceptance of the null: Accepting a null hypothesis is never a purely statistical affair.},
  file = {/Users/cristian/Zotero/storage/KUEYKTCA/Morey et al. - 2018 - Beyond Statistics Accepting the Null Hypothesis i.pdf}
}

@article{morgantiImpactMetaanalysesClinical2007,
  title = {The Impact of Meta-Analyses on Clinical Practice: {{The}} Benefits},
  author = {Morganti, A.},
  year = {2007},
  journal = {Journal of Nephrology},
  volume = {20},
  number = {SUPPL. 12},
  pages = {S1-S3}
}

@article{morikawaFirstlineGefitinibElderly2015,
  title = {First-Line Gefitinib for Elderly Patients with Advanced {{NSCLC}} Harboring {{EGFR}} Mutations. {{A}} Combined Analysis of {{North-East Japan Study Group}} Studies},
  author = {Morikawa, N. and Minegishi, Y. and Inoue, A. and Maemondo, M. and Kobayashi, K. and Sugawara, S. and Harada, M. and Hagiwara, K. and Okinaga, S. and Oizumi, S. and Nukiwa, T. and Gemma, A.},
  year = {2015},
  journal = {Expert Opinion on Pharmacotherapy},
  volume = {16},
  number = {4},
  pages = {465--472},
  publisher = {Informa Healthcare},
  doi = {10.1517/14656566.2015.1002396}
}

@article{morrisCombiningEffectSize2002,
  title = {Combining Effect Size Estimates in Meta-Analysis with Repeated Measures and Independent-Groups Designs},
  author = {Morris, Scott B. and DeShon, Richard P.},
  year = {2002},
  month = mar,
  journal = {Psychological Methods},
  volume = {7},
  number = {1},
  pages = {105--125},
  issn = {1082-989X},
  doi = {10.1037/1082-989x.7.1.105},
  abstract = {When a meta-analysis on results from experimental studies is conducted, differences in the study design must be taken into consideration. A method for combining results across independent-groups and repeated measures designs is described, and the conditions under which such an analysis is appropriate are discussed. Combining results across designs requires that (a) all effect sizes be transformed into a common metric, (b) effect sizes from each design estimate the same treatment effect, and (c) meta-analysis procedures use design-specific estimates of sampling variance to reflect the precision of the effect size estimates.},
  langid = {english},
  pmid = {11928886},
  keywords = {{Models, Psychological},Humans,Meta-Analysis as Topic}
}

@article{morrisMetaAnalysisOrganizationalResearch2023,
  title = {Meta-{{Analysis}} in {{Organizational Research}}: {{A Guide}} to {{Methodological Options}}},
  shorttitle = {Meta-{{Analysis}} in {{Organizational Research}}},
  author = {Morris, Scott B.},
  year = {2023},
  month = jan,
  journal = {Annual Review of Organizational Psychology and Organizational Behavior},
  volume = {10},
  number = {Volume 10, 2023},
  pages = {225--259},
  publisher = {Annual Reviews},
  issn = {2327-0608, 2327-0616},
  doi = {10.1146/annurev-orgpsych-031921-021922},
  urldate = {2025-06-12},
  abstract = {Meta-analysis provides a powerful tool for integrating findings from the research literature and building statistical models to explore trends and inconsistencies in the research base. Meta-analysis starts with a process for translating results from each study into an effect size that represents all findings in a common metric. Statistical models are then applied to estimate the mean, variance, and moderators of effect size. This article explores several key decision points in conducting a meta-analysis, including issues in obtaining a common metric, accounting for psychometric artifacts, and choosing an appropriate statistical model. It provides recommendations for choosing among alternate approaches and reporting results to ensure transparency.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/AFLJIFGZ/Morris - 2023 - Meta-Analysis in Organizational Research A Guide .pdf;/Users/cristian/Zotero/storage/PPFHK4YG/annurev-orgpsych-031921-021922.html}
}

@article{motulskyCommonMisconceptionsData2014,
  title = {Common Misconceptions about Data Analysis and Statistics},
  author = {Motulsky, Harvey J.},
  year = {2014},
  journal = {Pharmacology Research \& Perspectives},
  volume = {387},
  number = {11},
  pages = {1017--1023},
  doi = {10.1002/prp2.93},
  abstract = {Ideally, any experienced investigator with the right tools should be able to reproduce a finding published in a peer-reviewed biomedical science journal. In fact, the reproducibility of a large percentage of published findings has been questioned. Undoubtedly, there are many reasons for this, but one reason may be that investigators fool themselves due to a poor understanding of statistical concepts. In particular, investigators often make these mistakes: (1) P-Hacking. This is when you reanalyze a data set in many different ways, or perhaps reanalyze with additional replicates, until you get the result you want. (2) Overemphasis on P values rather than on the actual size of the observed effect. (3) Overuse of statistical hypothesis testing, and being seduced by the word ``significant''. (4) Overreliance on standard errors, which are often misunderstood.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/J7YVPQL3/Motulsky - 2015 - Common misconceptions about data analysis and stat.pdf;/Users/cristian/Zotero/storage/UA3LSNUM/prp2.html}
}

@article{motylStateSocialPersonality2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  langid = {english},
  pmid = {28447837},
  keywords = {{Ethics, Research},{Psychology, Social},Attitude of Health Personnel,Female,Humans,Male,Personality,Psychology,Research,Research Design,Surveys and Questionnaires},
  file = {/Users/cristian/Zotero/storage/7BJLNIID/Motyl et al. - 2017 - The state of social and personality science Rotte.pdf}
}

@misc{MovingValuesJournal,
  title = {Moving beyond {{P}} Values in {{The Journal}} of {{Physiology}}: {{A}} Primer on the Value of Effect Sizes and Confidence Intervals - {{Google Search}}},
  urldate = {2023-11-17},
  howpublished = {https://www.google.com/search?q=Moving+beyond+P+values+in+The+Journal+of+Physiology\%3A+A+primer+on+the+value+of+effect+sizes+and+confidence+intervals\&oq=Moving+beyond+P+values+in+The+Journal+of+Physiology\%3A+A+primer+on+the+value+of+effect+sizes+and+confidence+intervals\&gs\_lcrp=EgZjaHJvbWUqBggAEEUYOzIGCAAQRRg7MgYIARBFGDwyBggCEEUYPDIGCAMQRRg80gEHMzc5ajBqOagCALACAA\&sourceid=chrome\&ie=UTF-8}
}

@misc{MultiplicityIssuesClinical2003,
  title = {Multiplicity Issues in Clinical Trials - {{Scientific}} Guideline {\textbar} {{European Medicines Agency}} ({{EMA}})},
  year = {2003},
  month = apr,
  urldate = {2025-04-12},
  howpublished = {https://www.ema.europa.eu/en/multiplicity-issues-clinical-trials-scientific-guideline},
  langid = {english}
}

@article{munafoManifestoReproducibleScience2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  doi = {10.1038/s41562-016-0021},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  keywords = {Social sciences},
  file = {/Users/cristian/Zotero/storage/Z8MRAL92/Munaf et al. - 2017 - A manifesto for reproducible science.pdf;/Users/cristian/Zotero/storage/JU33IK3G/s41562-016-0021.html}
}

@article{murayama_2022,
  title = {Summary-Statistics-Based Power Analysis: {{A}} New and Practical Method to Determine Sample Size for Mixed-Effects Modeling},
  shorttitle = {Summary-Statistics-Based Power Analysis},
  author = {Murayama, Kou and Usami, Satoshi and Sakaki, Michiko},
  year = {2022},
  journal = {Psychological Methods},
  volume = {27},
  number = {6},
  pages = {1014--1038},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000330},
  abstract = {This article proposes a summary-statistics-based power analysis---a practical method for conducting power analysis for mixed-effects modeling with two-level nested data (for both binary and continuous predictors), complementing the existing formula-based and simulation-based methods. The proposed method bases its logic on conditional equivalence of the summary-statistics approach and mixed-effects modeling, paring back the power analysis for mixed-effects modeling to that for a simpler statistical analysis (e.g., one-sample t test). Accordingly, the proposed method allows us to conduct power analysis for mixed-effects modeling using popular software such as G*Power or the pwr package in R and, with minimum input from relevant prior work (e.g., t value). We provide analytic proof and a series of statistical simulations to show the validity and robustness of the summary-statistics-based power analysis and show illustrative examples with real published work. We also developed a web app (https://koumurayama.shinyapps.io/summary\_statistics\_based\_power/) to facilitate the utility of the proposed method. While the proposed method has limited flexibilities compared with the existing methods in terms of the models and designs that can be appropriately handled, it provides a convenient alternative for applied researchers when there is limited information to conduct power analysis. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Computer Simulation,Models,Sample Size,Simulation,Statistical Analysis,Statistical Power,Statistical Validity},
  file = {/Users/cristian/Zotero/storage/3ANG2PTI/Murayama et al. - 2022 - Summary-statistics-based power analysis A new and.pdf}
}

@article{murphy_minimum-effect-test_1999,
  title = {Testing the Hypothesis That Treatments Have Negligible Effects: {{Minimum-effect}} Tests in the General Linear Model},
  shorttitle = {Testing the Hypothesis That Treatments Have Negligible Effects},
  author = {Murphy, Kevin R. and Myors, Brett},
  year = {1999},
  journal = {Journal of Applied Psychology},
  volume = {84},
  number = {2},
  pages = {234--248},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1854},
  doi = {10.1037/0021-9010.84.2.234},
  abstract = {Researchers are often interested in testing the hypothesis that the effects of treatments, interventions, and so on are negligibly small rather than testing the hypothesis that treatments have no effect whatsoever. A number of procedures for conducting such tests have been suggested but have yet to be widely adopted. In this article, simple methods of testing such minimum-effect hypotheses are illustrated in a variety of applications of the general linear model. Tables and computational routines that can be used in conjunction with the familiar F test to evaluate the hypothesis that the effects of treatments or interventions exceed some minimum level are also provided. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Null Hypothesis Testing,Statistical Tests,Treatment,Treatment Effectiveness Evaluation}
}

@article{murphy_replicability_2025,
  title = {Estimating the {{Replicability}} of {{Sports}} and {{Exercise Science Research}}},
  author = {Murphy, Jennifer and Caldwell, Aaron R. and Mesquida, Cristian and Ladell, Aera J. M. and {Encarnaci{\'o}n-Mart{\'i}nez}, Alberto and Tual, Alexandre and Denys, Andrew and Cameron, Bailey and Van Hooren, Bas and Parr, Ben and DeLucia, Bianca and Mason, Billy R. J. and Clark, Brad and Egan, Brendan and Brown, Calum and Ade, Carl and Sforza, Chiarella and Taber, Christopher B. and Kirk, Christopher and McCrum, Christopher and Tighe, Cian Okeeffe and Byrne, Ciara and Brunetti, Claudia and Forestier, Cyril and Martin, Dan and Taylor, Danny and Diggin, David and Gallagher, Dearbhla and King, Deborah L. and Rogers, Elizabeth and Bennett, Eric C. and Lopatofsky, Eric T. and Dunn, Gemma and Gauchar, G{\'e}rome C. and Mornieux, Guillaume and {Catal{\'a}-Vilaplana}, Ignacio and Caetan, Ines and {Aparicio-Aparicio}, Inmaculada and Barnes, Jack and Blaisdell, Jake and Steele, James and Fletcher, Jared R. and Hutchinson, Jasmin and Au, Jason and Oliemans, Jason P. and Bakhshinejad, Javad and Barrios, Joaquin and Quesada, Jose Ignacio Priego and Rager, Joseph and Capon, Julia B. and Walton, Julie S. J. and Stevens, Kailey and Heinrich, Katie and Wu, Kelly and Meijer, Kenneth and Richards, Laura and Jutlah, Lauren and Tong, Le and Bridgeman, Lee and Banet, Leo and Mbiyu, Leonard and Sefton, Lucy and {de Chanaleilles}, Margaret and Charisi, Maria and Beerse, Matthew and Major, Matthew J. and Caon, Maya and Bargh, Mel and Rowley, Michael and Moran, Miguel Vaca and Croker, Nicholas and Hanen, Nicolas C. and Montague, Nicole and Brick, Noel E. and Runswick, Oliver R. and Willems, Paul and {P{\'e}rez-Soriano}, Pedro and Blake, Rebecca and Jones, Rebecca and Quinn, Rebecca Louise and {Sanchis-Sanchis}, Roberto and Rabello, Rodrigo and Bolger, Roisin and Shohat, Roy and Cotton, Sadie and Chua, Samantha and Norwood, Samuel and Vimeau, Samuel and Dias, Sandro and Pedersen, Sissel and Skaper, Spencer S. and Coyle, Taylor and Desai, Terun and Gee, Thomas I. and Edwards, Tobias and Pohl, Torsten and Yingling, Vanessa and Ribeiro, Vinicius and Duchene, Youri and Papadakis, Zacharias and Warne, Joe P.},
  year = {2025},
  journal = {Sports Medicine},
  issn = {1179-2035},
  doi = {10.1007/s40279-025-02201-w},
  urldate = {2025-06-17},
  abstract = {The replicability of sports and exercise research has not been assessed previously despite concerns about scientific practices within the field.},
  langid = {english},
  keywords = {Communication and replication,Experimental Psychology,Psychometrics,Sport Psychology,Sport Science,Sport Theory},
  file = {/Users/cristian/Zotero/storage/JWDTUMR6/Murphy et al. - 2025 - Estimating the Replicability of Sports and Exercis.pdf}
}

@article{murphyNonsignificanceMisinterpretedEffects2025,
  title = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology: Prevalence and Temporal Analyses},
  shorttitle = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology},
  author = {Murphy, Stephen Lee and Merz, Raphael and Reimann, Linda-Elisabeth and Fern{\'a}ndez, Aurelio},
  year = {2025},
  month = mar,
  journal = {Royal Society Open Science},
  volume = {12},
  number = {3},
  pages = {242167},
  issn = {2054-5703},
  doi = {10.1098/rsos.242167},
  urldate = {2025-04-22},
  abstract = {Nonsignificant findings in psychological research are frequently misinterpreted as reflecting the effect's absence. However, this issue's exact prevalence remains unclear, as does whether this issue is getting better or worse. In this pre-registered study, we sought to answer these questions by examining the discussion sections of 599 articles published across 10 psychology journals and three time points (2009, 2015 and 2021), and coding whether a nonsignificant finding was interpreted in such a way as to suggest the effect does not exist. Our models indicate that between 76\% and 85\% of psychology articles published between 2009 and 2021 that discussed a nonsignificant finding misinterpreted nonsignificance as reflecting no effect. It is likely between 54\% and 62\% of articles over this time period claimed explicitly that this meant no effect on the population of interest. Our findings also indicate that only between 4\% and 8\% of articles explicitly discussed the possibility that the nonsignificant effect may exist but could not be found. Differences in prevalence rates over time were nonsignificant. Collectively, our findings indicate this interpretative error is a major problem in psychology. We call on stakeholders with an interest in improving psychological science to prioritize tackling it.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/7PEE2W3L/Murphy et al. - 2025 - Nonsignificance misinterpreted as an effects abse.pdf}
}

@article{murphyPreliminaryAnalysesReplicability2023,
  title = {Preliminary {{Analyses Of The Replicability Of Sports And Exercise Science Research}}: 2396},
  shorttitle = {Preliminary {{Analyses Of The Replicability Of Sports And Exercise Science Research}}},
  author = {Murphy, Jennifer and Caldwell, Aaron R. and Warne, Joe},
  year = {2023},
  month = sep,
  journal = {Medicine \& Science in Sports \& Exercise},
  volume = {55},
  number = {9S},
  pages = {798},
  issn = {0195-9131},
  doi = {10.1249/01.mss.0000987352.35970.ea},
  urldate = {2023-10-17},
  abstract = {An abstract is unavailable.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/YMWJJV74/preliminary_analyses_of_the_replicability_of.1915.html}
}

@article{murphyProposalSelectionProtocol2022,
  title = {Proposal of a {{Selection Protocol}} for {{Replication}} of {{Studies}} in {{Sports}} and {{Exercise Science}}},
  author = {Murphy, Jennifer and Mesquida, Cristian and Caldwell, Aaron R. and Earp, Brian D. and Warne, Joe P.},
  year = {2022},
  month = sep,
  journal = {Sports Medicine},
  issn = {1179-2035},
  doi = {10.1007/s40279-022-01749-1},
  abstract = {To improve the rigor of science, experimental evidence for scientific claims ideally needs to be replicated repeatedly with comparable analyses and new data to increase the collective confidence in the veracity of those claims. Large replication projects in psychology and cancer biology have evaluated the replicability of their fields but no collaborative effort has been undertaken in sports and exercise science. We propose to undertake such an effort here. As this is the first large replication project in this field, there is no agreed-upon protocol for selecting studies to replicate. Criticism of previous selection protocols include claims they were non-randomised and non-representative. Any selection protocol in sports and exercise science must be representative to provide an accurate estimate of replicability of the field. Our aim is to produce a protocol for selecting studies to replicate for inclusion in a large replication project in sports and exercise science. METHODS: The proposed selection protocol uses multiple inclusion and exclusion criteria for replication study selection, including: the year of publication and citation rankings, research disciplines, study types, the research question and key dependent variable, study methods and feasibility. Studies selected for replication will be stratified into pools based on instrumentation and expertise required, and will then be allocated to volunteer laboratories for replication. Replication outcomes will be assessed using a multiple inferential strategy and descriptive information will be reported regarding the final number of included and excluded studies, and original author responses to requests for raw data.},
  langid = {english},
  pmid = {36066754},
  file = {/Users/cristian/Zotero/storage/IEKPQ28N/Murphy et al. - 2022 - Proposal of a Selection Protocol for Replication o.pdf}
}

@techreport{murphySelectionProtocolReplication2021,
  title = {Selection {{Protocol}} for {{Replication}} in {{Sports}} and {{Exercise Science}}},
  author = {Murphy, Jennifer and Mesquida, Cristian and Caldwell, Aaron R. and Earp, Brian D. and Warne, Joe},
  year = {2021},
  month = apr,
  institution = {OSF Preprints},
  abstract = {Introduction: To improve the rigor of science, experimental evidence for scientific claims ideally needs to be replicated repeatedly with sufficiently similar procedures to increase the collective confidence in the veracity of those claims. Large replication projects in psychology, cancer biology and social science have evaluated the replicability of their fields but no collaborative effort has been undertaken in sports and exercise science. We propose to undertake such an effort here. As this is the first large replication project in this field, there is no agreed-upon protocol for selecting studies to replicate. Criticism of the previous selection protocols include claims they were non-randomized and non-representative, and alleged to be biased. Any selection protocol in sports and exercise science must be unbiased and representative to provide an accurate estimate of replicability of the field. The aim of this document is to produce a protocol for selecting studies to replicate for inclusion in a large replication project in sports and exercise science. Methods: The proposed selection protocol uses multiple inclusion and exclusion criteria for replication study selection, including: the year of publication and citation rankings, research disciplines, study types, the research question and key dependent variable, study methods and feasibility. Studies selected for replication will be stratified into pools based on instrumentation and expertise required and will then be allocated to volunteer laboratories for replication. Replication outcomes will be assessed using a multiple inferential strategy and descriptive information will be reported regarding the final number of included and excluded studies, and original author contact.},
  keywords = {kinesiology,Kinesiology,Life Sciences,Medicine and Health Sciences,open science,replication,Social and Behavioral Sciences,sports and exercise science,Sports Sciences,Sports Studies},
  file = {/Users/cristian/Zotero/storage/W3IR3DIZ/Murphy et al. - 2021 - Selection Protocol for Replication in Sports and E.pdf}
}

@article{murphySurveyAttitudesPerception2023,
  title = {A {{Survey}} on the {{Attitudes Towards}} and {{Perception}} of {{Reproducibility}} and {{Replicability}} in {{Sports}} and {{Exercise Science}}},
  author = {Murphy, Jennifer and Mesquida, Cristian and Warne, Joe},
  year = {2023},
  month = may,
  journal = {Communications in Kinesiology},
  volume = {1},
  number = {5},
  issn = {2767-0732},
  doi = {10.51224/cik.2023.53},
  urldate = {2025-03-29},
  abstract = {There are formal calls for increased reproducibility and replicability in sports and exercise science, yet there is minimal information on the overall knowledge of these concepts at a field-wide level. Therefore, we conducted a survey on the attitudes and perceptions of sports and exercise science researchers towards reproducibility and replicability. Descriptive statistics (e.g., proportion of responses), and thematic analysis, were utilized to characterize the responses. Of the 511 respondents, 42\% (n = 217) believe there is a significant crisis of reproducibility or replicability in sports and exercise science while 36\% (n = 182) believe there is a slight crisis. 3\% (n = 15) of respondents believe there is no crisis while 19\% (n = 95) did not know. Four themes were generated in the thematic analysis: the research and publishing culture, educational barriers to research integrity, research responsibility to ensure reproducibility and replicability, and current practices facilitating reproducibility and replicability. Researchers believe that engaging in open science can be detrimental to career opportunities due to lack of incentives. They also feel journals are a barrier to reproducible and replicable research due to high publication charges and a focus on novelty. Statistical expertise was identified as a key factor for improving reproducibility and replicability in the future, particularly, a better understanding of study design and different statistical techniques. Statistical education should be prioritised for early career researchers which could positively affect publication and peer review. Researchers must accept responsibility for reproducibility and replicability with thorough project design, appropriate planning of analyses, and transparent reporting practices.},
  copyright = {Copyright (c) 2023 Jennifer Murphy, Cristian Mesquida, Joe P. Warne},
  langid = {english},
  keywords = {open science,replication,reproducibility,research methods,sports and exercise science,statistics},
  file = {/Users/cristian/Zotero/storage/59JED2KK/Murphy et al. - 2023 - A Survey on the Attitudes Towards and Perception o.pdf}
}

@article{murphySurveyAttitudesPerception2023a,
  title = {A {{Survey}} on the {{Attitudes Towards}} and {{Perception}} of {{Reproducibility}} and {{Replicability}} in {{Sports}} and {{Exercise Science}}},
  author = {Murphy, Jennifer and Mesquida, Cristian and Warne, Joe},
  year = {2023},
  month = may,
  journal = {Communications in Kinesiology},
  volume = {1},
  number = {5},
  issn = {2767-0732},
  doi = {10.51224/cik.2023.53},
  urldate = {2025-07-14},
  abstract = {There are formal calls for increased reproducibility and replicability in sports and exercise science, yet there is minimal information on the overall knowledge of these concepts at a field-wide level. Therefore, we conducted a survey on the attitudes and perceptions of sports and exercise science researchers towards reproducibility and replicability. Descriptive statistics (e.g., proportion of responses), and thematic analysis, were utilized to characterize the responses. Of the 511 respondents, 42\% (n = 217) believe there is a significant crisis of reproducibility or replicability in sports and exercise science while 36\% (n = 182) believe there is a slight crisis. 3\% (n = 15) of respondents believe there is no crisis while 19\% (n = 95) did not know. Four themes were generated in the thematic analysis: the research and publishing culture, educational barriers to research integrity, research responsibility to ensure reproducibility and replicability, and current practices facilitating reproducibility and replicability. Researchers believe that engaging in open science can be detrimental to career opportunities due to lack of incentives. They also feel journals are a barrier to reproducible and replicable research due to high publication charges and a focus on novelty. Statistical expertise was identified as a key factor for improving reproducibility and replicability in the future, particularly, a better understanding of study design and different statistical techniques. Statistical education should be prioritised for early career researchers which could positively affect publication and peer review. Researchers must accept responsibility for reproducibility and replicability with thorough project design, appropriate planning of analyses, and transparent reporting practices.},
  copyright = {Copyright (c) 2023 Jennifer Murphy, Cristian Mesquida, Joe P. Warne},
  langid = {english},
  keywords = {open science,replication,reproducibility,research methods,sports and exercise science,statistics},
  file = {/Users/cristian/Zotero/storage/RFHISTKC/Murphy et al. - 2023 - A Survey on the Attitudes Towards and Perception o.pdf}
}

@article{murphyTestingHypothesisThat1999,
  title = {Testing the Hypothesis That Treatments Have Negligible Effects: {{Minimum-effect}} Tests in the General Linear Model},
  shorttitle = {Testing the Hypothesis That Treatments Have Negligible Effects},
  author = {Murphy, Kevin R. and Myors, Brett},
  year = {1999},
  journal = {Journal of Applied Psychology},
  volume = {84},
  number = {2},
  pages = {234--248},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1854},
  doi = {10.1037/0021-9010.84.2.234},
  abstract = {Researchers are often interested in testing the hypothesis that the effects of treatments, interventions, and so on are negligibly small rather than testing the hypothesis that treatments have no effect whatsoever. A number of procedures for conducting such tests have been suggested but have yet to be widely adopted. In this article, simple methods of testing such minimum-effect hypotheses are illustrated in a variety of applications of the general linear model. Tables and computational routines that can be used in conjunction with the familiar F test to evaluate the hypothesis that the effects of treatments or interventions exceed some minimum level are also provided. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Null Hypothesis Testing,Statistical Tests,Treatment,Treatment Effectiveness Evaluation},
  file = {/Users/cristian/Zotero/storage/MVVB26AS/1999-13895-007.html}
}

@article{murrayColdApplicationsRecovery2015,
  title = {Cold Applications for Recovery in Adolescent Athletes: {{A}} Systematic Review and Meta Analysis},
  author = {Murray, A. and Cardinale, M.},
  year = {2015},
  journal = {Extreme Physiology and Medicine},
  volume = {4},
  number = {1},
  publisher = {BioMed Central Ltd.},
  doi = {10.1186/s13728-015-0035-8}
}

@misc{N3FattyAcid,
  title = {N-3 {{Fatty Acid Supplementation During}} 4 {{Weeks}} of {{Training Leads}} to {{Improved Anaerobic Endurance Capacity}}, but Not {{Maximal Strength}}, {{Speed}}, or {{Power}} in {{Soccer Players}} - {{PubMed}}},
  urldate = {2023-07-10},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/28387540/},
  file = {/Users/cristian/Zotero/storage/76UHPEXQ/28387540.html}
}

@article{nakagawaFarewellBonferroniProblems2004,
  title = {A Farewell to {{Bonferroni}}: The Problems of Low Statistical Power and Publication Bias},
  shorttitle = {A Farewell to {{Bonferroni}}},
  author = {Nakagawa, Shinichi},
  year = {2004},
  month = nov,
  journal = {Behavioral Ecology},
  volume = {15},
  number = {6},
  pages = {1044--1045},
  issn = {1045-2249},
  doi = {10.1093/beheco/arh107},
  urldate = {2024-08-23},
  file = {/Users/cristian/Zotero/storage/VE2VSNPJ/Nakagawa - 2004 - A farewell to Bonferroni the problems of low stat.pdf;/Users/cristian/Zotero/storage/CR6ES65B/206216.html}
}

@article{nakagawaMethodsTestingPublication2022,
  title = {Methods for Testing Publication Bias in Ecological and Evolutionary Meta-Analyses},
  author = {Nakagawa, Shinichi and Lagisz, Malgorzata and Jennions, Michael D. and Koricheva, Julia and Noble, Daniel W. A. and Parker, Timothy H. and {S{\'a}nchez-T{\'o}jar}, Alfredo and Yang, Yefeng and O'Dea, Rose E.},
  year = {2022},
  journal = {Methods in Ecology and Evolution},
  volume = {13},
  number = {1},
  pages = {4--21},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13724},
  urldate = {2025-05-06},
  abstract = {Publication bias threatens the validity of quantitative evidence from meta-analyses as it results in some findings being overrepresented in meta-analytic datasets because they are published more frequently or sooner (e.g. `positive' results). Unfortunately, methods to test for the presence of publication bias, or assess its impact on meta-analytic results, are unsuitable for datasets with high heterogeneity and non-independence, as is common in ecology and evolutionary biology. We first review both classic and emerging publication bias tests (e.g. funnel plots, Egger's regression, cumulative meta-analysis, fail-safe N, trim-and-fill tests, p-curve and selection models), showing that some tests cannot handle heterogeneity, and, more importantly, none of the methods can deal with non-independence. For each method, we estimate current usage in ecology and evolutionary biology, based on a representative sample of 102 meta-analyses published in the last 10 years. Then, we propose a new method using multilevel meta-regression, which can model both heterogeneity and non-independence, by extending existing regression-based methods (i.e. Egger's regression). We describe how our multilevel meta-regression can test not only publication bias, but also time-lag bias, and how it can be supplemented by residual funnel plots. Overall, we provide ecologists and evolutionary biologists with practical recommendations on which methods are appropriate to employ given independent and non-independent effect sizes. No method is ideal, and more simulation studies are required to understand how Type 1 and Type 2 error rates are impacted by complex data structures. Still, the limitations of these methods do not justify ignoring publication bias in ecological and evolutionary meta-analyses.},
  copyright = {{\copyright} 2021 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {decline effect,effective sample size,multilevel meta-analysis,outcome reporting bias,p-hacking,radial plot,selection bias,time-lag bias},
  file = {/Users/cristian/Zotero/storage/NXKG3PW3/Nakagawa et al. - 2022 - Methods for testing publication bias in ecological.pdf}
}

@article{nakagawaQuantitativeEvidenceSynthesis2023,
  title = {Quantitative Evidence Synthesis: A Practical Guide on Meta-Analysis, Meta-Regression, and Publication Bias Tests for Environmental Sciences},
  shorttitle = {Quantitative Evidence Synthesis},
  author = {Nakagawa, Shinichi and Yang, Yefeng and Macartney, Erin L. and Spake, Rebecca and Lagisz, Malgorzata},
  year = {2023},
  month = apr,
  journal = {Environmental Evidence},
  volume = {12},
  number = {1},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-023-00301-6},
  urldate = {2025-05-17},
  abstract = {Meta-analysis is a quantitative way of synthesizing results from multiple studies to obtain reliable evidence of an intervention or phenomenon. Indeed, an increasing number of meta-analyses are conducted in environmental sciences, and resulting meta-analytic evidence is often used in environmental policies and decision-making. We conducted a survey of recent meta-analyses in environmental sciences and found poor standards of current meta-analytic practice and reporting. For example, only\,{\textasciitilde}\,40\% of the 73 reviewed meta-analyses reported heterogeneity (variation among effect sizes beyond sampling error), and publication bias was assessed in fewer than half. Furthermore, although almost all the meta-analyses had multiple effect sizes originating from the same studies, non-independence among effect sizes was considered in only half of the meta-analyses. To improve the implementation of meta-analysis in environmental sciences, we here outline practical guidance for conducting a meta-analysis in environmental sciences. We describe the key concepts of effect size and meta-analysis and detail procedures for fitting multilevel meta-analysis and meta-regression models and performing associated publication bias tests. We demonstrate a clear need for environmental scientists to embrace multilevel meta-analytic models, which explicitly model dependence among effect sizes, rather than the commonly used random-effects models. Further, we discuss how reporting and visual presentations of meta-analytic results can be much improved by following reporting guidelines such as PRISMA-EcoEvo (Preferred Reporting Items for Systematic Reviews and Meta-Analyses for Ecology and Evolutionary Biology). This paper, along with the accompanying online tutorial, serves as a practical guide on conducting a complete set of meta-analytic procedures (i.e., meta-analysis, heterogeneity quantification, meta-regression, publication bias tests and sensitivity analysis) and also as a gateway to more advanced, yet appropriate, methods.},
  keywords = {Hierarchical models,Meta-analysis of variance,Missing data,Multivariate meta-analysis,Network meta-analysis,Robust variance estimation,Spatial dependency,Variance--covariance matrix},
  file = {/Users/cristian/Zotero/storage/3XT29QQD/Nakagawa et al. - 2023 - Quantitative evidence synthesis a practical guide.pdf;/Users/cristian/Zotero/storage/ZG9VTF92/s13750-023-00301-6.html}
}

@book{nationalacademiesofsciencesengineeringandmedicine;policyandglobalaffairs;committeeonscienceengineeringmedicineandpublicpolicy;boardonresearchdataandinformation;divisiononengineeringandphysicalsciences;committeeonappliedandtheoreticalstatistics;boardonmathematicalsciencesandanalytics;divisiononearthandlifestudies;nuclearandradiationstudiesboard;divisionofbehavioralandsocialsciencesandeducation;committeeonnationalstatistics;boardonbehavioralcognitiveandsensorysciences;committeeonreproducibilityandreplicabilityinscienceReproducibilityReplicabilityScience2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine; Policy and Global Affairs; Committee on Science, Engineering, Medicine, and Public Policy; Board on Research Data and Information; Division on Engineering and Physical Sciences; Committee on Applied and Theoretical Statistics; Board on Mathematical Sciences and Analytics; Division on Earth and Life Studies; Nuclear and Radiation Studies Board; Division of Behavioral and Social Sciences and Education; Committee on National Statistics; Board on Behavioral, Cognitive, and Sensory Sciences; Committee on Reproducibility and Replicability in Science}},
  year = {2019},
  publisher = {National Academies Press (US)},
  address = {Washington (DC)},
  urldate = {2023-02-27},
  abstract = {One of the pathways by which the scientific community confirms the validity of a new scientific discovery is by repeating the research that produced it. When a scientific effort fails to independently confirm the computations or results of a previous study, some fear that it may be a symptom of a lack of rigor in science, while others argue that such an observed inconsistency can be an important precursor to new discovery. Concerns about reproducibility and replicability have been expressed in both scientific and popular media. As these concerns came to light, Congress requested that the National Academies of Sciences, Engineering, and Medicine conduct a study to assess the extent of issues related to reproducibility and replicability and to offer recommendations for improving rigor and transparency in scientific research. Reproducibility and Replicability in Science defines reproducibility and replicability and examines the factors that may lead to non-reproducibility and non-replicability in research. Unlike the typical expectation of reproducibility between two computations, expectations about replicability are more nuanced, and in some cases a lack of replicability can aid the process of scientific discovery. This report provides recommendations to researchers, academic institutions, journals, and funders on steps they can take to improve reproducibility and replicability in science.},
  copyright = {Copyright 2019 by the National Academy of Sciences. All rights reserved.},
  isbn = {978-0-309-48616-3},
  langid = {english},
  lccn = {NBK547537},
  pmid = {31596559}
}

@book{nationalacademiesofsciencesUnderstandingReproducibilityReplicability2019,
  title = {Understanding {{Reproducibility}} and {{Replicability}}},
  author = {{National Academies of Sciences}, Engineering and Affairs, Policy {and} Global and {Committee on Science}, Engineering and and Information, Board on Research Data and Sciences, Division on Engineering {and} Physical and Statistics, Committee on Applied {and} Theoretical and and Analytics, Board on Mathematical Sciences and Studies, Division on Earth {and} Life and Board, Nuclear {and} Radiation Studies and and Education, Division of Behavioral {and} Social Sciences and Statistics, Committee on National and {Board on Behavioral}, Cognitive and in Science, Committee on Reproducibility {and} Replicability},
  year = {2019},
  month = may,
  journal = {Reproducibility and Replicability in Science},
  publisher = {National Academies Press (US)},
  urldate = {2023-02-27},
  abstract = {Scientific research has evolved from an activity mainly undertaken by individuals operating in a few locations to many teams, large communities, and complex organizations involving hundreds to thousands of individuals worldwide. In the 17th century, scientists would communicate through letters and were able to understand and assimilate major developments across all the emerging major disciplines. In 2016---the most recent year for which data are available---more than 2,295,000 scientific and engineering research articles were published worldwide (National Science Foundation, 2018e). In addition, the number of scientific and engineering fields and subfields of research is large and has greatly expanded in recent years, especially in fields that intersect disciplines (e.g., biophysics); more than 230 distinct fields and subfields can now be identified. The published literature is so voluminous and specialized that some researchers look to information retrieval, machine learning, and artificial intelligence techniques to track and apprehend the important work in their own fields.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/2R9588QS/NBK547546.html}
}

@article{nazaroffSensoryEnhancementWarmup2022,
  title = {Sensory Enhancement of Warm-up Amplifies Subsequent Grip Strength and Cycling Performance},
  author = {Nazaroff, Benjamin M. and Pearcey, Gregory E. P. and Munro, Bridget and Zehr, E. Paul},
  year = {2022},
  month = jul,
  journal = {European Journal of Applied Physiology},
  volume = {122},
  number = {7},
  pages = {1695--1707},
  issn = {1439-6327},
  doi = {10.1007/s00421-022-04952-0},
  abstract = {PURPOSE: In sport and exercise, warm-ups induce various physiological changes that facilitate subsequent performance. We have shown that delivering patterned stimulation to cutaneous afferents during sprint cycling mitigates fatigue-related decrements in performance, and that repeated sensory stimulation amplifies spinal reflex excitability. Therefore, the purpose of this study was to assess whether sensory enhancement of warm-up would affect subsequent high-intensity arm cycling performance. METHODS: Participants completed three experimental sessions, in which they randomly performed either a control, stim, or sleeve warm-up condition prior to maximal duration arm cycling. During the control condition, warmup consisted of low-intensity arm cycling for 15~min. The stim condition was the same, except they received alternating pulses (400~ms, 50~Hz) of stimulation just above their perceptual threshold to the wrists during warm-up. The third condition required participants to wear custom fabricated compression sleeves around the elbow during warm-up. Grip strength and spinal reflex excitability were measured before and after each warm-up and fatigue protocol, which required participants to arm cycle at 85\% of peak power output until they reached volitional fatigue. Peak power output was determined during an incremental test at minimum 72~h prior to the first session. RESULTS: Both sensory enhanced warm-up conditions amplified subsequent high-intensity arm cycling performance by\,{\textasciitilde}\,30\%. Additionally, the stim and sleeve warm-up conditions yielded improvements in grip strength (increased by\,{\textasciitilde}\,5\%) immediately after the sensory enhanced warm-ups. Ergogenic benefits from the sensory enhanced warm-up conditions did not differ between one another. CONCLUSION: These findings demonstrate that enhanced sensory input during warm-up can elicit improvements in both maximal and submaximal performance measures.},
  langid = {english},
  pmid = {35471257},
  keywords = {{Muscle, Skeletal},Bicycling,Compression,Exercise,Fatigue,Hand Strength,Humans,Sensory enhancement,Sensory stimulation,Warm-up,Warm-Up Exercise}
}

@article{nemetProteinAminoAcid2007,
  title = {Protein and Amino Acid Supplementation in Sport},
  author = {Nemet, D. and Eliakim, A.},
  year = {2007},
  journal = {International SportMed Journal},
  volume = {8},
  number = {1},
  pages = {11--23}
}

@article{neymanProblemInductiveInference1955,
  title = {The Problem of Inductive Inference},
  shorttitle = {The Problem of Inductive Inference},
  author = {Neyman, Jerzy},
  year = {1955},
  month = feb,
  journal = {Communications on Pure and Applied Mathematics},
  volume = {8},
  number = {1},
  pages = {13--45},
  issn = {00103640},
  doi = {10.1002/cpa.3160080103},
  urldate = {2022-04-20},
  langid = {english}
}

@article{neymanProblemMostEfficient1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  author = {Neyman, Jerzy and Pearson, Egon Sharpe},
  year = {1933},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {231},
  number = {694-706},
  pages = {289--337},
  publisher = {Royal Society},
  doi = {10.1098/rsta.1933.0009},
  urldate = {2022-10-29},
  abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible ``causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a ``system'' or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character, x, of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and vice versa. Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character, x, of the observed facts were properly chosen---were, in fact, a character which he terms ``en quelque sorte remarquable.''},
  file = {/Users/cristian/Zotero/storage/UVBN9T5T/Neyman et al. - 1933 - IX. On the problem of the most efficient tests of .pdf}
}

@article{nickersonConfirmationBiasUbiquitous1998,
  title = {Confirmation Bias: {{A}} Ubiquitous Phenomenon in Many Guises},
  shorttitle = {Confirmation Bias},
  author = {Nickerson, Raymond S.},
  year = {1998},
  journal = {Review of General Psychology},
  volume = {2},
  number = {2},
  pages = {175--220},
  publisher = {Educational Publishing Foundation},
  address = {US},
  issn = {1939-1552},
  doi = {10.1037/1089-2680.2.2.175},
  abstract = {Confirmation bias, as the term is typically used in the psychological literature, connotes the seeking or interpreting of evidence in ways that are partial to existing beliefs, expectations, or a hypothesis in hand. The author reviews evidence of such a bias in a variety of guises and gives examples of its operation in several practical contexts. Possible explanations are considered, and the question of its utility or disutility is discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Reasoning,Response Bias},
  file = {/Users/cristian/Zotero/storage/6GLATGN9/Nickerson - 1998 - Confirmation bias A ubiquitous phenomenon in many.pdf;/Users/cristian/Zotero/storage/8DE2I6E9/2018-70006-003.html}
}

@article{nickersonNullHypothesisSignificance2000,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  year = {2000},
  journal = {Psychological Methods},
  volume = {5},
  pages = {241--301},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.5.2.241},
  abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/P28F45R9/2000-07827-007.html}
}

@article{nieuwenhuisErroneousAnalysesInteractions2011,
  ids = {nieuwenhuisErroneousAnalysesInteractions2011a},
  title = {Erroneous Analyses of Interactions in Neuroscience: A Problem of Significance},
  shorttitle = {Erroneous Analyses of Interactions in Neuroscience},
  author = {Nieuwenhuis, Sander and Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  year = {2011},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {14},
  number = {9},
  pages = {1105--1107},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.2886},
  urldate = {2023-08-11},
  abstract = {The authors analyze a large corpus of the neuroscience literature and demonstrate that nearly half of the published studies considered incorrectly compared effect sizes by comparing their significance levels.},
  copyright = {2011 Springer Nature America, Inc.},
  langid = {english},
  pmid = {21878926},
  keywords = {Statistics},
  file = {/Users/cristian/Zotero/storage/69KS2KZI/Nieuwenhuis et al. - 2011 - Erroneous analyses of interactions in neuroscience.pdf}
}

@article{nivenReproducibilityClinicalResearch2018,
  title = {Reproducibility of Clinical Research in Critical Care: A Scoping Review},
  shorttitle = {Reproducibility of Clinical Research in Critical Care},
  author = {Niven, Daniel J. and McCormick, T. Jared and Straus, Sharon E. and Hemmelgarn, Brenda R. and Jeffs, Lianne and Barnes, Tavish R. M. and Stelfox, Henry T.},
  year = {2018},
  month = feb,
  journal = {BMC Medicine},
  volume = {16},
  number = {1},
  pages = {26},
  issn = {1741-7015},
  doi = {10.1186/s12916-018-1018-6},
  urldate = {2021-01-25},
  abstract = {The ability to reproduce experiments is a defining principle of science. Reproducibility of clinical research has received relatively little scientific attention. However, it is important as it may inform clinical practice, research agendas, and the design of future studies.},
  keywords = {Adoption,Critical care,De-adoption,ICU,Intensive care,Replication research,Reproducibility},
  file = {/Users/cristian/Zotero/storage/5UDS3GHA/Niven et al. - 2018 - Reproducibility of clinical research in critical c.pdf;/Users/cristian/Zotero/storage/QZNL9XG8/s12916-018-1018-6.html}
}

@misc{NoInfluenceAcute,
  title = {No {{Influence}} of {{Acute Moderate Normobaric Hypoxia}} on {{Performance}} and {{Blood Lactate Concentration Responses}} to {{Repeated Wingates}} in: {{International Journal}} of {{Sports Physiology}} and {{Performance Volume}} 16 {{Issue}} 1 (2020)},
  urldate = {2025-03-12},
  howpublished = {https://journals.humankinetics.com/view/journals/ijspp/16/1/article-p154.xml},
  file = {/Users/cristian/Zotero/storage/Z9MRY3MV/article-p154.html}
}

@misc{NoInfluenceAcutea,
  title = {No {{Influence}} of {{Acute Moderate Normobaric Hypoxia}} on {{Performance}} and {{Blood Lactate Concentration Responses}} to {{Repeated Wingates}} - {{Google Search}}},
  urldate = {2025-03-27},
  howpublished = {https://www.google.com/search?client=safari\&rls=en\&q=No+Influence+of+Acute+Moderate+Normobaric+Hypoxia+on+Performance+and+Blood+Lactate+Concentration+Responses+to+Repeated+Wingates\&ie=UTF-8\&oe=UTF-8},
  file = {/Users/cristian/Zotero/storage/XRTG55DQ/search.html}
}

@misc{NoInfluenceAcuteb,
  title = {No {{Influence}} of {{Acute Moderate Normobaric Hypoxia}} on {{Performance}} and {{Blood Lactate Concentration Responses}} to {{Repeated Wingates}} - {{PubMed}}},
  urldate = {2025-03-27},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/33120358/},
  file = {/Users/cristian/Zotero/storage/W7NATIXL/33120358.html}
}

@article{nordPowerupReanalysisPower2017,
  title = {Power-up: {{A Reanalysis}} of '{{Power Failure}}' in {{Neuroscience Using Mixture Modeling}}},
  shorttitle = {Power-Up},
  author = {Nord, Camilla L. and Valton, Vincent and Wood, John and Roiser, Jonathan P.},
  year = {2017},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {37},
  number = {34},
  pages = {8051--8061},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3592-16.2017},
  urldate = {2021-02-15},
  abstract = {Recently, evidence for endemically low statistical power has cast neuroscience findings into doubt. If low statistical power plagues neuroscience, then this reduces confidence in the reported effects. However, if statistical power is not uniformly low, then such blanket mistrust might not be warranted. Here, we provide a different perspective on this issue, analyzing data from an influential study reporting a median power of 21\% across 49 meta-analyses (Button et al., 2013). We demonstrate, using Gaussian mixture modeling, that the sample of 730 studies included in that analysis comprises several subcomponents so the use of a single summary statistic is insufficient to characterize the nature of the distribution. We find that statistical power is extremely low for studies included in meta-analyses that reported a null result and that it varies substantially across subfields of neuroscience, with particularly low power in candidate gene association studies. Therefore, whereas power in neuroscience remains a critical issue, the notion that studies are systematically underpowered is not the full story: low power is far from a universal problem. SIGNIFICANCE STATEMENT Recently, researchers across the biomedical and psychological sciences have become concerned with the reliability of results. One marker for reliability is statistical power: the probability of finding a statistically significant result given that the effect exists. Previous evidence suggests that statistical power is low across the field of neuroscience. Our results present a more comprehensive picture of statistical power in neuroscience: on average, studies are indeed underpowered---some very seriously so---but many studies show acceptable or even exemplary statistical power. We show that this heterogeneity in statistical power is common across most subfields in neuroscience. This new, more nuanced picture of statistical power in neuroscience could affect not only scientific understanding, but potentially policy and funding decisions for neuroscience research.},
  chapter = {Research Articles},
  copyright = {Copyright {\copyright} 2017 Nord, Valton et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution License Creative Commons Attribution 4.0 International, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  langid = {english},
  pmid = {28706080},
  keywords = {neuroscience,power,statistics},
  file = {/Users/cristian/Zotero/storage/L2RI5BT8/Nord et al. - 2017 - Power-up A Reanalysis of 'Power Failure' in Neuro.pdf;/Users/cristian/Zotero/storage/9REL4R8J/8051.html}
}

@article{normanInterpretationChangesHealthrelated2003,
  title = {Interpretation of {{Changes}} in {{Health-related Quality}} of {{Life}}: {{The Remarkable Universality}} of {{Half}} a {{Standard Deviation}}},
  shorttitle = {Interpretation of {{Changes}} in {{Health-related Quality}} of {{Life}}},
  author = {Norman, Geoffrey R. and Sloan, Jeff A. and Wyrwich, Kathleen W.},
  year = {2003},
  month = may,
  journal = {Medical Care},
  volume = {41},
  number = {5},
  pages = {582},
  issn = {0025-7079},
  doi = {10.1097/01.MLR.0000062554.74615.4C},
  urldate = {2023-10-18},
  abstract = {Background. ~           A number of studies have computed the minimally important difference (MID) for health-related quality of life instruments.           Objective. ~           To determine whether there is consistency in the magnitude of MID estimates from different instruments.           Methods. ~           We conducted a systematic review of the literature to identify studies that computed an MID and contained sufficient information to compute an effect size (ES). Thirty-eight studies fulfilled the criteria, resulting in 62 ESs.           Results. ~           For all but 6 studies, the MID estimates were close to one half a SD (mean = 0.495, SD = 0.155). There was no consistent relationship with factors such as disease-specific or generic instrument or the number of response options. Negative changes were not associated with larger ESs. Population-based estimation procedures and brief follow-up were associated with smaller ESs, and acute conditions with larger ESs. An explanation for this consistency is that research in psychology has shown that the limit of people's ability to discriminate over a wide range of tasks is approximately 1 part in 7, which is very close to half a SD.           Conclusion. ~           In most circumstances, the threshold of discrimination for changes in health-related quality of life for chronic diseases appears to be approximately half a SD.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/XEPMAA2R/interpretation_of_changes_in_health_related.4.html}
}

@article{northeastEffectCreatineSupplementation2021,
  title = {The Effect of Creatine Supplementation on Markers of Exercise-Induced Muscle Damage: {{A}} Systematic Review and Meta-Analysis of Human Intervention Trials},
  author = {Northeast, B. and Clifford, T.},
  year = {2021},
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {31},
  number = {3},
  pages = {276--291},
  publisher = {Human Kinetics Publishers Inc.},
  doi = {10.1123/IJSNEM.2020-0282}
}

@article{nosekMakingSenseReplications2017,
  title = {Making Sense of Replications},
  author = {Nosek, Brian A and Errington, Timothy M},
  year = {2017},
  month = jan,
  journal = {eLife},
  volume = {6},
  pages = {e233283},
  doi = {10.7554/eLife.23383},
  abstract = {The first results from the Reproducibility Project: Cancer Biology suggest that there is scope for improving reproducibility in pre-clinical cancer research.},
  keywords = {metascience,methodology,open science,replication,reproducibility,Reproducibility Project: Cancer Biology},
  file = {/Users/cristian/Zotero/storage/2PBYML6A/Nosek and Errington - 2017 - Making sense of replications.pdf;/Users/cristian/Zotero/storage/SN3J47TB/Nosek and Errington - 2017 - Making sense of replications.pdf}
}

@article{nosekPreregistrationHardWorthwhile2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2019.07.009},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  langid = {english},
  pmid = {31421987},
  keywords = {confirmatory research,exploratory research,Humans,preregistration,Registries,reproducibility,Reproducibility of Results,Research,Research Design,transparency},
  file = {/Users/cristian/Zotero/storage/KBTDNBBP/Nosek et al. - 2019 - Preregistration Is Hard, And Worthwhile.pdf}
}

@article{nosekPreregistrationRevolution2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  urldate = {2022-01-13},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {{\copyright} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration},
  file = {/Users/cristian/Zotero/storage/XHCTS8TW/Nosek et al. - 2018 - The preregistration revolution.pdf;/Users/cristian/Zotero/storage/P8MWGWGT/2600.html}
}

@article{nosekPromotingOpenResearch2015,
  ids = {nosekPromotingOpenResearch2015a},
  title = {Promoting an Open Research Culture: {{Author}} Guidelines for Journals Could Help to Promote Transparency, Openness, and Reproducibility},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. A. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1422--1425},
  publisher = {American Assn for the Advancement of Science},
  address = {US},
  doi = {10.1126/science.aab2374},
  abstract = {This article discusses the promotion of an open research culture. Transparency, openness, and reproducibility are readily recognized as vital features of science. When asked, most scientists embrace these features as disciplinary norms and values. Therefore, one might expect that these valued features would be routine in daily practice. Yet, a growing body of evidence suggests that this is not the case. In the present reward system, emphasis on innovation may undermine practices that support verification. The situation is a classic collective action problem. Many individual researchers lack strong incentives to be more transparent, even though the credibility of science would benefit if everyone were more transparent. The journal article is central to the research communication process. Guidelines for authors define what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. Scientists recognize the value of transparency, openness, and reproducibility. Improvement of journal policies can help those values become more evident in daily practice and ultimately improve the public trust in science, and science itself. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  pmcid = {PMC4550299},
  pmid = {26113702},
  keywords = {Experimentation,Innovation,Methodology,Rewards,Sciences,Scientists},
  file = {/Users/cristian/Zotero/storage/4J5X9GKQ/Nosek et al. - 2015 - Promoting an open research culture.pdf;/Users/cristian/Zotero/storage/A4ST53W7/Nosek et al. - 2015 - Promoting an open research culture Author guideli.pdf;/Users/cristian/Zotero/storage/GXUVH3DH/2015-29173-003.html}
}

@article{nosekRegisteredReportsMethod2014,
  title = {Registered Reports: {{A}} Method to Increase the Credibility of Published Results.},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  doi = {10.1027/1864-9335/a000192},
  file = {/Users/cristian/Zotero/storage/A8XSDE67/Nosek and Lakens - Registered reports A method to increase the credi.pdf;/Users/cristian/Zotero/storage/T5JFHB4E/2014-20922-001.html}
}

@article{nosekReplicabilityRobustnessReproducibility2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Struhl, Melissa Kline and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {719--748},
  doi = {10.1146/annurev-psych-020821-114157},
  abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress. Expected final online publication date for the Annual Review of Psychology, Volume 73 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  pmid = {34665669},
  file = {/Users/cristian/Zotero/storage/8YYETN32/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}

@article{nosekScientificUtopiaII2012,
  title = {Scientific {{Utopia}}: {{II}}. {{Restructuring Incentives}} and {{Practices}} to {{Promote Truth Over Publishability}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {615--631},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691612459058},
  urldate = {2021-09-29},
  abstract = {An academic scientist's professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive---getting it right---competitive with the more tangible and concrete incentive---getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
  langid = {english},
  keywords = {false positives,incentives,methodology,motivated reasoning,replication},
  file = {/Users/cristian/Zotero/storage/AVAICZ9C/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf}
}

@article{nosekWhatReplication2020,
  title = {What Is Replication?},
  author = {Nosek, Brian A. and Errington, Timothy M.},
  year = {2020},
  journal = {PLOS Biology},
  volume = {18},
  number = {3},
  pages = {e3000691},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pbio.3000691},
  abstract = {Credibility of scientific claims is established with evidence for their replicability using new data. According to common understanding, replication is repeating a study's procedure and observing whether the prior finding recurs. This definition is intuitive, easy to apply, and incorrect. We propose that replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research. This definition reduces emphasis on operational characteristics of the study and increases emphasis on the interpretation of possible outcomes. The purpose of replication is to advance theory by confronting existing understanding with new evidence. Ironically, the value of replication may be strongest when existing understanding is weakest. Successful replication provides evidence of generalizability across the conditions that inevitably differ from the original study; Unsuccessful replication indicates that the reliability of the finding may be more constrained than recognized previously. Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.},
  langid = {english},
  keywords = {Acetylcholine,Elections,Frogs,Obesity,Open data,Philippines,Reproducibility,Social sciences},
  file = {/Users/cristian/Zotero/storage/E3PIIPIY/Nosek and Errington - 2020 - What is replication.pdf;/Users/cristian/Zotero/storage/TC695ZHI/article.html}
}

@article{nuijten_prevalencestatisticalreporting_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985--2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2015},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2}
}

@article{nuijten_prevalencestatisticalreporting_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985--2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2015},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2}
}

@article{nuijtenEffectSizesPower2020,
  title = {Effect {{Sizes}}, {{Power}}, and {{Biases}} in {{Intelligence Research}}: {{A Meta-Meta-Analysis}}},
  shorttitle = {Effect {{Sizes}}, {{Power}}, and {{Biases}} in {{Intelligence Research}}},
  author = {Nuijten, Mich{\`e}le B. and {van Assen}, Marcel A. L. M. and Augusteijn, Hilde E. M. and Crompvoets, Elise A. V. and Wicherts, Jelte M.},
  year = {2020},
  month = oct,
  journal = {Journal of Intelligence},
  volume = {8},
  number = {4},
  pages = {E36},
  issn = {2079-3200},
  doi = {10.3390/jintelligence8040036},
  abstract = {In this meta-study, we analyzed 2442 effect sizes from 131 meta-analyses in intelligence research, published from 1984 to 2014, to estimate the average effect size, median power, and evidence for bias. We found that the average effect size in intelligence research was a Pearson's correlation of 0.26, and the median sample size was 60. Furthermore, across primary studies, we found a median power of 11.9\% to detect a small effect, 54.5\% to detect a medium effect, and 93.9\% to detect a large effect. We documented differences in average effect size and median estimated power between different types of intelligence studies (correlational studies, studies of group differences, experiments, toxicology, and behavior genetics). On average, across all meta-analyses (but not in every meta-analysis), we found evidence for small-study effects, potentially indicating publication bias and overestimated effects. We found no differences in small-study effects between different study types. We also found no convincing evidence for the decline effect, US effect, or citation bias across meta-analyses. We concluded that intelligence research does show signs of low power and publication bias, but that these problems seem less severe than in many other scientific fields.},
  langid = {english},
  pmcid = {PMC7720125},
  pmid = {33023250},
  keywords = {bias,effect size,intelligence,meta-meta-analysis,meta-science,power},
  file = {/Users/cristian/Zotero/storage/DHIWJB8B/Nuijten et al. - 2020 - Effect Sizes, Power, and Biases in Intelligence Re.pdf}
}

@article{nuijtenPrevalenceStatisticalReporting2016,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985--2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2016},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {48},
  pages = {1205--1225},
  doi = {10.3758/s13428-015-0664-2},
  abstract = {This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package ``statcheck.'' statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called ``co-pilot model,'' and to use statcheck to flag possible inconsistencies in one's own manuscript or during the review process.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/S7EBT5XD/Nuijten et al. - 2016 - The prevalence of statistical reporting errors in .pdf}
}

@article{nuijtenPreventingStatisticalErrors,
  title = {Preventing Statistical Errors in Scientific Journals},
  author = {Nuijten, Mich{\`e}le B},
  journal = {European Science Editing},
  pages = {3},
  abstract = {There is evidence of a high prevalence of statistical reporting errors in psychology and other scientific fields. These errors display a systematic preference for statistically significant results, distorting the scientific literature. There are several possible causes for this systematic error prevalence, with publication bias being the most prominent one. Journal editors could play an important role in preventing statistical errors in published literature. Concrete solutions entail encouraging sharing data and preregistration, and using the automated procedure ``statcheck'' to check manuscripts for errors.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/6NFNUFXS/Nuijten - Preventing statistical errors in scientific journa.pdf}
}

@article{nuijtenStatcheckAutomaticallyDetect2020,
  title = {``Statcheck'': {{Automatically}} Detect Statistical Reporting Inconsistencies to Increase Reproducibility of Meta-Analyses},
  shorttitle = {``Statcheck''},
  author = {Nuijten, Mich{\`e}le B. and Polanin, Joshua R.},
  year = {2020},
  journal = {Research Synthesis Methods},
  volume = {11},
  number = {5},
  pages = {574--579},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1408},
  urldate = {2022-10-10},
  abstract = {We present the R package and web app statcheck to automatically detect statistical reporting inconsistencies in primary studies and meta-analyses. Previous research has shown a high prevalence of reported p-values that are inconsistent - meaning a re-calculated p-value, based on the reported test statistic and degrees of freedom, does not match the author-reported p-value. Such inconsistencies affect the reproducibility and evidential value of published findings. The tool statcheck can help researchers to identify statistical inconsistencies so that they may correct them. In this paper, we provide an overview of the prevalence and consequences of statistical reporting inconsistencies. We also discuss the tool statcheck in more detail and give an example of how it can be used in a meta-analysis. We end with some recommendations concerning the use of statcheck in meta-analyses and make a case for better reporting standards of statistical results.},
  langid = {english},
  keywords = {meta-analysis,reporting standards,reproducibility,statcheck,statistical error},
  file = {/Users/cristian/Zotero/storage/HX9ESVEF/Nuijten and Polanin - 2020 - statcheck Automatically detect statistical repo.pdf;/Users/cristian/Zotero/storage/Y7KE6DLZ/jrsm.html}
}

@article{nuijtenVerifyOriginalResults2018,
  title = {Verify Original Results through Reanalysis before Replicating},
  author = {Nuijten, Mich{\`e}le B. and Bakker, Marjan and Maassen, Esther and Wicherts, Jelte M.},
  year = {2018},
  journal = {Behavioral and Brain Sciences},
  edition = {2018/07/27},
  volume = {41},
  pages = {e143},
  publisher = {Cambridge University Press},
  issn = {0140-525X},
  doi = {10.1017/S0140525X18000791},
  abstract = {In determining the need to directly replicate, it is crucial to first verify the original results through independent reanalysis of the data. Original results that appear erroneous and that cannot be reproduced by reanalysis offer little evidence to begin with, thereby diminishing the need to replicate. Sharing data and scripts is essential to ensure reproducibility.}
}

@article{obelsAnalysisOpenData2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  urldate = {2023-02-28},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/SZ7LUHZP/Obels et al. - 2020 - Analysis of Open Data and Computational Reproducib.pdf}
}

@article{oberauerAddressingTheoryCrisis2019,
  title = {Addressing the Theory Crisis in Psychology},
  author = {Oberauer, Klaus and Lewandowsky, Stephan},
  year = {2019},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {5},
  pages = {1596--1618},
  doi = {10.3758/s13423-019-01645-2},
  abstract = {A worrying number of psychological findings are not replicable. Diagnoses of the causes of this ``replication crisis,'' and recommendations to address it, have nearly exclusively focused on methods of data collection, analysis, and reporting. We argue that a further cause of poor replicability is the often weak logical link between theories and their empirical tests. We propose a distinction between discovery-oriented and theory-testing research. In discovery-oriented research, theories do not strongly imply hypotheses by which they can be tested, but rather define a search space for the discovery of effects that would support them. Failures to find these effects do not question the theory. This endeavor necessarily engenders a high risk of Type I errors---that is, publication of findings that will not replicate. Theory-testing research, by contrast, relies on theories that strongly imply hypotheses, such that disconfirmation of the hypothesis provides evidence against the theory. Theory-testing research engenders a smaller risk of Type I errors. A strong link between theories and hypotheses is best achieved by formalizing theories as computational models. We critically revisit recommendations for addressing the ``replication crisis,'' including the proposal to distinguish exploratory from confirmatory research, and the preregistration of hypotheses and analysis plans.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/G4FL457H/Oberauer and Lewandowsky - 2019 - Addressing the theory crisis in psychology.pdf}
}

@article{obryanMultiingredientProteinSupplements2019,
  title = {Do Multi-Ingredient Protein Supplements Augment Resistance Training-Induced Gains in Skeletal Muscle Mass and Strength? {{A}} Systematic Review and Meta-Analysis of 35 Trials},
  author = {O'Bryan, K.R. and Doering, T.M. and Morton, R.W. and Coffey, V.G. and Phillips, S.M. and Cox, G.R.},
  year = {2019},
  journal = {British Journal of Sports Medicine},
  publisher = {BMJ Publishing Group},
  doi = {10.1136/bjsports-2018-099889}
}

@misc{ObtainingEvidenceNo,
  title = {Obtaining {{Evidence}} for {{No Effect}} {\textbar} {{Collabra}}: {{Psychology}} {\textbar} {{University}} of {{California Press}}},
  urldate = {2024-02-01},
  howpublished = {https://online.ucpress.edu/collabra/article/7/1/28202/118660/Obtaining-Evidence-for-No-Effect},
  file = {/Users/cristian/Zotero/storage/8AG4UQRM/Obtaining-Evidence-for-No-Effect.html}
}

@article{oliveraNonantimuscarinicTreatmentOveractive2016,
  title = {Nonantimuscarinic Treatment for Overactive Bladder: {{A}} Systematic Review},
  author = {Olivera, C.K. and Meriwether, K. and {El-Nashar}, S. and Grimes, C.L. and Chen, C.C.G. and Orejuela, F. and Antosh, D. and Gleason, J. and {Kim-Fine}, S. and Wheeler, T. and McFadden, B. and Balk, E.M. and Murphy, M.},
  year = {2016},
  journal = {American Journal of Obstetrics and Gynecology},
  volume = {215},
  number = {1},
  pages = {34--57},
  publisher = {Mosby Inc.},
  doi = {10.1016/j.ajog.2016.01.156}
}

@article{olsson-collentineHeterogeneityDirectReplications2020,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  journal = {Psychological Bulletin},
  volume = {146},
  pages = {922--940},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the ``verbal overshadowing'' effect, and various priming effects such as ``anchoring'' effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Experimental Laboratories,Experimental Replication,Homogeneity of Variance,Meta Analysis,Priming,Psychology,Social Psychology,Stroop Effect},
  file = {/Users/cristian/Zotero/storage/9T9ZY43V/Olsson-Collentine et al. - 2020 - Heterogeneity in direct replications in psychology.pdf;/Users/cristian/Zotero/storage/ABLP2DGH/2020-53959-001.html}
}

@misc{olsson-collentinePreprintMetaAnalyzingMultiverse2021,
  title = {Preprint - {{Meta-Analyzing}} the {{Multiverse}}: {{A Peek Under}} the {{Hood}} of {{Selective Reporting}}},
  shorttitle = {Preprint - {{Meta-Analyzing}} the {{Multiverse}}},
  author = {{Olsson-Collentine}, Anton and van Aert, Robbie C. M. and Bakker, Marjan and Wicherts, Jelte},
  year = {2021},
  month = apr,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/43yae},
  urldate = {2021-09-22},
  abstract = {There are arbitrary decisions to be made (i.e., researcher degrees of freedom) in the execution and reporting of most research. These decisions allow for many possible outcomes from a single study. Selective reporting of results from this `multiverse' of outcomes, whether intentional (\_p\_-hacking) or not, can lead to inflated effect size estimates and false positive results in the literature. In this study, we examine and illustrate the consequences of researcher degrees of freedom in primary research, both for primary outcomes and for subsequent meta-analyses. We used a set of 10 preregistered multi-lab direct replication projects from psychology (Registered Replication Reports) with a total of 14 primary outcome variables, 236 labs and 37,602 participants. By exploiting researcher degrees of freedom in each project, we were able to compute between 3,840 and 2,621,440 outcomes per lab. We show that researcher degrees of freedom in primary research can cause substantial variability in effect size that we denote the Underlying Multiverse Variability (UMV). In our data, the median UMV across labs was 0.1 standard deviations (interquartile range = 0.09 -- 0.15). In one extreme case, the effect size estimate could change by \_d\_ = 1.27, evidence that \_p\_-hacking in some (rare) cases can provide support for almost any conclusion. We also show that researcher degrees of freedom in primary research provide another source of uncertainty in meta-analysis beyond those usually estimated. This would not be a large concern for meta-analysis if researchers made all arbitrary decisions at random. However, emulating selective reporting of lab results led to inflation of meta-analytic average effect size estimates in our data by as much as 0.1 - 0.48 standard deviations, depending to a large degree on the number of possible outcomes at the lab level (i.e., multiverse size). Our results illustrate the importance of making research decisions transparent (e.g., through preregistration and multiverse analysis), evaluating studies for selective reporting, and whenever feasible  making raw data available.},
  keywords = {Meta-science,Social and Behavioral Sciences},
  file = {/Users/cristian/Zotero/storage/QMLRRQFM/Olsson-Collentine et al. - 2021 - Preprint - Meta-Analyzing the Multiverse A Peek U.pdf}
}

@article{olsson-collentinePrevalenceMarginallySignificant2019,
  title = {The {{Prevalence}} of {{Marginally Significant Results}} in {{Psychology Over Time}}},
  author = {{Olsson-Collentine}, Anton and {van Assen}, Marcel A. L. M. and Hartgerink, Chris H. J.},
  year = {2019},
  month = apr,
  journal = {Psychological Science},
  volume = {30},
  number = {4},
  pages = {576--586},
  issn = {1467-9280},
  doi = {10.1177/0956797619830326},
  abstract = {We examined the percentage of p values (.05 {$<$} p {$\leq$} .10) reported as marginally significant in 44,200 articles, across nine psychology disciplines, published in 70 journals belonging to the American Psychological Association between 1985 and 2016. Using regular expressions, we extracted 42,504 p values between .05 and .10. Almost 40\% of p values in this range were reported as marginally significant, although there were considerable differences between disciplines. The practice is most common in organizational psychology (45.4\%) and least common in clinical psychology (30.1\%). Contrary to what was reported by previous researchers, our results showed no evidence of an increasing trend in any discipline; in all disciplines, the percentage of p values reported as marginally significant was decreasing or constant over time. We recommend against reporting these results as marginally significant because of the low evidential value of p values between .05 and .10.},
  langid = {english},
  pmcid = {PMC6472145},
  pmid = {30789796},
  keywords = {{Psychology, Clinical},{Societies, Scientific},APA,Bias,Humans,marginal significance,null-hypothesis significance testing,open data,open materials,over time,Prevalence,Psychology,Research,values},
  file = {/Users/cristian/Zotero/storage/Q9CLTFTY/Olsson-Collentine et al. - 2019 - The Prevalence of Marginally Significant Results i.pdf}
}

@article{OntheReproducibilityofPowerAnalysesinMotorBehaviorResearch,
  title = {On the Reproducibility of Power Analyses in Motor Behavior Research},
  author = {McKay, Brad and Bacelar, Mariane F.B. and Carter, Michael J.},
  year = {2023},
  journal = {Journal of Motor Learning and Development},
  volume = {11},
  number = {1},
  pages = {29--44},
  publisher = {Human Kinetics},
  address = {Champaign IL, USA},
  doi = {10.1123/jmld.2022-0061}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  pmid = {26315443},
  keywords = {Behavioral Research,Confidence Intervals,Periodicals as Topic,Psychology,Publication Bias,Publishing,Reproducibility of Results,Research,Research Design},
  file = {/Users/cristian/Zotero/storage/SC9BGVNR/Open Science Collaboration - 2015 - PSYCHOLOGY. Estimating the reproducibility of psyc.pdf}
}

@article{orssattoNeuromuscularDeterminantsExplosive2020,
  title = {Neuromuscular Determinants of Explosive Torque: {{Differences}} among Strength-Trained and Untrained Young and Older Men},
  shorttitle = {Neuromuscular Determinants of Explosive Torque},
  author = {Orssatto, Lucas B. R. and Wiest, Matheus J. and Moura, Bruno M. and Collins, David F. and Diefenthaeler, Fernando},
  year = {2020},
  month = nov,
  journal = {Scandinavian Journal of Medicine \& Science in Sports},
  volume = {30},
  number = {11},
  pages = {2092--2100},
  issn = {1600-0838},
  doi = {10.1111/sms.13788},
  abstract = {This study compared the differences in neural and muscular mechanisms related to explosive torque in chronically strength-trained young and older men ({$>$}5~years). Fifty-four participants were allocated into four groups according to age and strength training level: older untrained (n~=~14; 65.6~{\textpm}~2.9~years), older trained (n~=~12; 63.6~{\textpm}~3.8~years), young untrained (n~=~14; 26.2~{\textpm}~3.7~years), and young trained (n~=~14; 26.7~{\textpm}~3.4~years). Knee extension isometric voluntary explosive torque (absolute and normalized as a percentage of maximal voluntary torque) was assessed at the beginning of the contraction (ie, 50, 100, and 150~ms-T50, T100, and T150, respectively), and surface electromyogram (sEMG) amplitude (normalized as a percentage of sEMG recorded during maximal voluntary isometric contraction) at 0-50, 50-100, and 100-150 time windows. Supramaximal electrically evoked T50 was assessed with octet trains delivered to the femoral nerve (8 pulses at 300~Hz). Voluntary T50, T100, and T150 were higher for trained than untrained in absolute (P~{$<~$}0.001) and normalized (P~{$<~$}0.030) terms, accompanied by higher sEMG at 0-50, 50-100, and 100-150~ms (P~{$<~$}0.001), and voluntary T50/octet T50 ratio for trained. Greater octet T50 was observed for the young trained (P~{$<~$}0.001) but not for the older trained (P~=~0.273) compared to their untrained counterparts. Age effect was observed for voluntary T50, T100, and T150 (P~{$<~$}0.050), but normalization removed these differences (P~{$>~$}0.417). Chronically strength-trained young and older men presented a greater explosive torque than their untrained pairs. In young trained, the greater explosive performance was attributed to enhanced muscular and neural mechanisms, while in older trained to neural mechanisms only.},
  langid = {english},
  pmid = {32749004},
  keywords = {{Evoked Potentials, Motor},Aged,aging,Aging,Cross-Sectional Studies,Electromyography,geriatrics,Humans,Isometric Contraction,Knee,Male,Middle Aged,rapid force,rate of force development,resistance training,Resistance Training,Torque,Young Adult}
}

@article{otterDiagnosisTherapyAcute2004,
  title = {Diagnosis and {{Therapy}} of the {{Acute Myocardial Infarction}} in {{Diabetes}}},
  shorttitle = {Diagnose Und {{Therapie}} Des {{Akuten Myokardinfarkts}} Bei {{Diabetes}} Mellitus},
  author = {Otter, W. and Doering, W. and Standl, E. and Schnell, O.},
  year = {2004},
  journal = {Diabetes und Stoffwechsel},
  volume = {13},
  number = {1},
  pages = {17--26}
}

@article{ouedraogoApplicationRandomForest2019,
  title = {Application of Random Forest Regression and Comparison of Its Performance to Multiple Linear Regression in Modeling Groundwater Nitrate Concentration at the {{African}} Continent Scale},
  shorttitle = {},
  author = {Ouedraogo, I. and Defourny, P. and Vanclooster, M.},
  year = {2019},
  journal = {Hydrogeology Journal},
  volume = {27},
  number = {3},
  pages = {1081--1098},
  publisher = {Springer Verlag},
  doi = {10.1007/s10040-018-1900-5}
}

@misc{OutlierInfluenceDiagnostics,
  title = {Outlier and Influence Diagnostics for Meta-Analysis - {{PubMed}}},
  urldate = {2024-06-11},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/26061377/},
  file = {/Users/cristian/Zotero/storage/F7XPJZCI/26061377.html}
}

@article{paddon-jonesPotentialErgogenicEffects2004,
  title = {Potential Ergogenic Effects of Arginine and Creatine Supplementation},
  author = {{Paddon-Jones}, D. and B{\o}rsheim, E. and Wolfe, R.R.},
  year = {2004},
  journal = {Journal of Nutrition},
  volume = {134},
  number = {10 SUPPL.},
  pages = {2888S-2894S},
  publisher = {American Institute of Nutrition},
  doi = {10.1093/jn/134.10.2888s}
}

@article{palijNewStatisticalRituals2012,
  title = {New Statistical Rituals for Old.},
  author = {Palij, Michael},
  year = {2012},
  month = jan,
  journal = {PsycCRITIQUES},
  volume = {57},
  doi = {10.1037/a0028079},
  abstract = {Reviews the book, Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis by Geoff Cumming (see record 2011-21220-000). Cumming's ``new statistics'' are, as he acknowledges, not new at all---no Bayesian analysis or scary mathematical modeling of phenomena or other approaches (e.g., Ludbrook \& Dudley, 1998). Instead, it is based on the use of effect sizes (ES), confidence intervals (CI), and meta-analyses instead of null hypothesis statistical testing (NHST) (i.e., the ``old statistics''). This volume's real strength is that it provides a good overview of the different types of ES, how to view CI in a variety of situations, and how to conduct meta-analyses. Cumming even acknowledges that there are problems with the ``new statistics'' that have to be worked out before progress can be made (e.g., confusion over the ES Cohen's d when it is difficult to determine which version is being reported; see ``Names and Symbols,'' pp. 295--296). The reviewer thinks that there is much merit in Cumming's text and that it would make a good core text in undergraduate or graduate statistics courses after the traditional introductory course on psychological statistics. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  file = {/Users/cristian/Zotero/storage/DVIYTTMA/item.zot}
}

@article{pannucciIdentifyingAvoidingBias2010,
  title = {Identifying and {{Avoiding Bias}} in {{Research}}},
  author = {Pannucci, Christopher J. and Wilkins, Edwin G.},
  year = {2010},
  month = aug,
  journal = {Plastic and reconstructive surgery},
  volume = {126},
  number = {2},
  pages = {619--625},
  issn = {0032-1052},
  doi = {10.1097/PRS.0b013e3181de24bc},
  urldate = {2021-03-12},
  abstract = {This narrative review provides an overview on the topic of bias as part of Plastic and Reconstructive Surgery's series of articles on evidence-based medicine. Bias can occur in the planning, data collection, analysis, and publication phases of research. Understanding research bias allows readers to critically and independently review the scientific literature and avoid treatments which are suboptimal or potentially harmful. A thorough understanding of bias and how it affects study results is essential for the practice of evidence-based medicine.},
  pmcid = {PMC2917255},
  pmid = {20679844},
  file = {/Users/cristian/Zotero/storage/7D76FQ5B/Pannucci and Wilkins - 2010 - Identifying and Avoiding Bias in Research.pdf}
}

@article{panzarellaDenouncingUseFieldspecific2021,
  ids = {panzarellaDenouncingUseFieldspecific2021a},
  title = {Denouncing the Use of Field-Specific Effect Size Distributions to Inform Magnitude},
  author = {Panzarella, Emily and Beribisky, Nataly and Cribbie, Robert A.},
  year = {2021},
  month = jun,
  journal = {PeerJ},
  volume = {9},
  pages = {e11383},
  issn = {2167-8359},
  doi = {10.7717/peerj.11383},
  urldate = {2023-10-16},
  abstract = {An effect size (ES) provides valuable information regarding the magnitude of effects, with the interpretation of magnitude being the most important. Interpreting ES magnitude requires combining information from the numerical ES value and the context of the research. However, many researchers adopt popular benchmarks such as those proposed by Cohen. More recently, researchers have proposed interpreting ES magnitude relative to the distribution of observed ESs in a specific field, creating unique benchmarks for declaring effects small, medium or large. However, there is no valid rationale whatsoever for this approach. This study was carried out in two parts: (1) We identified articles that proposed the use of field-specific ES distributions to interpret magnitude (primary articles); and (2) We identified articles that cited the primary articles and classified them by year and publication type. The first type consisted of methodological papers. The second type included articles that interpreted ES magnitude using the approach proposed in the primary articles. There has been a steady increase in the number of methodological and substantial articles discussing or adopting the approach of interpreting ES magnitude by considering the distribution of observed ES in that field, even though the approach is devoid of a theoretical framework. It is hoped that this research will restrict the practice of interpreting ES magnitude relative to the distribution of ES values in a field and instead encourage researchers to interpret such by considering the specific context of the study.},
  pmcid = {PMC8210805},
  pmid = {34178435},
  file = {/Users/cristian/Zotero/storage/XJZFTDAU/Panzarella et al. - 2021 - Denouncing the use of field-specific effect size d.pdf}
}

@article{parikhBloodbasedBiomarkersHepatocellular2023,
  title = {Blood-Based Biomarkers for Hepatocellular Carcinoma Screening: {{Approaching}} the End of the Ultrasound Era?},
  author = {Parikh, N.D. and Tayob, N. and Singal, A.G.},
  year = {2023},
  journal = {Journal of Hepatology},
  volume = {78},
  number = {1},
  pages = {207--216},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.jhep.2022.08.036}
}

@article{parkerImportanceClinicalImportance2023,
  title = {The Importance of Clinical Importance When Determining the Target Difference in Sample Size Calculations},
  author = {Parker, Richard A. and Cook, Jonathan A.},
  year = {2023},
  month = aug,
  journal = {Trials},
  volume = {24},
  number = {1},
  pages = {495},
  issn = {1745-6215},
  doi = {10.1186/s13063-023-07532-5},
  abstract = {Recently, it was argued that clinically important differences should play no role in sample size calculations. Instead, it was proposed that sample size calculations should focus on setting realistic estimates of treatment benefit. We disagree, and argue in this article that considering the importance of a target difference is necessary in the context of randomised controlled trials of effectiveness, particularly definitive phase III trials. Ignoring clinical importance could have serious ethical and practical consequences.},
  langid = {english},
  pmcid = {PMC10401796},
  pmid = {37542276},
  keywords = {Assumed benefit,Clinical Relevance,Clinical trial,Clinically relevant difference,Effect size,Humans,Minimum important difference,Power,Randomized Controlled Trials as Topic,RCT,Sample Size,Target difference},
  file = {/Users/cristian/Zotero/storage/VK6724U4/Parker and Cook - 2023 - The importance of clinical importance when determi.pdf}
}

@article{parkerNonadjustmentMultipleTesting2020,
  title = {Non-Adjustment for Multiple Testing in Multi-Arm Trials of Distinct Treatments: {{Rationale}} and Justification},
  shorttitle = {Non-Adjustment for Multiple Testing in Multi-Arm Trials of Distinct Treatments},
  author = {Parker, Richard A and Weir, Christopher J},
  year = {2020},
  month = oct,
  journal = {Clinical Trials (London, England)},
  volume = {17},
  number = {5},
  pages = {562--566},
  issn = {1740-7745},
  doi = {10.1177/1740774520941419},
  urldate = {2024-08-23},
  abstract = {There is currently a lack of consensus and uncertainty about whether one should adjust for multiple testing in multi-arm trials of distinct treatments. A detailed rationale is presented to justify non-adjustment in this situation. We argue that non-adjustment should be the default starting position in simple multi-arm trials of distinct treatments.},
  pmcid = {PMC7534018},
  pmid = {32666813},
  file = {/Users/cristian/Zotero/storage/WPZYL766/Parker and Weir - 2020 - Non-adjustment for multiple testing in multi-arm t.pdf}
}

@article{paternainMetabolomicsTranscriptomicsMetabolic2013,
  title = {Metabolomics and {{Transcriptomics}} of {{Metabolic Disorders}}},
  author = {Paternain, L. and Campion, J.},
  year = {2013},
  journal = {Current Nutrition Reports},
  volume = {2},
  number = {4},
  pages = {199--206},
  publisher = {Current Science Inc.},
  doi = {10.1007/s13668-013-0062-2}
}

@article{patilWhatShouldResearchers2016,
  title = {What {{Should Researchers Expect When They Replicate Studies}}? {{A Statistical View}} of {{Replicability}} in {{Psychological Science}}},
  shorttitle = {What {{Should Researchers Expect When They Replicate Studies}}?},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2016},
  month = jul,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {11},
  number = {4},
  pages = {539--544},
  issn = {1745-6924},
  doi = {10.1177/1745691616646366},
  abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers' intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals-and a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
  langid = {english},
  pmcid = {PMC4968573},
  pmid = {27474140},
  keywords = {{Data Interpretation, Statistical},Behavioral Research,Humans,p values,prediction intervals,Psychology,replication,reproducibility,Reproducibility of Results,Reproducibility Project: Psychology},
  file = {/Users/cristian/Zotero/storage/34U7BG96/Patil et al. - 2016 - What Should Researchers Expect When They Replicate.pdf}
}

@misc{PDFBenefitsReporting2025,
  title = {({{PDF}}) {{The}} Benefits of Reporting Critical Effect Size Values},
  year = {2025},
  month = jan,
  journal = {ResearchGate},
  doi = {10.31234/osf.io/7qe92},
  urldate = {2025-03-20},
  abstract = {PDF {\textbar} Critical effect size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/387968572\_The\_benefits\_of\_reporting\_critical\_effect\_size\_values},
  langid = {english}
}

@article{perezgonzalezFisherNeymanPearsonNHST2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} Tutorial for Teaching Data Testing},
  author = {Perezgonzalez, Jose D.},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00223},
  abstract = {Despite frequent calls for the overhaul of null hypothesis significance testing (NHST), this controversial procedure remains ubiquitous in behavioral, social and biomedical teaching and research. Little change seems possible once the procedure becomes well ingrained in the minds and current practice of researchers; thus, the optimal opportunity for such change is at the time the procedure is taught, be this at undergraduate or at postgraduate levels. This paper presents a tutorial for the teaching of data testing procedures, often referred to as hypothesis testing theories. The first procedure introduced is Fisher's approach to data testing---tests of significance; the second is Neyman-Pearson's approach---tests of acceptance; the final procedure is the incongruent combination of the previous two theories into the current approach---NSHT. For those researchers sticking with the latter, two compromise solutions on how to improve NHST conclude the tutorial.}
}

@article{pernetNullHypothesisSignificance2016,
  title = {Null Hypothesis Significance Testing: A Short Tutorial},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Pernet, Cyril},
  year = {2016},
  month = oct,
  journal = {F1000Research},
  volume = {4},
  issn = {2046-1402},
  doi = {10.12688/f1000research.6963.3},
  urldate = {2021-04-13},
  abstract = {Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences. In this short tutorial, I first summarize the concepts behind the method, distinguishing test of significance (Fisher) and test of acceptance (Newman-Pearson) and point to common interpretation errors regarding the p-value. I then present the related concepts of confidence intervals and again point to common interpretation errors. Finally, I discuss what should be reported in which context. The goal is to clarify concepts to avoid interpretation errors and propose reporting practices.},
  pmcid = {PMC5635437},
  pmid = {29067159},
  file = {/Users/cristian/Zotero/storage/C8UEMMJI/Pernet - 2016 - Null hypothesis significance testing a short tuto.pdf}
}

@article{perugini_critical_es_2025,
  title = {The {{Benefits}} of {{Reporting Critical-Effect-Size Values}}},
  author = {Perugini, Ambra and Gambarota, Filippo and Toffalini, Enrico and Lakens, Dani{\"e}l and Pastore, Massimiliano and Finos, Livio and Alto{\`e}, Gianmarco},
  year = {2025},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {8},
  number = {2},
  pages = {25152459251335298},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459251335298},
  urldate = {2025-06-07},
  abstract = {Critical-effect-size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha level, and test statistic. It can be useful to calculate the critical effect size when designing a study and evaluate whether such effects are plausible. Reporting critical-effect-size values may be useful when the sample size has not been planned a priori, there is uncertainty about the expected sample size that can be collected, or researchers plan to analyze the data with a statistical hypothesis test. To assist researchers in calculating critical-effect-size values, we developed an R package that allows researchers to report critical-effect-size values for group comparisons, correlations, linear regressions, and meta-analyses. Reflecting on critical-effect-size values could benefit researchers during the planning phase of the study by helping them to understand the limitations of their research design. Critical-effect-size values are also useful when evaluating studies performed by other researchers when a priori power analyses were not performed, especially when nonsignificant results are observed.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/3FLIYQSY/Perugini et al. - 2025 - The Benefits of Reporting Critical-Effect-Size Val.pdf}
}

@article{perugini_guidelines,
  title = {A {{Practical Primer To Power Analysis}} for {{Simple Experimental Designs}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2018},
  month = jul,
  journal = {International Review of Social Psychology},
  volume = {31},
  number = {1},
  issn = {2397-8570},
  doi = {10.5334/irsp.181},
  urldate = {2024-09-05},
  abstract = {The International Review of Social Psychology publishes empirical research and theoretical notes in all areas of social psychology. The IRSP emphasizes the scientific quality of its publications in every areas of social psychology. Submitted papers are reviewed by international experts. The journal was created to reflect research advances in a field where theoretical and fundamental questions inevitably convey social significance and implications. It is supported by ADRIPS.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/749D8BNG/Perugini et al. - 2018 - A Practical Primer To Power Analysis for Simple Ex.pdf}
}

@article{perugini_safeguard,
  title = {Safeguard {{Power}} as a {{Protection Against Imprecise Power Estimates}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614528519},
  urldate = {2024-07-26},
  abstract = {An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/VYAF2MVB/Perugini et al. - 2014 - Safeguard Power as a Protection Against Imprecise .pdf}
}

@misc{peruginiBenefitsReportingCritical2025,
  title = {The Benefits of Reporting Critical Effect Size Values},
  author = {Perugini, Ambra and Toffalini, Enrico and Gambarota, Filippo and Lakens, Daniel and Pastore, Massimiliano and Finos, Livio and Alto{\`e}, Gianmarco},
  year = {2025},
  month = jan,
  publisher = {OSF},
  doi = {10.31234/osf.io/7qe92},
  urldate = {2025-03-20},
  abstract = {Critical effect size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha level and test statistic. It can be useful to calculate the critical effect size when designing a study, and evaluate whether such effects are plausible. Reporting critical effect size values may be useful when the sample size has not been planned a priori, or when there is uncertainty about the expected sample size that can be collected, when researchers plan to analyze the data with a statistical hypothesis test. To assist researchers in calculating critical effect size values we have developed an R package that allows researchers to report critical effect size values for group comparisons, correlations, linear regressions, and meta-analyses. Reflecting on critical effect size values could benefit researchers during the planning phase of the study by helping them to understand the limitations of their research design. Critical effect size values are also useful when evaluating studies performed by other researchers when a-priori power analyses were not performed, especially when non significant results are observed.},
  archiveprefix = {OSF},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/I8B63FMY/Perugini et al. - 2025 - The benefits of reporting critical effect size val.pdf}
}

@article{petersPerformanceTrimFill2007,
  title = {Performance of the Trim and Fill Method in the Presence of Publication Bias and Between-Study Heterogeneity},
  author = {Peters, Jaime L. and Sutton, Alex J. and Jones, David R. and Abrams, Keith R. and Rushton, Lesley},
  year = {2007},
  journal = {Statistics in Medicine},
  volume = {26},
  number = {25},
  pages = {4544--4562},
  issn = {1097-0258},
  doi = {10.1002/sim.2889},
  urldate = {2025-05-11},
  abstract = {The trim and fill method allows estimation of an adjusted meta-analysis estimate in the presence of publication bias. To date, the performance of the trim and fill method has had little assessment. In this paper, we provide a more comprehensive examination of different versions of the trim and fill method in a number of simulated meta-analysis scenarios, comparing results with those from usual unadjusted meta-analysis models and two simple alternatives, namely use of the estimate from: (i) the largest; or (ii) the most precise study in the meta-analysis. Findings suggest a great deal of variability in the performance of the different approaches. When there is large between-study heterogeneity the trim and fill method can underestimate the true positive effect when there is no publication bias. However, when publication bias is present the trim and fill method can give estimates that are less biased than the usual meta-analysis models. Although results suggest that the use of the estimate from the largest or most precise study seems a reasonable approach in the presence of publication bias, when between-study heterogeneity exists our simulations show that these estimates are quite biased. We conclude that in the presence of publication bias use of the trim and fill method can help to reduce the bias in pooled estimates, even though the performance of this method is not ideal. However, because we do not know whether funnel plot asymmetry is truly caused by publication bias, and because there is great variability in the performance of different trim and fill estimators and models in various meta-analysis scenarios, we recommend use of the trim and fill method as a form of sensitivity analysis as intended by the authors of the method. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {between-study heterogeneity,meta-analysis,publication bias,simulation,trim and fill},
  file = {/Users/cristian/Zotero/storage/CLQS67GH/sim.html}
}

@article{petroffAssessmentHepaticSteatosis2021,
  title = {Assessment of Hepatic Steatosis by Controlled Attenuation Parameter Using the {{M}} and {{XL}} Probes: An Individual Patient Data Meta-Analysis},
  author = {Petroff, D. and Blank, V. and Newsome, P.N. and {Shalimar} and Voican, C.S. and Thiele, M. and {de L{\'e}dinghen}, V. and Baumeler, S. and Chan, W.K. and Perlemuter, G. and Cardoso, A.-C. and Aggarwal, S. and Sasso, M. and Eddowes, P.J. and Allison, M. and Tsochatzis, E. and Anstee, Q.M. and Sheridan, D. and Cobbold, J.F. and Naveau, S. and {Lupsor-Platon}, M. and Mueller, S. and Krag, A. and {Irles-Depe}, M. and Semela, D. and Wong, G.L.-H. and Wong, V.W.-S. and {Villela-Nogueira}, C.A. and Garg, H. and Chazouill{\`e}res, O. and Wiegand, J. and Karlas, T.},
  year = {2021},
  journal = {The Lancet Gastroenterology and Hepatology},
  volume = {6},
  number = {3},
  pages = {185--198},
  publisher = {Elsevier Ltd},
  doi = {10.1016/S2468-1253(20)30357-5}
}

@article{phuaRoleMaternalSerum2014,
  title = {The Role of Maternal Serum Biomarkers in the Early Diagnosis of Ectopic Pregnancy},
  author = {Phua, C. and Reid, S. and Condous, G.},
  year = {2014},
  journal = {Current Chemical Biology},
  volume = {8},
  number = {2},
  pages = {76--88},
  publisher = {Bentham Science Publishers},
  doi = {10.2174/2212796809666150203231334}
}

@article{piepoliExerciseTrainingChronic2013,
  title = {Exercise Training in Chronic Heart Failure: {{Mechanisms}} and Therapies},
  author = {Piepoli, M.F.},
  year = {2013},
  journal = {Netherlands Heart Journal},
  volume = {21},
  number = {2},
  pages = {85--90},
  doi = {10.1007/s12471-012-0367-6}
}

@article{PMID:36596953,
  title = {Is My Study Useless? {{Why}} Researchers Need Methodological Review Boards},
  author = {Lakens, Dani{\"e}l},
  year = {2023},
  month = jan,
  journal = {Nature},
  volume = {613},
  number = {7942},
  pages = {9},
  issn = {0028-0836},
  doi = {10.1038/d41586-022-04504-8}
}

@article{pointonColdWaterImmersion2012,
  title = {Cold Water Immersion Recovery after Simulated Collision Sport Exercise},
  author = {Pointon, Monique and Duffield, Rob},
  year = {2012},
  month = feb,
  journal = {Medicine and Science in Sports and Exercise},
  volume = {44},
  number = {2},
  pages = {206--216},
  issn = {1530-0315},
  doi = {10.1249/MSS.0b013e31822b0977},
  abstract = {PURPOSE: This investigation examined the effects of cold water immersion (CWI) recovery after simulated collision sport exercise. METHODS: Ten male rugby athletes performed three sessions consisting of a 2 {\texttimes} 30-min intermittent-sprint exercise (ISE) protocol with either tackling (T) or no tackling (CONT), followed by a 20-min CWI intervention (TCWI) or passive recovery (TPASS and CONT) in a randomized order. The ISE consisted of a 15-m sprint every minute separated by self-paced bouts of hard running, jogging, and walking for the remainder of the minute. Every sixth rotation, participants performed 5 {\texttimes} 10-m runs, receiving a shoulder-led tackle to the lower body on each effort. Sprint time and distance covered during ISE were recorded, with voluntary (maximal voluntary contraction; MVC) and evoked neuromuscular function (voluntary activation; VA), electromyogram (root mean square (RMS)), ratings of perceived muscle soreness (MS), capillary and venous blood markers for metabolites and muscle damage, respectively measured before and after exercise, immediately after recovery, and 2 and 24 h after recovery. RESULTS: Total distance covered during exercise was significantly greater in CONT (P = 0.01), without differences between TPASS and TCWI (P {$>$} 0.05). TCWI resulted in increased MVC, VA, and RMS immediately after recovery (P {$<$} 0.05). M-wave amplitude and peak twitch were significantly increased after recovery and 2 h after recovery, respectively, in TCWI (P {$<$} 0.05). Although TCWI had no effect on the elevation in blood markers for muscle damage (P {$>$} 0.05), lactate was significantly reduced after recovery compared with TPASS (P = 0.04). CWI also resulted in reduced MS 2 h after recovery compared with TPASS (P {$<$} 0.05). CONCLUSIONS: The introduction of body contact reduces exercise performance, whereas the use of CWI results in a faster recovery of MVC, VA, and RMS and improves muscle contractile properties and perceptions of soreness after collision-based exercise.},
  langid = {english},
  pmid = {21716151},
  keywords = {{Muscle, Skeletal},Adult,Athletic Injuries,Athletic Performance,Cold Temperature,Electromyography,Football,Humans,Immersion,Lactic Acid,Male,Muscle Contraction,Recovery of Function,Running,Walking,Water,Young Adult}
}

@article{polaninUseMetaanalyticStatistical2015,
  title = {The Use of Meta-Analytic Statistical Significance Testing},
  author = {Polanin, Joshua R. and Pigott, Terri D.},
  year = {2015},
  month = mar,
  journal = {Research Synthesis Methods},
  volume = {6},
  number = {1},
  pages = {63--73},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1124},
  abstract = {Meta-analysis multiplicity, the concept of conducting multiple tests of statistical significance within one review, is an underdeveloped literature. We address this issue by considering how Type I errors can impact meta-analytic results, suggest how statistical power may be affected through the use of multiplicity corrections, and propose how meta-analysts should analyze multiple tests of statistical significance. The context for this study is a meta-review of meta-analyses published in two leading review journals in education and psychology. Our review of 130 meta-analyses revealed a strong reliance on statistical significance testing without consideration of Type I errors or the use of multiplicity corrections. In order to provide valid conclusions, meta-analysts must consider these issues prior to conducting the review.},
  langid = {english},
  pmid = {26035470},
  keywords = {{Data Interpretation, Statistical},{Models, Statistical},Biostatistics,Education,Humans,meta-analysis,Meta-Analysis as Topic,meta-review,moderator analyses,multiplicity corrections,power analysis,Psychology,statistical significance testing,Type I errors}
}

@article{poldrackPublicationReproducibilityChallenges2015,
  title = {The Publication and Reproducibility Challenges of Shared Data},
  author = {Poldrack, Russell A. and Poline, Jean-Baptiste},
  year = {2015},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {19},
  number = {2},
  pages = {59--61},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2014.11.008},
  urldate = {2023-02-28},
  abstract = {The amount of shared data available for re-analysis has greatly increased in the last few years. Here we discuss some of the challenges raised by the analysis of these shared datasets and propose some strategies to address these issues.},
  langid = {english},
  keywords = {cognitive neuroscience,data sharing,fMRI,meta-analysis,publication},
  file = {/Users/cristian/Zotero/storage/UX8DDYBP/S1364661314002526.html}
}

@book{popperLogicScientificDiscovery1959,
  title = {The {{Logic}} of {{Scientific Discovery}}},
  author = {Popper, Karl},
  year = {1959},
  publisher = {Hutchison},
  address = {London},
  doi = {10.4324/9780203994627},
  abstract = {Described by the philosopher A.J. Ayer as a work of 'great originality and power', this book revolutionized contemporary thinking on science and knowledge. Ideas such as~the now legendary doctrine of 'falsificationism' electrified the scientific community, influencing even working scientists, as well as post-war philosophy. This astonishing work ranks alongside The Open Society and Its Enemies as one of Popper's most enduring books and contains insights and arguments that demand to be read to this day.},
  isbn = {978-0-203-99462-7}
}

@article{porcelComparingApproachesManagement2017,
  title = {Comparing Approaches to the Management of Malignant Pleural Effusions},
  author = {Porcel, J.M. and Lui, M.M.-S. and Lerner, A.D. and Davies, H.E. and {Feller-Kopman}, D. and Lee, Y.C.G.},
  year = {2017},
  journal = {Expert Review of Respiratory Medicine},
  volume = {11},
  number = {4},
  pages = {273--284},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/17476348.2017.1300532}
}

@misc{PowerAnalysisEffect,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}} - {{Google Search}}},
  urldate = {2025-09-01},
  howpublished = {https://www.google.com/search?client=safari\&rls=en\&q=Power+Analysis+and+Effect+Size+in+Mixed+Effects+Models\%3A+A+Tutorial\&ie=UTF-8\&oe=UTF-8},
  file = {/Users/cristian/Zotero/storage/MDWC78ZV/search.html}
}

@misc{PowerAnalysisEffecta,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}} {\textbar} {{Journal}} of {{Cognition}}},
  urldate = {2025-09-01},
  howpublished = {https://journalofcognition.org/articles/10.5334/joc.10},
  file = {/Users/cristian/Zotero/storage/D3492WAB/joc.html}
}

@article{poynardPerformanceBiomarkersFibroTest2012,
  title = {Performance of Biomarkers {{FibroTest}}, {{ActiTest}}, {{SteatoTest}}, and {{NashTest}} in Patients with Severe Obesity: {{Meta}} Analysis of Individual Patient Data},
  author = {Poynard, T. and Lassailly, G. and Diaz, E. and Clement, K. and Ca{\"i}azzo, R. and Tordjman, J. and Munteanu, M. and Perazzo, H. and Demol, B. and Callafe, R. and Pattou, F. and Charlotte, F. and Bedossa, P. and Mathurin, P. and Ratziu, V.},
  year = {2012},
  journal = {PLoS ONE},
  volume = {7},
  number = {3},
  doi = {10.1371/journal.pone.0030325}
}

@misc{PracticalSolutionPervasive,
  title = {A Practical Solution to the Pervasive Problems Ofp Values {\textbar} {{SpringerLink}}},
  urldate = {2023-08-30},
  howpublished = {https://link.springer.com/article/10.3758/BF03194105},
  file = {/Users/cristian/Zotero/storage/JE4Z8ICF/BF03194105.html}
}

@article{primbsAreSmallEffects2022,
  ids = {primbsAreSmallEffects2023},
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022)},
  shorttitle = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}?},
  author = {Primbs, Maximilian A. and Pennington, Charlotte R. and Lakens, Dani{\"e}l and Silan, Miguel Alejandro A. and Lieck, Dwayne S. N. and Forscher, Patrick S. and Buchanan, Erin M. and Westwood, Samuel J.},
  year = {2022},
  month = sep,
  journal = {Perspectives on Psychological Science},
  pages = {17456916221100420},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/17456916221100420},
  urldate = {2022-12-07},
  abstract = {In the January 2022 issue of Perspectives, G{\"o}tz et al. argued that small effects are ?the indispensable foundation for a cumulative psychological science.? They supported their argument by claiming that (a) psychology, like genetics, consists of complex phenomena explained by additive small effects; (b) psychological-research culture rewards large effects, which means small effects are being ignored; and (c) small effects become meaningful at scale and over time. We rebut these claims with three objections: First, the analogy between genetics and psychology is misleading; second, p values are the main currency for publication in psychology, meaning that any biases in the literature are (currently) caused by pressure to publish statistically significant results and not large effects; and third, claims regarding small effects as important and consequential must be supported by empirical evidence or, at least, a falsifiable line of reasoning. If accepted uncritically, we believe the arguments of G{\"o}tz et al. could be used as a blanket justification for the importance of any and all ?small? effects, thereby undermining best practices in effect-size interpretation. We end with guidance on evaluating effect sizes in relative, not absolute, terms.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/3X2TV9Z7/Primbs et al. - 2022 - Are Small Effects the Indispensable Foundation for.pdf}
}

@article{pritschetMarginallySignificantEffects2016,
  title = {Marginally {{Significant Effects}} as {{Evidence}} for {{Hypotheses}}: {{Changing Attitudes Over Four Decades}}},
  shorttitle = {Marginally {{Significant Effects}} as {{Evidence}} for {{Hypotheses}}},
  author = {Pritschet, Laura and Powell, Derek and Horne, Zachary},
  year = {2016},
  month = jul,
  journal = {Psychological Science},
  volume = {27},
  number = {7},
  pages = {1036--1042},
  issn = {1467-9280},
  doi = {10.1177/0956797616645672},
  abstract = {Some effects are statistically significant. Other effects do not reach the threshold of statistical significance and are sometimes described as "marginally significant" or as "approaching significance." Although the concept of marginal significance is widely deployed in academic psychology, there has been very little systematic examination of psychologists' attitudes toward these effects. Here, we report an observational study in which we investigated psychologists' attitudes concerning marginal significance by examining their language in over 1,500 articles published in top-tier cognitive, developmental, and social psychology journals. We observed a large change over the course of four decades in psychologists' tendency to describe a p value as marginally significant, and overall rates of use appear to differ across subfields. We discuss possible explanations for these findings, as well as their implications for psychological research.},
  langid = {english},
  pmid = {27207874},
  keywords = {{Psychology, Developmental},{Psychology, Social},Behavioral Research,Cognitive Science,Humans,marginal significance,methodology,null-hypothesis significance testing,open data,Statistics as Topic}
}

@article{prokopidisEffectsCreatineSupplementation2023,
  title = {Effects of Creatine Supplementation on Memory in Healthy Individuals: A Systematic Review and Meta-Analysis of Randomized Controlled Trials.},
  author = {Prokopidis, Konstantinos and Giannos, Panagiotis and Triantafyllidis, Konstantinos K. and Kechagias, Konstantinos S. and Forbes, Scott C. and Candow, Darren G.},
  year = {2023},
  month = mar,
  journal = {Nutrition reviews},
  volume = {81},
  number = {4},
  pages = {416--427},
  address = {United States},
  issn = {1753-4887 0029-6643},
  doi = {10.1093/nutrit/nuac064},
  abstract = {CONTEXT: From an energy perspective, the brain is very metabolically demanding. It is well documented that creatine plays a key role in brain bioenergetics.  There is some evidence that creatine supplementation can augment brain creatine  stores, which could increase memory. OBJECTIVE: A systematic review and  meta-analysis of randomized controlled trials (RCTs) was conducted to determine  the effects of creatine supplementation on memory performance in healthy humans.  DATA SOURCES: The literature was searched through the PubMed, Web of Science,  Cochrane Library, and Scopus databases from inception until September 2021. DATA  EXTRACTION: Twenty-three eligible RCTs were initially identified. Ten RCTs  examining the effect of creatine supplementation compared with placebo on  measures of memory in healthy individuals met the inclusion criteria for  systematic review, 8 of which were included in the meta-analysis. DATA ANALYSIS:  Overall, creatine supplementation improved measures of memory compared with  placebo (standard mean difference [SMD]\>=\>0.29, 95\%CI, 0.04-0.53; I2\,=\,66\%;  P\,=\,0.02). Subgroup analyses revealed a significant improvement in memory in  older adults (66-76\,years) (SMD\>=\>0.88; 95\%CI, 0.22-1.55; I2\,=\,83\%; P\,=\,0.009)  compared with their younger counterparts (11-31\,years) (SMD\>=\>0.03; 95\%CI, -0.14  to 0.20; I2\,=\,0\%; P\,=\,0.72). Creatine dose ({$\approx$}\>2.2-20\,g/d), duration of  intervention (5\,days to 24\,weeks), sex, or geographical origin did not influence  the findings. CONCLUSION: Creatine supplementation enhanced measures of memory  performance in healthy individuals, especially in older adults (66-76\,years).  SYSTEMATIC REVIEW REGISTRATION: PROSPERO registration no. 42021281027.},
  copyright = {{\copyright} The Author(s) 2022. Published by Oxford University Press on behalf of the International Life Sciences Institute.},
  langid = {english},
  pmcid = {PMC9999677},
  pmid = {35984306},
  keywords = {*Creatine/pharmacology,*Dietary Supplements,Aged,ageing,cognition,Cognition,creatine monohydrate,Humans,memory,nutrition,Randomized Controlled Trials as Topic}
}

@misc{PsycNETRecordDisplay,
  title = {{{PsycNET Record Display}}},
  urldate = {2025-05-17},
  howpublished = {https://psycnet.apa.org/record/2007-19368-002},
  file = {/Users/cristian/Zotero/storage/6KXV8VFB/2007-19368-002.html}
}

@misc{PsycNETRecordDisplaya,
  title = {{{PsycNET Record Display}}},
  urldate = {2025-06-16},
  howpublished = {https://psycnet.apa.org/record/2011-18061-018},
  file = {/Users/cristian/Zotero/storage/RI5C9ZKG/2011-18061-018.html}
}

@article{quekNovelSerumUrinary2021,
  title = {Novel Serum and Urinary Metabolites Associated with Diabetic Retinopathy in Three Asian Cohorts},
  author = {Quek, D.Q.Y. and He, F. and Sultana, R. and Banu, R. and Chee, M.L. and Nusinovici, S. and Thakur, S. and Qian, C. and Cheng, C.-Y. and Wong, T.Y. and Sabanayagam, C.},
  year = {2021},
  journal = {Metabolites},
  volume = {11},
  number = {9},
  publisher = {MDPI},
  doi = {10.3390/metabo11090614}
}

@article{quinlivanPharmacologicalNutritionalTreatment2010,
  title = {Pharmacological and Nutritional Treatment for {{McArdle}} Disease ({{Glycogen Storage Disease}} Type {{V}}).},
  author = {Quinlivan, R. and Martinuzzi, A. and Schoser, B.},
  year = {2010},
  journal = {Cochrane database of systematic reviews (Online)},
  volume = {12}
}

@article{quinlivanPharmacologicalNutritionalTreatment2014,
  title = {Pharmacological and Nutritional Treatment for {{McArdle}} Disease ({{Glycogen Storage Disease}} Type {{V}})},
  author = {Quinlivan, R. and Martinuzzi, A. and Schoser, B.},
  year = {2014},
  journal = {Cochrane Database of Systematic Reviews},
  volume = {2014},
  number = {11},
  publisher = {{John Wiley and Sons Ltd}},
  doi = {10.1002/14651858.CD003458.pub5}
}

@article{quintanaBetterHypothesisTests2022,
  title = {Towards Better Hypothesis Tests in Oxytocin Research: {{Evaluating}} the Validity of Auxiliary Assumptions.},
  author = {Quintana, Daniel S.},
  year = {2022},
  month = mar,
  journal = {Psychoneuroendocrinology},
  volume = {137},
  pages = {105642},
  address = {England},
  issn = {1873-3360 0306-4530},
  doi = {10.1016/j.psyneuen.2021.105642},
  abstract = {Various factors have been attributed to the inconsistent reproducibility of human oxytocin research in the cognitive and behavioral sciences. These factors include  small sample sizes, a lack of pre-registered studies, and the absence of  overarching theoretical frameworks that can account for oxytocin's effects over a  broad range of contexts. While there have been efforts to remedy these issues,  there has been very little systematic scrutiny of the role of auxiliary  assumptions, which are claims that are not central for testing a hypothesis but  nonetheless critical for testing theories. For instance, the hypothesis that  oxytocin increases the salience of social cues is predicated on the assumption  that intranasally administered oxytocin increases oxytocin levels in the brain.  Without robust auxiliary assumptions, it is unclear whether a hypothesis testing  failure is due to an incorrect hypothesis or poorly supported auxiliary  assumptions. Consequently, poorly supported auxiliary assumptions can be blamed  for hypothesis failure, thereby safeguarding theories from falsification. In this  article, I will evaluate the body of evidence for key auxiliary assumptions in  human behavioral oxytocin research in terms of theory, experimental design, and  statistical inference, and highlight assumptions that require stronger evidence.  Strong auxiliary assumptions will leave hypotheses vulnerable for falsification,  which will improve hypothesis testing and consequently advance our understanding  of oxytocin's role in cognition and behavior.},
  copyright = {Copyright {\copyright} 2021 The Author(s). Published by Elsevier Ltd.. All rights reserved.},
  langid = {english},
  pmid = {34991063},
  keywords = {{Administration, Intranasal},*Oxytocin/pharmacology,*Social Behavior,Cognition,Cues,Humans,Methodology,Oxytocin,Reproducibility,Reproducibility of Results,Social behavior,Social cognition,Theory}
}

@misc{quintanaMetametaMetametaanalysisPackage2022,
  title = {Metameta: {{A Meta-meta-analysis Package}} for {{R}}},
  shorttitle = {Metameta},
  author = {Quintana, Dan},
  year = {2022},
  urldate = {2022-05-21},
  abstract = {metameta: A Meta-meta-analysis Package for R},
  copyright = {MIT},
  keywords = {meta-analysis,rstats}
}

@article{quintanaMostOxytocinAdministration2020,
  title = {Most Oxytocin Administration Studies Are Statistically Underpowered to Reliably Detect (or Reject) a Wide Range of Effect Sizes},
  author = {Quintana, Daniel S.},
  year = {2020},
  month = nov,
  journal = {Comprehensive Psychoneuroendocrinology},
  volume = {4},
  pages = {100014},
  issn = {2666-4976},
  doi = {10.1016/j.cpnec.2020.100014},
  abstract = {The neuropeptide oxytocin has attracted substantial research interest for its role in behaviour and cognition; however, the evidence for its effects have been mixed. Meta-analysis is viewed as the gold-standard for synthesizing evidence, but the evidential value of a meta-analysis is dependent on the evidential value of the studies it synthesizes, and the analytical approaches used to derive conclusions. To assess the evidential value of oxytocin administration meta-analyses, this study calculated the statistical power of 107 studies from 35 meta-analyses and assessed the statistical equivalence of reported results. The mean statistical power across all studies was 12.2\% and there has been no noticeable improvement in power over an eight-year period. None of the 26 non-significant meta-analyses were statistically equivalent, assuming a smallest effect size of interest of 0.1. Altogether, most oxytocin treatment study designs are statistically underpowered to either detect or reject a wide range of worthwhile effect sizes.},
  langid = {english},
  pmcid = {PMC9216440},
  pmid = {35755627},
  keywords = {Neuroendocrinology,Oxytocin,Social behaviour,Statistics},
  file = {/Users/cristian/Zotero/storage/CCWHE2GZ/Quintana - 2020 - Most oxytocin administration studies are statistic.pdf}
}

@article{quintas-froufeIndicadoresCalidadPublicaciones2015,
  title = {{Indicadores de calidad de las publicaciones cient{\'i}ficas en el {\'a}rea de Ciencias Sociales en Espa{\~n}a: un an{\'a}lisis comparativo entre agencias evaluadoras}},
  shorttitle = {{Indicadores de calidad de las publicaciones cient{\'i}ficas en el {\'a}rea de Ciencias Sociales en Espa{\~n}a}},
  author = {{Quintas-Froufe}, Natalia},
  year = {2015},
  month = dec,
  journal = {Revista de Investigaci{\'o}n Educativa},
  volume = {34},
  number = {1},
  pages = {259},
  issn = {1989-9106, 0212-4068},
  doi = {10.6018/rie.34.1.210191},
  urldate = {2025-06-25},
  abstract = {Scientific publications (articles and books) are one of the basics for evaluating research. This article describes, analyzes and compares the quality criteria employed by the three major agencies which evaluate the scientific activity of Spanish university faculty when evaluating publications in the area of Social Sciences. The three selected entities are: ANECA (National Agency for Quality Assessment and Accreditation of Spain), ANEP (National Evaluation and Planning Agency) and CNEAI (National Research Activity Evaluation Commission). The results show that the most important quality criterion is the publication of articles in journals included in the Journal Citation Reports and SCOPUS. The conclusion reached was that international bibliographic and bibliometric databases are the only unanimously accepted quality indicators.},
  langid = {spanish}
}

@article{quiros-quirosEffectCreatineSupplementation2019,
  title = {Effect of Creatine Supplementation on Anaerobic Capacity: {{A}} Meta-Analysis},
  author = {{Quir{\'o}s-Quir{\'o}s}, A. and {Jim{\'e}nez-D{\'i}az}, J. and {Zamora-Salas}, J.D.},
  year = {2019},
  journal = {Archivos de Medicina del Deporte},
  volume = {36},
  number = {5},
  pages = {310--318},
  publisher = {Archivos de Medicina del Deporte}
}

@article{rabeloQuestionableResearchPractices2020,
  title = {Questionable Research Practices among {{Brazilian}} Psychological Researchers: {{Results}} from a Replication Study and an International Comparison},
  shorttitle = {Questionable Research Practices among {{Brazilian}} Psychological Researchers},
  author = {Rabelo, Andr{\'e} L. A. and Farias, J{\'e}ssica E. M. and Sarmet, Maur{\'i}cio M. and Joaquim, Teresa C. R. and Hoersting, Raquel C. and Victorino, Luiz and Modesto, Jo{\~a}o G. N. and Pilati, Ronaldo},
  year = {2020},
  journal = {International Journal of Psychology},
  volume = {55},
  number = {4},
  pages = {674--683},
  issn = {1464-066X},
  doi = {10.1002/ijop.12632},
  urldate = {2023-03-02},
  abstract = {Research on scientific integrity is growing in psychology, and questionable research practices (QRPs) have received more attention due to its harmful effect on science. By replicating the procedures of previous research, the present study aimed at describing the use of QRPs among Brazilian psychological researchers and to make an international comparison with previous studies in other countries---the US and Italy. Two hundred and thirty-two Brazilian researchers in the field of psychology answered questions related to 10 different QRPs. Brazilian researchers indicated a lower tendency to engage in two QRPs (failing to report all of a study's dependent measures; deciding whether to collect more data after looking to see whether the results were significant) when compared to their Italian and North American counterparts, but indicated a higher tendency to engage in two other QRPs (selectively reporting studies that ``worked''; not reporting all of a study's conditions). Most of the sample did not admit integrity conflicts in their own research but indicated that others have integrity problems, as observed in previous studies. Those discrepancies could be attributed to contextual and systemic factors regarding different publication demands among the different nations. Further studies should focus on identifying the antecedents of QRPs.},
  langid = {english},
  keywords = {Bias,Meta-research,Questionable research practices,Replicability,Scientific integrity},
  file = {/Users/cristian/Zotero/storage/JSXHEYH8/ijop.html}
}

@misc{raineyDataCodeAvailability2025,
  title = {Data and {{Code Availability}} in {{Political Science Publications}} from 1995 to 2022},
  author = {Rainey, Carlisle and Roe, Harley and Wang, Qing and Zhou, Hao},
  year = {2025},
  month = feb,
  publisher = {OSF},
  doi = {10.31235/osf.io/a5yxe_v2},
  urldate = {2025-02-11},
  abstract = {In this paper, we assess the availability of reproduction archives in political science. By ``reproduction archive,'' we mean the data and code supporting quantitative research articles that allows others to reproduce the computations described in the published paper. We collect a random sample of quantitative research articles published in political science from 1995 to 2022. We find that---even in 2022---most quantitative research articles do not point a reproduction archive. However, practices are improving. In 2014, when the DA-RT symposium was published in PS, about 12\% of quantitative research articles point to the data and code. Eight years later, in 2022, that has increased to 31\%. This underscores a massive shift in norms, requirements, and infrastructure. Still, only a minority of articles share the supporting data and code. In 2014, Lupia and Alter wrote: ``Today, information on the data production and analytic decisions that underlie many published works in political science is unavailable.'' They could write the same today; much work remains to be done.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {DA-RT,open science,reproducibility},
  file = {/Users/cristian/Zotero/storage/5WZ257WB/Rainey et al. - 2025 - Data and Code Availability in Political Science Pu.pdf}
}

@article{ramirez-campilloEffectsPlyometricJump2019,
  title = {Effects of Plyometric Jump Training on the Physical Fitness of Young Male Soccer Players: {{Modulation}} of Response by Inter-Set Recovery Interval and Maturation Status},
  shorttitle = {Effects of Plyometric Jump Training on the Physical Fitness of Young Male Soccer Players},
  author = {{Ramirez-Campillo}, Rodrigo and Alvarez, Cristian and {Sanchez-Sanchez}, Javier and Slimani, Maamer and Gentil, Paulo and Chelly, Mohamed Souhaiel and Shephard, Roy J.},
  year = {2019},
  month = dec,
  journal = {Journal of Sports Sciences},
  volume = {37},
  number = {23},
  pages = {2645--2652},
  issn = {1466-447X},
  doi = {10.1080/02640414.2019.1626049},
  abstract = {The effects of plyometric jump training on the physical fitness of male youth (age~=~10-17~years) soccer players was examined in relation to inter-set recovery intervals and the maturity of the players in a single-blind, randomized-and controlled crossover trial. Jumping tests and kicking velocities were measured before (T0), after a 6~week control period (T1), after 6~weeks of plyometrics (T2), after 6~weeks of wash-out (T3), and after a further 6~weeks of plyometrics (T4). Subjects were divided into pre- and post- peak-height-velocity (PHV) groups, and were randomly assigned to 30~s or 120~s inter-set intervals during periods T2 and T4. Any changes in jumping and maximum kicking velocities during T1 and T3, had trivial effect sizes (0.01-0.15), but small to moderate improvements (effect size~=~0.20-0.99) were observed in both groups during T2 and T4. Gains in pre-PHV players were similar for the two inter-set intervals, but gains in post-PHV players were greater (p~{$<~$}0.05) with an inter-set recovery of 120~s than with a 30~s recovery. We conclude that plyometric jump training improves the physical fitness of adolescents, irrespective of their maturity, but that in older individuals gains are greater with a longer inter-set recovery interval.},
  langid = {english},
  pmid = {31159655},
  keywords = {Adolescent,Child,Cross-Over Studies,development,fatigue,football,Humans,Male,Physical Fitness,plyometric exercise,Plyometric Exercise,Resistance training,Sexual Maturation,Single-Blind Method,Soccer}
}

@article{ramirez-campilloEffectsPlyometricTraining2013,
  title = {Effects of Plyometric Training Volume and Training Surface on Explosive Strength},
  author = {{Ram{\'i}rez-Campillo}, Rodrigo and Andrade, David C. and Izquierdo, Mikel},
  year = {2013},
  month = oct,
  journal = {Journal of Strength and Conditioning Research},
  volume = {27},
  number = {10},
  pages = {2714--2722},
  issn = {1533-4287},
  doi = {10.1519/JSC.0b013e318280c9e9},
  abstract = {The purpose of this study is to examine the effects of different volume and training surfaces during a short-term plyometric training program on neuromuscular performance. Twenty-nine subjects were randomly assigned to 4 groups: control group (CG, n = 5), moderate volume group (MVG, n = 9, 780 jumps), moderate volume hard surface group (MVGHS, n = 8, 780 jumps), and high volume group (HVG, n = 7, 1,560 jumps). A series of tests were performed by the subjects before and after 7 weeks of plyometric training. These tests were measurement of maximum strength (5 maximum repetitions [5RMs]), drop jumps (DJs) of varying heights (20, 40, and 60 cm), squat and countermovement jumps (SJ and CMJ, respectively), timed 20-m sprint, agility, body weight, and height. The results of the present study suggest that high training volume leads to a significant increase in explosive performance that requires fast stretch-shortening cycle (SSC) actions (such as DJ and sprint) in comparison to what is observed after a moderate training volume regimen. Second, when plyometric training is performed on a hard training surface (high-impact reaction force), a moderate training volume induces optimal stimulus to increase explosive performance requiring fast SSC actions (e.g., DJ), maximal dynamic strength enhancement, and higher training efficiency. Thus, a finding of interest in the study was that after 7 weeks of plyometric training, performance enhancement in maximal strength and in actions requiring fast SSC (such as DJ and sprint) were dependent on the volume of training and the surface on which it was performed. This must be taken into account when using plyometric training on different surfaces.},
  langid = {english},
  pmid = {23254550},
  keywords = {Adolescent,Anthropometry,Athletic Performance,Exercise Test,Humans,Male,Muscle Strength,Plyometric Exercise,Surface Properties}
}

@article{ranaDiagnosticAccuracyNoninvasive2020,
  title = {Diagnostic Accuracy of Non-Invasive Methods Detecting Clinically Significant Portal Hypertension in Liver Cirrhosis: {{A}} Systematic Review and Meta-Analysis},
  author = {Rana, R. and Wang, S. and Li, J. and Basnet, S. and Zheng, L. and Yang, C.},
  year = {2020},
  journal = {Minerva Medica},
  volume = {11},
  number = {31},
  pages = {266--280},
  publisher = {Edizioni Minerva Medica},
  doi = {10.23736/S0026-4806.19.06143-3}
}

@article{ranganathanPrematureTheorizingHurting2023,
  title = {Is Premature Theorizing Hurting Skill Acquisition Research?},
  author = {Ranganathan, Rajiv and Driska, Andrew},
  year = {2023},
  month = oct,
  journal = {Frontiers in Sports and Active Living},
  volume = {5},
  publisher = {Frontiers},
  issn = {2624-9367},
  doi = {10.3389/fspor.2023.1185734},
  urldate = {2025-02-24},
  abstract = {\&quot;{\dots}the dominant model of science in the field [that prioritizes experiments and hypothesis testing over real-world description] is appropriate only for a well-developed science, in which basic, real-world phenomena have been identified, important invariances in these phenomena have been documented, and appropriate model systems that capture the essence of these phenomena have been developed\&quot;. [Rozin, 2001, p. 2] Debates about competing approaches to skill acquisition (specifically information-processing vs. ecological based approaches) have dominated recent conversations, both among academics and practitioners. Our central argument in this article is that the above referenced quote by Rozin (1), originally made in the context of social psychology, is equally applicable to the field of skill acquisition in sport. Focusing on the current state of empirical work, we argue that there is not sufficient empirical data to constrain theories in skill acquisition research, and that trying to choose theories based on such limited data is both premature and detrimental to the development of the field itself.A theory is only as good as the data it explains. For example, consider the difference between skill acquisition and a closely related research field like motor control. In motor control, there are examples of well-established invariances such as Fitts\&\#39; law (2,3) or spatiotemporal characteristics of reaching trajectories (4,5). These robust and replicable phenomena provide such a strong empirical constraint that any new proposed theory of motor control is a non-starter if it did not account for these fundamental observations (6,7). In stark contrast, it is difficult to think of any finding that poses such a constraint to a theory of skill acquisition in sport. Instead, most phenomena in skill acquisition are characterized by two features -(a) they tend to be highly context-sensitive (i.e., influenced by factors such as the type of task and the stage of learning), and (b) they tend not to have quantitative process-level descriptions and instead focus mainly on outcome measures. While context-sensitivity is perhaps a reflection of the fact that skill acquisition is inherently sensitive to the learner and the learning context as seen by concepts such as desirable difficulties (8) or the challenge point (9), this also means that literature is filled with fragmented and seemingly contradictory findings. Coupled with the lack of quantitative descriptions, this has meant that current theories of skill acquisition tend to live within little bubbles of data.Can we really choose between theories of skill acquisition? However, despite the lack of robust findings, there is an attempt to follow the conventions of more mature sciences such as doing \&\#39;strong inference\&\#39; (10). Theories are often posed as being diametrically opposite on particular issues (e.g., \&quot;in theory A, variability is good for learning, whereas in theory B variability is bad for learning\&quot;). But while this contrast is helpful in that it raises awareness about the different ways in which we think about these phenomena, in most cases, theories are not specific enough to make predictions. In other words, there is no subsequent critical experiment which could distinguish between theory A or theory B because the question itself -i.e., \&quot;is variability good or bad for learning\&quot; -is ill-posed without knowledge of the context (e.g., the stage of learning, how much variability is being introduced, and what type of task is being learned) (11). In these cases, the strategy of having theories compete by constructing these binary oppositions is less likely to advance science (12).Another major issue regarding the data is the question of how much these results matter in realworld conditions. For example, most motor learning experiments, often used as the basis for skill acquisition, still rely on constrained tasks that rarely resemble the complexity of real-world (13,14). In addition, with limited sample sizes being an important factor constraining research studies, it is ideal in an experimental sense to maximize the effect size (i.e., the potential difference between groups) as much as possible. As a result, the contrast between groups is often exaggerated with \&\#39;strawman\&\#39; versions of groups that bear little resemblance to real-world skill acquisition (15) . For example, the information processing approach has been associated with \&\#39;prescription\&\#39; of an ideal movement pattern (often borrowed straight from a manual) with no room for individual differences, variability, or real-time flexibility, whereas the ecological approach has been associated with a trial-and-error \&\#39;self-organization\&\#39; approach to finding a movement solution with no room for planning, instructions demonstrations, or explicit strategies. As a result, even when these methods are directly compared (16)(17)(18), many researchers and practitioners remain unconvinced about the impact of such evidence on realworld contexts.We wish to emphasize that our goal is not to criticize theorizing itself. Challenging the theoretical status quo has brought important new perspectives to the field, which in turn has guided empirical data collection in new directions. For example, the focus on organism-taskenvironment as a whole ( 19) is an important perspective change on the role of the coach in terms of being \&\#39;environment designers\&\#39; (20,21). However, in prematurely trying to choose between theories, or derive implications for real-world situations, there is a danger of overgeneralization based on phenomena that have mainly been observed in niche experimental paradigms. We suggest two recommendations for improving the discourse-from a researcher\&\#39;s view and a practitioner\&\#39;s view.From a researcher\&\#39;s view, we propose that instead of the standard hypothesis testing/falsification paradigm, skill acquisition is much more suited to the \&\#39;inductive theory building\&\#39; approach (22), which argues for building a \&\#39;substantial body of data\&\#39; across different real-world contexts (using different methods, participants, time spans etc.) . In particular, there is a need for data collection outside of the domain of lab experiments, which tend to focus on extremely time-limited constrained observations, that by themselves are too rudimentary for theory building. The need for field-based data is a not new observation (1,23,24) but the continued lack of field-based data in guiding theories of skill acquisition seems to point to a systemic problem in what type of research is incentivized, and the need for large scale collaborations (including adversarial collaborations). In short, we need phenomena that everyone can agree on before we can test theories that people may disagree on.For an illustrative example of such theory building, one might look at the development of Self-Determination Theory (SDT) over the past 50 years. SDT began with experimental designs to examine the effects of incentives on creative problem solving using SOMA puzzles (25). When these experiments showed that incentives undermined participants\&\#39; willingness to engage with the puzzles during a break period, it questioned behaviorist tenets of the importance of incentives and reinforcement in volitional behavior and gave rise to the core construct of intrinsic motivation. Today, SDT is a \&quot;meta-theory\&quot; composed of six subtheories, each with an explanatory capacity for specific aspects of volitional behavior. Two features of SDT stand out as a model for skill acquisition. First, although SDT enjoys rather wide acceptance amongst behavioral scientists; few would argue that SDT accounts for all variance in volitional behavior, and few would neglect the effects of reinforcement, group norms, or other psychosocial factors in certain contexts. Second, SDT emerged from tightly-controlled, laboratory-based experimental designs, but then expanded to less-controlled, field-based quasi-experimental and even descriptive and qualitative designs. From a practitioner\&\#39;s view, we propose that skill acquisition and the utility of different coaching practices be examined from multiple lenses. Often, there is a tacit assumption that evidencebased coaching requires complete alignment of the goals of the researcher and the coach.However, this assumption can be misleading since the objectives of the researcher and the coach are quite different -researchers prioritize the search for systematic and generalizable principles whereas coaches are pragmatic and solution-focused. One example of how this difference manifests in practice is that a less optimal method of practice at one level may be preferred if it can be more efficient at a different level of analysis.For example, experimental comparisons of skill acquisition methods assume that participants receive the same number of practice repetitions in each method. However, this assumption may not always hold in the realworld. With a fixed amount of practice time, participants may often be able to do many more repetitions in a blocked or constant practice schedules because they require less changes in the environment and lesser effort from the coach. Therefore, as long as there is a non-zero learning benefit, some activities that seem suboptimal at one level (say the amount of learning/unit practice repetition) may actually be more efficient in terms of other levels (the amount of learning per unit time or per unit person-hours of coaching). Understanding these trade-offs at multiple scales of analysis is currently outside the domain of most experiments and highlights the need for more field-based work to complement lab-based work.In addition, the solution-focused approach of coaches may also explain certain coaching practices. For example, many coaches still use an \&\#39;ideal\&\#39; movement pattern even if they do not believe in imprinting this movement pattern on the learner simply because there is no other alternative. Relying purely on discovery learning may be time-consuming and increase the risk of getting stuck in maladaptive movement patterns. A researcher faced with this problem has the option of choosing a different context that is more tractable for study, but this is not an option for the solution-focused coach. Therefore, using the elite athlete\&\#39;s movement patterns may represent a reasonable compromise in this scenario, as long as it is followed by a trial-and-error process to identify an individual\&\#39;s optimal solution. In addition, many pedagogical techniques may also be effective in that they satisfy different goals beyond skill improvement (29). For example, demonstrations are associated with increasing selfefficacy, reducing anxiety, and learning strategies or game plans (30). Although the above arguments are not meant as a general defense of all current coaching practices, it may be more fruitful to move debates away from \&quot;is coaching practice X important?\&quot; to \&quot;when is coaching practice X important?\&quot;. In summary, we need a large and robust body of data to advance theoretical debates on skill acquisition in a meaningful way. One approach that may especially be fruitful in this process is the \&\#39;informed curiosity approach\&\#39; (1). Taking a middle path between the two extremes of hypothesis testing and simply amassing more data, the informed curiosity approach is characterized by attempts to answer open-ended questions that prioritize description of phenomena in ecologically valid contexts. For example, to understand the role of variability in learning, instead of a typical hypothesis-testing approach that compares two groups, an informed-curiosity approach would focus on describing the entire dose-response curve between variability and learning using multiple groups in a real-world task. Such descriptions of a functional relation between variables (31) provides a much better constraint on theory development than the two-group design where the nature of the result (both in terms of direction and effect size) is often highly sensitive to how the two groups are selected (32).Given the much higher effort involved in collecting this type of descriptive data and the breadth of skill acquisition in sport, a first step is to identify a few representative contexts that can be the focus of immediate efforts. Even in the laboratory setting, the use of select \&\#39;model tasks\&\#39; has been proposed to reduce task fragmentation and strike a balance between internal and ecological validity (32). By identifying a small set of common tasks that can capture different aspects of skill acquisition (similar to how model organisms are used in biology), researchers will be able to compile data across labs and obtain larger sample sizes, which can potentially lead to the discovery of invariances (in the same mold as Fitts\&\#39; law) that become the basis for theorizing. However, based on similar efforts in other domains (33)(34)(35)(36), achieving even this first step requires coordinated large-scale collaborations between academic researchers, sport scientists, coaches, and athletes in a way that runs counter to the current model of conducting research within a single lab. Creating the infrastructure and the incentive structure for these types of collaborations may ultimately be the most important piece for a theory of skill acquisition.},
  langid = {english},
  keywords = {coaching,ecological approach,Ecological Validity,Information Processing,Model tasks,Theory testing},
  file = {/Users/cristian/Zotero/storage/NRXDFKR4/Ranganathan and Driska - 2023 - Is premature theorizing hurting skill acquisition .pdf}
}

@article{rasmussenAssociationTrialRegistration2009,
  title = {Association of Trial Registration with the Results and Conclusions of Published Trials of New Oncology Drugs},
  author = {Rasmussen, Nicolas and Lee, Kirby and Bero, Lisa},
  year = {2009},
  journal = {Trials},
  volume = {10},
  pages = {116},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-10-116},
  abstract = {Background: Registration of clinical trials has been introduced largely to reduce bias toward statistically significant results in the trial literature. Doubts remain about whether advance registration alone is an adequate measure to reduce selective publication, selective outcome reporting, and biased design. One of the first areas of medicine in which registration was widely adopted was oncology, although the bulk of registered oncology trials remain unpublished. The net influence of registration on the literature remains untested. This study compares the prevalence of favorable results and conclusions among published reports of registered and unregistered randomized controlled trials of new oncology drugs. Methods: We conducted a cross-sectional study of published original research articles reporting clinical trials evaluating the efficacy of drugs newly approved for antimalignancy indications by the United States Food and Drug Administration (FDA) from 2000 through 2005. Drugs receiving first-time approval for indications in oncology were identified using the FDA web site and Thomson Centerwatch. Relevant trial reports were identified using PubMed and the Cochrane Library. Evidence of advance trial registration was obtained by a search of clinicaltrials.gov, WHO, ISRCTN, NCI-PDQ trial databases and corporate trial registries, as well as articles themselves. Data on blinding, results for primary outcomes, and author conclusions were extracted independently by two coders. Univariate and multivariate logistic regression identified associations between favorable results and conclusions and independent variables including advance registration, study design characteristics, and industry sponsorship. Results: Of 137 original research reports from 115 distinct randomized trials assessing 25 newly approved drugs for treating cancer, the 54 publications describing data from trials registered prior to publication were as likely to report statistically significant efficacy results and reach conclusions favoring the test drug (for results, OR = 1.77; 95\% CI = 0.87 to 3.61) as reports of trials not registered in advance. In multivariate analysis, reports of prior registered trials were again as likely to favor the test drug (OR = 1.29; 95\% CI = 0.54 to 3.08); large sample sizes and surrogate outcome measures were statistically significant predictors of favorable efficacy results at p {$<$} 0.05. Subgroup analysis of the main reports from each trial (n = 115) similarly indicated that registered trials were as likely to report results favoring the test drug as trials not registered in advance (OR = 1.11; 95\% CI = 0.44 to 2.80), and also that large trials and trials with nonstringent blinding were significantly more likely to report results favoring the test drug. Conclusions: Trial registration alone, without a requirement for full reporting of research results, does not appear to reduce a bias toward results and conclusions favoring new drugs in the clinical trials literature. Our findings support the inclusion of full results reporting in trial registers, as well as protocols to allow assessment of whether results have been completely reported.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/W4D3PQ4W/Rasmussen et al. - 2009 - Association of trial registration with the results.pdf}
}

@misc{rastiNeedScientificCoordination2025,
  title = {The Need for Scientific Coordination},
  author = {Rasti, Sajedeh and Vaesen, Krist and Lakens, Daniel},
  year = {2025},
  publisher = {OSF. https://doi. org/10.31234/osf. io/vjcfk\_v2},
  archiveprefix = {OSF. https://doi. org/10.31234/osf. io/vjcfk\_v2}
}

@article{raudenbushModelingMultivariateEffect1988,
  title = {Modeling Multivariate Effect Sizes},
  author = {Raudenbush, Stephen W. and Becker, Betsy Jane and Kalaian, Hripsime},
  year = {1988},
  journal = {Psychological Bulletin},
  volume = {103},
  number = {1},
  pages = {111--120},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.103.1.111},
  abstract = {In this article, we present a flexible approach to the modeling of multiple effect sizes in meta-analysis. The method uses generalized least squares regression to account for interdependence among multiple outcomes within studies and to allow for different numbers of effect sizes across studies. Furthermore, the approach allows great flexibility in modeling linear equations for multivariate outcomes by means of the inclusion of different sets of predictors for each outcome. We use data from studies of the effectiveness of coaching on performance on the Scholastic Aptitude Test to illustrate application of the method. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Meta Analysis,Multivariate Analysis},
  file = {/Users/cristian/Zotero/storage/H3JX35KT/1988-09603-001.html}
}

@article{rawsonMechanismsMuscularAdaptations2007,
  title = {Mechanisms of Muscular Adaptations to Creatine Supplementation},
  author = {Rawson, E.S. and Persky, A.M.},
  year = {2007},
  journal = {International SportMed Journal},
  volume = {8},
  number = {2},
  pages = {43--53}
}

@article{raymondWhenDoesHeart1990,
  title = {When Does the Heart Fail during Shock?},
  author = {Raymond, R.M.},
  year = {1990},
  journal = {Circulatory Shock},
  volume = {30},
  number = {1},
  pages = {27--41}
}

@misc{RecommendationsPreregistrationsInternal,
  title = {Recommendations in Pre-Registrations and Internal Review Board Proposals Promote Formal Power Analyses but Do Not Increase Sample Size {\textbar} {{PLOS One}}},
  urldate = {2025-06-30},
  howpublished = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079},
  file = {/Users/cristian/Zotero/storage/8AQNXS6F/article.html}
}

@misc{ReplicationConcernsSports,
  title = {Replication Concerns in Sports and Exercise Science: A Narrative Review of Selected Methodological Issues in the Field {\textbar} {{Royal Society Open Science}}},
  urldate = {2025-03-29},
  howpublished = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.220946},
  file = {/Users/cristian/Zotero/storage/8L7U6DDU/rsos.html}
}

@misc{ReportingQuantitativeResearch,
  title = {Reporting {{Quantitative Research}} in {{Psychology}}, {{Second Edition}}},
  urldate = {2025-07-14},
  howpublished = {https://www.apa.org/pubs/books/reporting-quantitative-research-psychology-second-edition-revised}
}

@article{rheaDeterminingMagnitudeTreatment2004,
  title = {Determining the Magnitude of Treatment Effects in Strength Training Research through the Use of the Effect Size},
  author = {Rhea, Matthew R.},
  year = {2004},
  month = nov,
  journal = {Journal of Strength and Conditioning Research},
  volume = {18},
  number = {4},
  pages = {918--920},
  doi = {10.1519/14403.1},
  abstract = {In order to improve the applicability of research to exercise professionals, it is suggested that researchers analyze and report data in intervention studies that can be interpreted in relation to other studies. The effect size and proposed scale for determining the magnitude of the treatment effect can assist strength and conditioning professionals in interpreting and applying the findings of the strength training studies.},
  langid = {english},
  keywords = {Humans,Physical Education and Training,Research Design,Sports Medicine,Statistics as Topic}
}

@article{richardOneHundredYears2003,
  title = {One {{Hundred Years}} of {{Social Psychology Quantitatively Described}}},
  author = {Richard, F. D. and Bond Jr., Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  journal = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  publisher = {Educational Publishing Foundation},
  address = {US},
  issn = {1939-1552},
  doi = {10.1037/1089-2680.7.4.331},
  abstract = {This article compiles results from a century of social psychological research, more than 25,000 studies of 8 million people. A large number of social psychological conclusions are listed alongside meta-analytic information about the magnitude and variability of the corresponding effects. References to 322 meta-analyses of social psychological phenomena are presented, as well as statistical effect-size summaries. Analyses reveal that social psychological effects typically yield a value of r equal to .21 and that, in the typical research literature, effects vary from study to study in ways that produce a standard deviation in r of .15. Uses, limitations, and implications of this large-scale compilation are noted. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Experimentation,Social Psychology},
  file = {/Users/cristian/Zotero/storage/KNCRU8S8/Richard et al. - 2003 - One Hundred Years of Social Psychology Quantitativ.pdf;/Users/cristian/Zotero/storage/V3HEWQ6X/2018-70028-001.html}
}

@article{richardsonInterpretationSubgroupAnalyses2019,
  title = {Interpretation of Subgroup Analyses in Systematic Reviews: {{A}} Tutorial},
  shorttitle = {Interpretation of Subgroup Analyses in Systematic Reviews},
  author = {Richardson, Marty and Garner, Paul and Donegan, Sarah},
  year = {2019},
  month = jun,
  journal = {Clinical Epidemiology and Global Health},
  volume = {7},
  number = {2},
  pages = {192--198},
  issn = {2213-3984},
  doi = {10.1016/j.cegh.2018.05.005},
  urldate = {2025-05-09},
  abstract = {Authors of systematic reviews may perform subgroup analyses to investigate how treatment effect varies across different subgroups of patients or trials. Previous research has shown that Cochrane review authors do not sufficiently report their interpretation of subgroup analyses. Consequently, we developed a tutorial with the aim of improving the interpretation of subgroup analyses in reviews. We explain the importance of interpreting subgroup analyses, and demonstrate how to interpret subgroup analyses using theoretical examples and a real-life subgroup analysis with clinical context. Finally, we provide recommendations for the interpretation of subgroup analyses in systematic reviews.},
  keywords = {Meta-analysis,Subgroup analyses,Systematic reviews},
  file = {/Users/cristian/Zotero/storage/692IAYHL/S221339841830099X.html}
}

@article{riesthuis_simulations,
  title = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}: {{A Confidence-Interval Approach}} for {{Minimum-Effect}} and {{Equivalence Testing}}},
  shorttitle = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}},
  author = {Riesthuis, Paul},
  year = {2024},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {2},
  pages = {25152459241240722},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459241240722},
  urldate = {2025-06-17},
  abstract = {Effect sizes are often used in psychology because they are crucial when determining the required sample size of a study and when interpreting the implications of a result. Recently, researchers have been encouraged to contextualize their effect sizes and determine what the smallest effect size is that yields theoretical or practical implications, also known as the ``smallest effect size of interest'' (SESOI). Having a SESOI will allow researchers to have more specific hypotheses, such as whether their findings are truly meaningful (i.e., minimum-effect testing) or whether no meaningful effect exists (i.e., equivalence testing). These types of hypotheses should be reflected in power analyses to accurately determine the required sample size. Through a confidence-interval-focused approach and simulations, I show how to conduct power analyses for minimum-effect and equivalence testing. Moreover, I show that conducting a power analysis for the SESOI might result in inconclusive results. This confidence-interval-focused simulation-based power analysis can be easily adopted to different types of research areas and designs. Last, I provide recommendations on how to conduct such simulation-based power analyses.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/EYUNKUWR/Riesthuis - 2024 - Simulation-Based Power Analyses for the Smallest E.pdf}
}

@article{riesthuisSimulationBasedPowerAnalyses2024,
  title = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}: {{A Confidence-Interval Approach}} for {{Minimum-Effect}} and {{Equivalence Testing}}},
  shorttitle = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}},
  author = {Riesthuis, Paul},
  year = {2024},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {2},
  pages = {25152459241240722},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459241240722},
  urldate = {2025-02-26},
  abstract = {Effect sizes are often used in psychology because they are crucial when determining the required sample size of a study and when interpreting the implications of a result. Recently, researchers have been encouraged to contextualize their effect sizes and determine what the smallest effect size is that yields theoretical or practical implications, also known as the ?smallest effect size of interest? (SESOI). Having a SESOI will allow researchers to have more specific hypotheses, such as whether their findings are truly meaningful (i.e., minimum-effect testing) or whether no meaningful effect exists (i.e., equivalence testing). These types of hypotheses should be reflected in power analyses to accurately determine the required sample size. Through a confidence-interval-focused approach and simulations, I show how to conduct power analyses for minimum-effect and equivalence testing. Moreover, I show that conducting a power analysis for the SESOI might result in inconclusive results. This confidence-interval-focused simulation-based power analysis can be easily adopted to different types of research areas and designs. Last, I provide recommendations on how to conduct such simulation-based power analyses.},
  file = {/Users/cristian/Zotero/storage/572VH72C/Riesthuis - 2024 - Simulation-Based Power Analyses for the Smallest E.pdf}
}

@misc{RobustVarianceEstimation,
  title = {Robust Variance Estimation in Meta-regression with Dependent Effect Size Estimates - {{Hedges}} - 2010 - {{Research Synthesis Methods}} - {{Wiley Online Library}}},
  urldate = {2025-05-17},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.5},
  file = {/Users/cristian/Zotero/storage/IEZWS7VR/jrsm.html}
}

@article{rojano-ortegaEffectsBeetrootSupplementation2022,
  title = {Effects of {{Beetroot Supplementation}} on {{Recovery After Exercise-Induced Muscle Damage}}: {{A Systematic Review}}},
  author = {{Rojano-Ortega}, D. and Pe{\~n}a Amaro, J. and {Berral-Aguilar}, A.J. and {Berral-de la Rosa}, F.J.},
  year = {2022},
  journal = {Sports Health},
  volume = {14},
  number = {4},
  pages = {556--565},
  publisher = {SAGE Publications Inc.},
  doi = {10.1177/19417381211036412}
}

@article{romdhaniImprovedPhysicalPerformance2020,
  title = {Improved {{Physical Performance}} and {{Decreased Muscular}} and {{Oxidative Damage With Postlunch Napping After Partial Sleep Deprivation}} in {{Athletes}}},
  author = {Romdhani, Mohamed and Souissi, Nizar and Chaabouni, Yassine and Mahdouani, Kacem and Driss, Tarak and Chamari, Karim and Hammouda, Omar},
  year = {2020},
  month = jul,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {15},
  number = {6},
  pages = {874--883},
  issn = {1555-0273},
  doi = {10.1123/ijspp.2019-0308},
  abstract = {PURPOSE: To investigate the effects of napping after partial sleep deprivation (PSD) on reaction time, mood, and biochemical response to repeated-sprint exercise in athletes. METHODS: Nine male judokas performed 4 test sessions in a counterbalanced and randomized order. Participants accomplished 1 control session after a normal sleep night (NSN) and 3 after PSD with (1)~no nap, (2)~{$\sim$}20-min nap (N20), and (3)~{$\sim$}90-min nap (N90) opportunities. Test sessions included the running-based anaerobic sprint test, reaction time, Hooper index, and Epworth Sleepiness Scale. Muscle-damage biomarkers and antioxidant status were evaluated before and after exercise. RESULTS: PSD decreased maximum (P {$<$} .001, d = 1.12), mean (P {$<$} .001, d = 1.33), and minimum (P {$<$} .001, d = 1.15) powers compared with NSN. However, N20 and N90 enhanced maximum power compared with PSD (P {$<$} .05, d = 0.54; P {$<$} .001, d = 1.06, respectively). Minimum power and mean power increased only after N90 (P {$<$} .001, d = 1.63; P {$<$} .001, d = 1.16, respectively). Epworth Sleepiness Scale increased after PSD (P {$<$} .001, d = 0.86) and decreased after N20 (P {$<$} .001, d = 1.36) and N90 (P {$<$} .001, d = 2.07). N20 reduced multiple-choice reaction time (P {$<$} .001, d = 0.61). Despite performance decrement, PSD increased postexercise aspartate aminotransferase (P {$<$} .001, d = 4.16) and decreased glutathione peroxidase (P {$<$} .001, d = 4.02) compared with NSN. However, the highest performances after N90 were accompanied with lesser aspartate aminotransferase (P {$<$} .001, d = 1.74) and higher glutathione peroxidase (P {$<$} .001, d = 0.86) compared with PSD. CONCLUSIONS: Napping could be preventive against performance degradation caused by sleep loss. A short nap opportunity could be more beneficial when the subsequent effort is brief and requires frequent decision making. However, a longer nap opportunity could be preventive against muscle and oxidative damage, even for higher performances.},
  langid = {english},
  pmid = {32023544},
  keywords = {{Muscle, Skeletal},Adolescent,Affect,antioxidant status,Aspartate Aminotransferases,Athletic Performance,Biomarkers,Glutathione Peroxidase,Humans,Lactic Acid,Male,midday sleep,mood state,Oxidative Stress,Perception,Physical Exertion,reaction time,Reaction Time,short-term performances,Sleep,Sleep Deprivation,sleepiness,Time Factors,Young Adult}
}

@article{romeroPhilosophyScienceReplicability2019,
  title = {Philosophy of Science and the Replicability Crisis},
  author = {Romero, Felipe},
  year = {2019},
  journal = {Philosophy Compass},
  volume = {14},
  number = {11},
  pages = {e12633},
  issn = {1747-9991},
  doi = {10.1111/phc3.12633},
  urldate = {2021-04-29},
  abstract = {Replicability is widely taken to ground the epistemic authority of science. However, in recent years, important published findings in the social, behavioral, and biomedical sciences have failed to replicate, suggesting that these fields are facing a ``replicability crisis.'' For philosophers, the crisis should not be taken as bad news but as an opportunity to do work on several fronts, including conceptual analysis, history and philosophy of science, research ethics, and social epistemology. This article introduces philosophers to these discussions. First, I discuss precedents and evidence for the crisis. Second, I discuss methodological, statistical, and social-structural factors that have contributed to the crisis. Third, I focus on the philosophical issues raised by the crisis. Finally, I discuss several proposals for solutions and highlight the gaps that philosophers could focus on.},
  copyright = {{\copyright} 2019 The Author. Philosophy Compass published by John Wiley \& Sons Ltd},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/S2PWDGC3/Romero - 2019 - Philosophy of science and the replicability crisis.pdf;/Users/cristian/Zotero/storage/B6RKFBJI/phc3.html}
}

@article{rosanoPharmacologicalManagementChronic2016,
  title = {Pharmacological {{Management}} of {{Chronic Stable Angina}}: {{Focus}} on {{Ranolazine}}},
  author = {Rosano, G.M.C. and Vitale, C. and Volterrani, M.},
  year = {2016},
  journal = {Cardiovascular Drugs and Therapy},
  volume = {30},
  number = {4},
  pages = {393--398},
  publisher = {Springer New York LLC},
  doi = {10.1007/s10557-016-6674-1}
}

@article{rosenthalFileDrawerProblem1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {83},
  number = {3},
  pages = {638--641},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors},
  file = {/Users/cristian/Zotero/storage/LA56HT4M/1979-27602-001.html}
}

@article{rosenthalMetaanalysisRecentDevelopments2001,
  title = {Meta-Analysis: Recent Developments in Quantitative Methods for Literature Reviews},
  shorttitle = {Meta-Analysis},
  author = {Rosenthal, R. and DiMatteo, M. R.},
  year = {2001},
  journal = {Annual Review of Psychology},
  volume = {52},
  pages = {59--82},
  issn = {0066-4308},
  doi = {10.1146/annurev.psych.52.1.59},
  abstract = {We describe the history and current status of the meta-analytic enterprise. The advantages and historical criticisms of meta-analysis are described, as are the basic steps in a meta-analysis and the role of effect sizes as chief coins of the meta-analytic realm. Advantages of the meta-analytic procedures include seeing the "landscape" of a research domain, keeping statistical significance in perspective, minimizing wasted data, becoming intimate with the data summarized, asking focused research questions, and finding moderator variables. Much of the criticism of meta-analysis has been based on simple misunderstanding of how meta-analyses are actually carried out. Criticisms of meta-analysis that are applicable are equally applicable to traditional, nonquantitative, narrative reviews of the literature. Much of the remainder of the chapter deals with the processes of effect size estimation, the understanding of the heterogeneity of the obtained effect sizes, and the practical and scientific importance of the effect sizes obtained.},
  langid = {english},
  pmid = {11148299},
  keywords = {Humans,Meta-Analysis as Topic,Research,Science}
}

@article{rosenthalMetaAnalyticProceduresSocial1986,
  title = {Meta-{{Analytic Procedures}} for {{Social Science Research Sage Publications}}: {{Beverly Hills}}, 1984, 148 Pp.},
  shorttitle = {Meta-{{Analytic Procedures}} for {{Social Science Research Sage Publications}}},
  author = {Rosenthal, Robert},
  year = {1986},
  month = oct,
  journal = {Educational Researcher},
  volume = {15},
  number = {8},
  pages = {18--20},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X015008018},
  urldate = {2025-05-04},
  langid = {english}
}

@article{rosenthalPSYCHOLOGYSCIENTISTXIV1965,
  title = {{{PSYCHOLOGY OF THE SCIENTIST}}. {{XIV}}. {{EXPERIMENTERS}}' {{HYPOTHESIS-CONFIRMATION AND MOOD AS DETERMINANTS OF EXPERIMENTAL RESULTS}}},
  author = {Rosenthal, R. and Kohn, P. and Greenfield, P. M.},
  year = {1965},
  month = jun,
  journal = {Perceptual and Motor Skills},
  volume = {20},
  pages = {SUPPL:1237-1252},
  issn = {0031-5125},
  doi = {10.2466/pms.1965.20.3c.1237},
  langid = {english},
  pmid = {14330870},
  keywords = {{Set, Psychology},Humans,Psychology,PSYCHOLOGY,Research,RESEARCH,SET (PSYCHOLOGY)}
}

@article{rosnowFocusedTestsSignificance1988,
  title = {Focused Tests of Significance and Effect Size Estimation in Counseling Psychology},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  year = {1988},
  journal = {Journal of Counseling Psychology},
  volume = {35},
  number = {2},
  pages = {203--208},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2168},
  doi = {10.1037/0022-0167.35.2.203},
  abstract = {The purpose of this article is twofold: first, to present the strategic advantages of focused over omnibus tests of statistical significance in counseling research and, second, to demonstrate the utility of interpreting the magnitude of the effect as a way of assessing practical significance (i.e., in addition to computing p levels). Simple procedures are given for computing effect size using the product-moment r in the context of focused significance testing. We show that recasting the effect size into a binomial display is a convenient way of revealing the practical significance of r in terms of whatever equivalency measure is deemed appropriate (e.g., improvement rate, success rate, survival rate). (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Counseling Psychology,Effect Size (Statistical),Experimentation,Statistical Significance,Statistical Tests},
  file = {/Users/cristian/Zotero/storage/I45ATVKM/1988-34727-001.html}
}

@article{rothmanPlanningStudySize2018,
  title = {Planning {{Study Size Based}} on {{Precision Rather Than Power}}},
  author = {Rothman, Kenneth J. and Greenland, Sander},
  year = {2018},
  month = sep,
  journal = {Epidemiology},
  volume = {29},
  number = {5},
  pages = {599--603},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000876},
  urldate = {2022-10-10},
  abstract = {Study size has typically been planned based on statistical power and therefore has been heavily influenced by the philosophy of statistical hypothesis testing. A worthwhile alternative is to plan study size based on precision, for example by aiming to obtain a desired width of a confidence interval for the targeted effect. This article presents formulas for planning the size of an epidemiologic study based on the desired precision of the basic epidemiologic effect measures.},
  langid = {american},
  file = {/Users/cristian/Zotero/storage/HETYA3RJ/Planning_Study_Size_Based_on_Precision_Rather_Than.1.html}
}

@article{rovelliWhyBadPhilosophy2025,
  title = {Why Bad Philosophy Is Stopping Progress in Physics},
  author = {Rovelli, Carlo},
  year = {2025},
  month = may,
  journal = {Nature},
  volume = {641},
  number = {8063},
  pages = {585--587},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/d41586-025-01465-6},
  urldate = {2025-06-05},
  abstract = {Theoretical physicists are in thrall to a misguided mindset that allows viable ideas to be advanced only by overturning what already exists.},
  copyright = {2025 Springer Nature Limited},
  langid = {english},
  keywords = {History,Philosophy,Physics,Quantum physics},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Essay\\
Subject\_term: Physics, Philosophy, Quantum physics, History},
  file = {/Users/cristian/Zotero/storage/KYIVUSWR/Rovelli - 2025 - Why bad philosophy is stopping progress in physics.pdf;/Users/cristian/Zotero/storage/INUFE3FP/d41586-025-01465-6.html}
}

@article{roweGlucoseFructoseHydrogel2022,
  title = {Glucose and {{Fructose Hydrogel Enhances Running Performance}}, {{Exogenous Carbohydrate Oxidation}}, and {{Gastrointestinal Tolerance}}},
  author = {Rowe, Joshua T. and King, Roderick F. G. J. and King, Andy J. and Morrison, Douglas J. and Preston, Thomas and Wilson, Oliver J. and O'Hara, John P.},
  year = {2022},
  month = jan,
  journal = {Medicine and Science in Sports and Exercise},
  volume = {54},
  number = {1},
  pages = {129--140},
  issn = {1530-0315},
  doi = {10.1249/MSS.0000000000002764},
  abstract = {PURPOSE: Beneficial effects of carbohydrate (CHO) ingestion on exogenous CHO oxidation and endurance performance require a well-functioning gastrointestinal (GI) tract. However, GI complaints are common during endurance running. This study investigated the effect of a CHO solution-containing sodium alginate and pectin (hydrogel) on endurance running performance, exogenous and endogenous CHO oxidation, and GI symptoms. METHODS: Eleven trained male runners, using a randomized, double-blind design, completed three 120-min steady-state runs at 68\% VO2max, followed by a 5-km time-trial. Participants ingested 90 g{$\cdot$}h-1 of 2:1 glucose-fructose (13C enriched) as a CHO hydrogel, a standard CHO solution (nonhydrogel), or a CHO-free placebo during the 120 min. Fat oxidation, total and exogenous CHO oxidation, plasma glucose oxidation, and endogenous glucose oxidation from liver and muscle glycogen were calculated using indirect calorimetry and isotope ratio mass spectrometry. GI symptoms were recorded throughout the trial. RESULTS: Time-trial performance was 7.6\% and 5.6\% faster after hydrogel ([min:s] 19:29 {\textpm} 2:24, P {$<$} 0.001) and nonhydrogel (19:54 {\textpm} 2:23, P = 0.002), respectively, versus placebo (21:05 {\textpm} 2:34). Time-trial performance after hydrogel was 2.1\% faster (P = 0.033) than nonhydrogel. Absolute and relative exogenous CHO oxidation was greater with hydrogel (68.6 {\textpm} 10.8 g, 31.9\% {\textpm} 2.7\%; P = 0.01) versus nonhydrogel (63.4 {\textpm} 8.1 g, 29.3\% {\textpm} 2.0\%; P = 0.003). Absolute and relative endogenous CHO oxidation was lower in both CHO conditions compared with placebo (P {$<$} 0.001), with no difference between CHO conditions. Absolute and relative liver glucose oxidation and muscle glycogen oxidation were not different between CHO conditions. Total GI symptoms were not different between hydrogel and placebo, but GI symptoms were higher in nonhydrogel compared with placebo and hydrogel (P {$<$} 0.001). CONCLUSION: The ingestion of glucose and fructose in hydrogel form during running benefited endurance performance, exogenous CHO oxidation, and GI symptoms compared with a standard CHO solution.},
  langid = {english},
  pmid = {34334720},
  keywords = {Adult,Athletic Performance,Double-Blind Method,Fructose,Gastrointestinal Tract,Glucose,Humans,Hydrogels,Male,Oxidation-Reduction,Performance-Enhancing Substances,Running,Young Adult},
  file = {/Users/cristian/Zotero/storage/HDDNKDBI/Rowe et al. - 2022 - Glucose and Fructose Hydrogel Enhances Running Per.pdf}
}

@article{rubinWhenAdjustAlpha2021,
  title = {When to Adjust Alpha during Multiple Testing: A Consideration of Disjunction, Conjunction, and Individual Testing},
  shorttitle = {When to Adjust Alpha during Multiple Testing},
  author = {Rubin, Mark},
  year = {2021},
  journal = {Synthese},
  volume = {199},
  number = {3},
  pages = {10969--11000},
  publisher = {Springer},
  file = {/Users/cristian/Zotero/storage/ETHMXVFW/Rubin - 2021 - When to adjust alpha during multiple testing a co.pdf;/Users/cristian/Zotero/storage/RJY5J9L8/s11229-021-03276-4.html}
}

@article{ruckerTreatmenteffectEstimatesAdjusted2011,
  ids = {ruckerTreatmenteffectEstimatesAdjusted2011a,ruckerTreatmenteffectEstimatesAdjusted2011b},
  title = {Treatment-Effect Estimates Adjusted for Small-Study Effects via a Limit Meta-Analysis},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R. and Binder, Harald and Schumacher, Martin},
  year = {2011},
  month = jan,
  journal = {Biostatistics (Oxford, England)},
  volume = {12},
  number = {1},
  pages = {122--142},
  issn = {1468-4357},
  doi = {10.1093/biostatistics/kxq046},
  abstract = {Statistical heterogeneity and small-study effects are 2 major issues affecting the validity of meta-analysis. In this article, we introduce the concept of a limit meta-analysis, which leads to shrunken, empirical Bayes estimates of study effects after allowing for small-study effects. This in turn leads to 3 model-based adjusted pooled treatment-effect estimators and associated confidence intervals. We show how visualizing our estimators using the radial plot indicates how they can be calculated using existing software. The concept of limit meta-analysis also gives rise to a new measure of heterogeneity, termed G(2), for heterogeneity that remains after small-study effects are accounted for. In a simulation study with binary data and small-study effects, we compared our proposed estimators with those currently used together with a recent proposal by Moreno and others. Our criteria were bias, mean squared error (MSE), variance, and coverage of 95\% confidence intervals. Only the estimators arising from the limit meta-analysis produced approximately unbiased treatment-effect estimates in the presence of small-study effects, while the MSE was acceptably small, provided that the number of studies in the meta-analysis was not less than 10. These limit meta-analysis estimators were also relatively robust against heterogeneity and one of them had a relatively small coverage error.},
  langid = {english},
  pmid = {20656692},
  keywords = {{Models, Statistical},Bayes Theorem,Computer Simulation,Humans,Meta-Analysis as Topic,Sample Size,Treatment Outcome},
  file = {/Users/cristian/Zotero/storage/4PB9XA9W/Rcker et al. - 2011 - Treatment-effect estimates adjusted for small-stud.pdf;/Users/cristian/Zotero/storage/XRYV2ZAC/Rcker et al. - 2011 - Treatment-effect estimates adjusted for small-stud.pdf}
}

@article{ruiz-fernandezLoadMuscleGroup2023,
  title = {Load and Muscle Group Size Influence the Ergogenic Effect of Acute Caffeine Intake in Muscular Strength, Power and Endurance},
  author = {{Ruiz-Fern{\'a}ndez}, Iv{\'a}n and Valad{\'e}s, David and Dominguez, Ra{\'u}l and Ferragut, Carmen and {P{\'e}rez-L{\'o}pez}, Alberto},
  year = {2023},
  month = jun,
  journal = {European Journal of Nutrition},
  volume = {62},
  number = {4},
  pages = {1783--1794},
  issn = {1436-6215},
  doi = {10.1007/s00394-023-03109-9},
  urldate = {2023-08-17},
  abstract = {Although acute caffeine intake seems to improve muscular strength--power--endurance performance, there is scarce evidence evaluating upper vs lower-body exercises at different loads. Thus, this study aimed to examine the effects of acute caffeine intake on upper and lower-body muscular strength, power and endurance performance at different loads.},
  langid = {english},
  keywords = {Caffeine,Ergogenic aids,Load--power relationship,Resistance exercise,Sport performance},
  file = {/Users/cristian/Zotero/storage/JKNFWFZW/Ruiz-Fernndez et al. - 2023 - Load and muscle group size influence the ergogenic.pdf}
}

@misc{RunningYourBest,
  ids = {RunningYourBesta},
  title = {Running {{Your Best Triathlon Race}} in: {{International Journal}} of {{Sports Physiology}} and {{Performance Volume}} 16 {{Issue}} 5 (2021)},
  urldate = {2025-03-29},
  howpublished = {https://journals.humankinetics.com/view/journals/ijspp/16/5/article-p744.xml},
  file = {/Users/cristian/Zotero/storage/D9KCJ7JA/article-p744.html}
}

@article{ruthirakuhanPharmacologicalInterventionsApathy2018,
  title = {Pharmacological Interventions for Apathy in {{Alzheimer}}'s Disease},
  author = {Ruthirakuhan, M.T. and Herrmann, N. and Abraham, E.H. and Chan, S. and Lanct{\^o}t, K.L.},
  year = {2018},
  journal = {Cochrane Database of Systematic Reviews},
  volume = {2018},
  number = {5},
  publisher = {{John Wiley and Sons Ltd}},
  doi = {10.1002/14651858.CD012197.pub2}
}

@article{sadikotPuttingRECORDStraight2009,
  title = {Putting the {{RECORD}} Straight?},
  author = {Sadikot, S.M.},
  year = {2009},
  journal = {Diabetes and Metabolic Syndrome: Clinical Research and Reviews},
  volume = {3},
  number = {3},
  pages = {178--187},
  doi = {10.1016/j.dsx.2009.07.014}
}

@article{saeedEvaluationDietaryApproaches2019,
  title = {Evaluation of Dietary Approaches for the Treatment of Non-Alcoholic Fatty Liver Disease: {{A}} Systematic Review},
  author = {Saeed, N. and Nadeau, B. and Shannon, C. and Tincopa, M.},
  year = {2019},
  journal = {Nutrients},
  volume = {11},
  number = {12},
  publisher = {MDPI AG},
  doi = {10.3390/nu11123064}
}

@misc{SafeguardPowerProtection,
  title = {Safeguard {{Power}} as a {{Protection Against Imprecise Power Estimates}} - {{Marco Perugini}}, {{Marcello Gallucci}}, {{Giulio Costantini}}, 2014},
  urldate = {2024-07-26},
  howpublished = {https://journals.sagepub.com/doi/10.1177/1745691614528519},
  file = {/Users/cristian/Zotero/storage/36KVJKQ5/1745691614528519.html}
}

@article{sahaImpactFactorValid2003,
  title = {Impact Factor: A Valid Measure of Journal Quality?},
  shorttitle = {Impact Factor},
  author = {Saha, Somnath and Saint, Sanjay and Christakis, Dimitri A.},
  year = {2003},
  month = jan,
  journal = {Journal of the Medical Library Association: JMLA},
  volume = {91},
  number = {1},
  pages = {42--46},
  issn = {1536-5050},
  abstract = {OBJECTIVES: Impact factor, an index based on the frequency with which a journal's articles are cited in scientific publications, is a putative marker of journal quality. However, empiric studies on impact factor's validity as an indicator of quality are lacking. The authors assessed the validity of impact factor as a measure of quality for general medical journals by testing its association with journal quality as rated by clinical practitioners and researchers. METHODS: We surveyed physicians specializing in internal medicine in the United States, randomly sampled from the American Medical Association's Physician Masterfile (practitioner group, n = 113) and from a list of graduates from a national postdoctoral training program in clinical and health services research (research group, n = 151). Respondents rated the quality of nine general medical journals, and we assessed the correlation between these ratings and the journals' impact factors. RESULTS: The correlation between impact factor and physicians' ratings of journal quality was strong (r2 = 0.82, P = 0.001). The correlation was higher for the research group (r2 = 0.83, P = 0.001) than for the practitioner group (r2 = 0.62, P = 0.01). CONCLUSIONS: Impact factor may be a reasonable indicator of quality for general medical journals.},
  langid = {english},
  pmcid = {PMC141186},
  pmid = {12572533},
  keywords = {Adult,Bibliometrics,Female,Humans,Internal Medicine,Library Collection Development,Male,Middle Aged,Periodicals as Topic,Physicians,Random Allocation,Reproducibility of Results,Research Personnel,Sex Factors,Social Change,Surveys and Questionnaires,United States}
}

@article{sainaniCallIncreaseStatistical2021a,
  title = {Call to Increase Statistical Collaboration in Sports Science, Sport and Exercise Medicine and Sports Physiotherapy},
  author = {Sainani, Kristin L. and Borg, David N. and Caldwell, Aaron R. and Butson, Michael L. and Tenan, Matthew S. and Vickers, Andrew J. and Vigotsky, Andrew D. and Warmenhoven, John and Nguyen, Robert and Lohse, Keith R. and Knight, Emma J. and Bargary, Norma},
  year = {2021},
  month = jan,
  journal = {British Journal of Sports Medicine},
  volume = {55},
  number = {2},
  pages = {118--122},
  issn = {0306-3674, 1473-0480},
  doi = {10.1136/bjsports-2020-102607},
  urldate = {2022-10-13},
  abstract = {Statistical errors are common in many biomedical fields.1--5 We believe the nature and impact of these errors to be great enough in sports science and medicine to warrant special attention.6--14 Poor methodological and statistical practices have led to calls for change in other fields, such as psychology.15--18 We believe that a similar call to action is needed in sports science and medicine. Specifically, we see two pressing needs: (1) to increase collaboration between researchers and statisticians, and (2) to increase statistical training within the exercise science/medicine/physiotherapy (PT) discipline. Our call to action extends the work of those who have previously called for increased statistical collaboration in sports medicine and sports injury research.19--21  Though some academic sports science and medicine studies employ statisticians, such collaborations are an exception rather than the norm. To determine the extent of collaboration, we performed a systematic review of articles published in quartile one sports science journals in 2019 (see online supplementary file 1 for methods and online supplementary file 2 for data). The initial extraction included 8970 articles; of the 400 articles selected at random, 299 were deemed eligible and included in the review (figure 1). We found that only 13.3\% (95\% CI: 9.5\% to 17.2\%) of papers had at least one coauthor affiliated with a biostatistics, statistics, data science, data analytics, epidemiology, maths, computer science or economics department (figure 2). It should be noted that we included a broad set of methodological departments because we recognise that individuals from these fields may possess considerable statistical expertise. When we use the term `statistician' in this paper, we broadly include individuals from other methods-focussed disciplines if they have extensive statistical training and experience.\#\#\# Supplementary data [bjsports-2020-102607supp001.pdf] \#\#\# Supplementary data [bjsports-2020-102607supp002.pdf] Figure 1 Flowchart of the article search and inclusion for the systematic review. Figure 2 Percentage of {\dots}},
  langid = {english},
  keywords = {methodology,statistical review,statistics}
}

@article{sainaniProblemMultipleTesting2009,
  title = {The {{Problem}} of {{Multiple Testing}}},
  author = {Sainani, Kristin L.},
  year = {2009},
  journal = {PM\&R},
  volume = {1},
  number = {12},
  pages = {1098--1103},
  issn = {1934-1563},
  doi = {10.1016/j.pmrj.2009.10.004},
  urldate = {2023-08-15},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/CUGZJB8T/Sainani - 2009 - The Problem of Multiple Testing.pdf;/Users/cristian/Zotero/storage/M5WY4F6Y/j.pmrj.2009.10.html}
}

@article{salvatiEpilepsyLAMA2relatedMuscular2021,
  title = {Epilepsy in {{LAMA2-related}} Muscular Dystrophy: {{A}} Systematic Review of the Literature},
  author = {Salvati, A. and Bonaventura, E. and Sesso, G. and Pasquariello, R. and Sicca, F.},
  year = {2021},
  journal = {Seizure},
  volume = {91},
  pages = {425--436},
  publisher = {W.B. Saunders Ltd},
  doi = {10.1016/j.seizure.2021.07.020}
}

@article{samiNoninvasiveTestsDetection2018,
  title = {Non-Invasive Tests for the Detection of Oesophageal Varices in Compensated Cirrhosis: Systematic Review and Meta-Analysis},
  author = {Sami, S.S. and Harman, D. and Ragunath, K. and B{\"o}hning, D. and Parkes, J. and Guha, I.N.},
  year = {2018},
  journal = {United European Gastroenterology Journal},
  volume = {6},
  number = {6},
  pages = {806--818},
  publisher = {SAGE Publications Ltd},
  doi = {10.1177/2050640618767604}
}

@misc{SampleSizeJustification,
  title = {Sample {{Size Justification}} {\textbar} {{Collabra}}: {{Psychology}} {\textbar} {{University}} of {{California Press}}},
  urldate = {2025-06-10},
  howpublished = {https://online.ucpress.edu/collabra/article/8/1/33267/120491/Sample-Size-Justification},
  file = {/Users/cristian/Zotero/storage/M9PTNJRJ/Sample-Size-Justification.html}
}

@article{sanchezdiazEffectsPolyphenolConsumption2022,
  title = {Effects of {{Polyphenol Consumption}} on {{Recovery}} in {{Team Sport Athletes}} of {{Both Sexes}}: {{A Systematic Review}}},
  author = {S{\'a}nchez D{\'i}az, M. and {Mart{\'i}n-Castellanos}, A. and {Fern{\'a}ndez-El{\'i}as}, V.E. and L{\'o}pez Torres, O. and Lorenzo Calvo, J.},
  year = {2022},
  journal = {Nutrients},
  volume = {14},
  number = {19},
  publisher = {MDPI},
  doi = {10.3390/nu14194085}
}

@article{sandbergWaysConstructingResearch2011,
  title = {Ways of Constructing Research Questions: Gap-Spotting or Problematization?},
  shorttitle = {Ways of Constructing Research Questions},
  author = {Sandberg, J{\"o}rgen and Alvesson, Mats},
  year = {2011},
  month = jan,
  journal = {Organization},
  volume = {18},
  number = {1},
  pages = {23--44},
  publisher = {SAGE Publications Ltd},
  issn = {1350-5084},
  doi = {10.1177/1350508410372151},
  urldate = {2022-10-21},
  abstract = {This article examines ways of constructing research questions from existing literature, which are likely to promote the development of interesting and influential theories. We review 52 articles in organization studies and develop a typology of how researchers construct their research questions from existing literature. The most common way across paradigmatic camps is to spot various ?gaps? in the literature and, based on that, to formulate specific research questions. The dominance of gap-spotting is surprising, given it is increasingly recognized that theory is made interesting and influential when it challenges assumptions that underlie existing literature. The article discusses why assumption-challenging approaches are rare, and it identifies a range of social norms that favour gap-spotting. Finally, the article proposes some ways of constructing research questions that move beyond gap-spotting, and discusses how these ways are likely to promote more interesting and significant theories.},
  langid = {english}
}

@article{savitzProblemMechanisticRisk2019,
  title = {The {{Problem}} with {{Mechanistic Risk}} of {{Bias Assessments}} in {{Evidence Synthesis}} of {{Observational Studies}} and a {{Practical Alternative}}: {{Assessing}} the {{Impact}} of {{Specific Sources}} of {{Potential Bias}}},
  author = {Savitz, D.A. and Wellenius, G.A. and Trikalinos, T.A.},
  year = {2019},
  journal = {American Journal of Epidemiology},
  volume = {188},
  number = {9},
  pages = {1581--1585},
  publisher = {Oxford University Press},
  doi = {10.1093/aje/kwz131}
}

@article{sawaiEffectsEstrogenProgesterone2020,
  title = {The Effects of Estrogen and Progesterone on Plasma Amino Acids Levels: Evidence from Change Plasma Amino Acids Levels during the Menstrual Cycle in Women},
  author = {Sawai, A. and Tsuzuki, K. and Yamauchi, M. and Kimura, N. and Tsushima, T. and Sugiyama, K. and Ota, Y. and Sawai, S. and Tochikubo, O.},
  year = {2020},
  journal = {Biological Rhythm Research},
  volume = {51},
  number = {1},
  pages = {151--164},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/09291016.2018.1526496}
}

@article{scammaccaMetaAnalysisComplexResearch2014,
  title = {Meta-{{Analysis With Complex Research Designs}}: {{Dealing With Dependence From Multiple Measures}} and {{Multiple Group Comparisons}}},
  shorttitle = {Meta-{{Analysis With Complex Research Designs}}},
  author = {Scammacca, Nancy and Roberts, Greg and Stuebing, Karla K.},
  year = {2014},
  month = sep,
  journal = {Review of educational research},
  volume = {84},
  number = {3},
  pages = {328--364},
  issn = {0034-6543},
  doi = {10.3102/0034654313500826},
  urldate = {2025-05-17},
  abstract = {Previous research has shown that treating dependent effect sizes as independent inflates the variance of the mean effect size and introduces bias by giving studies with more effect sizes more weight in the meta-analysis. This article summarizes the different approaches to handling dependence that have been advocated by methodologists, some of which are more feasible to implement with education research studies than others. A case study using effect sizes from a recent meta-analysis of reading interventions is presented to compare the results obtained from different approaches to dealing with dependence. Overall, mean effect sizes and variance estimates were found to be similar, but estimates of indexes of heterogeneity varied. Meta-analysts are advised to explore the effect of the method of handling dependence on the heterogeneity estimates before conducting moderator analyses and to choose the approach to dependence that is best suited to their research question and their data set.},
  pmcid = {PMC4191743},
  pmid = {25309002},
  file = {/Users/cristian/Zotero/storage/C8N3XW6L/Scammacca et al. - 2014 - Meta-Analysis With Complex Research Designs Deali.pdf}
}

@misc{scarglePublicationBiasFileDrawer1999,
  title = {Publication {{Bias}} ({{The}} "{{File-Drawer Problem}}") in {{Scientific Inference}}},
  author = {Scargle, Jeffrey D.},
  year = {1999},
  month = sep,
  number = {arXiv:physics/9909033},
  eprint = {physics/9909033},
  publisher = {arXiv},
  doi = {10.48550/arXiv.physics/9909033},
  urldate = {2025-06-07},
  abstract = {Publication bias arises whenever the probability that a study is published depends on the statistical significance of its results. This bias, often called the file-drawer effect since the unpublished results are imagined to be tucked away in researchers' file cabinets, is potentially a severe impediment to combining the statistical results of studies collected from the literature. With almost any reasonable quantitative model for publication bias, only a small number of studies lost in the file-drawer will produce a significant bias. This result contradicts the well known Fail Safe File Drawer (FSFD) method for setting limits on the potential harm of publication bias, widely used in social, medical and psychic research. This method incorrectly treats the file drawer as unbiased, and almost always misestimates the seriousness of publication bias. A large body of not only psychic research, but medical and social science studies, has mistakenly relied on this method to validate claimed discoveries. Statistical combination can be trusted only if it is known with certainty that all studies that have been carried out are included. Such certainty is virtually impossible to achieve in literature surveys.},
  archiveprefix = {arXiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability}},
  file = {/Users/cristian/Zotero/storage/SQV2DRHY/Scargle - 1999 - Publication Bias (The File-Drawer Problem) in Sc.pdf;/Users/cristian/Zotero/storage/HGVZ7HB6/9909033.html}
}

@article{scarglePublicationBiasFileDrawer2000,
  title = {Publication Bias: The ``{{File-Drawer}}'' Problem in Scientific Inference},
  shorttitle = {Publication Bias},
  author = {Scargle, Jeffrey},
  year = {2000},
  month = jan,
  journal = {Journal of Scientific Exploration},
  volume = {14},
  pages = {91--106},
  abstract = {Publication bias arises whenever the probability that a study is published depends on the statistical significance of its results. This bias, often called the file-drawer effect since the unpublished results are imagined to be tucked away in researchers' file cabinets, is potentially a severe impediment to combining the statistical results of studies collected from the literature. With almost any reasonable quantitative model for publication bias, only a small number of studies lost in the file-drawer will produce a significant bias. This result contradicts the well known Fail Safe File Drawer (FSFD) method for setting limits on the potential harm of publication bias, widely used in social, medical and psychic research. This method incorrectly treats the file drawer as unbiased, and almost always miss-estimates the seriousness of publication bias. A large body of not only psychic research, but medical and social science studies, has mistakenly relied on this method to validate claimed discoveries. Statistical combination can be trusted only if it is known with certainty that all studies that have been carried out are included. Such certainty is virtually impossible to achieve in literature surveys.}
}

@article{schadHowCapitalizePriori2020,
  title = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: {{A}} Tutorial},
  shorttitle = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models},
  author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
  year = {2020},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {110},
  pages = {104038},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2019.104038},
  urldate = {2024-10-09},
  abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be ``tested instead of, rather than as a supplement to, the ordinary `omnibus' F test'' (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  keywords = {A priori hypotheses,Contrasts,Linear models,Null hypothesis significance testing},
  file = {/Users/cristian/Zotero/storage/T66QCHD8/Schad et al. - 2020 - How to capitalize on a priori contrasts in linear .pdf;/Users/cristian/Zotero/storage/NNY84WSU/S0749596X19300695.html}
}

@article{schaferMeaningfulnessEffectSizes2019,
  title = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}: {{Differences Between Sub-Disciplines}} and the {{Impact}} of {{Potential Biases}}},
  author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A.},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {813},
  doi = {10.3389/fpsyg.2019.00813},
  abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes---when is an effect small, medium, or large?---has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
  file = {/Users/cristian/Zotero/storage/K6CKVD59/Schfer and Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychologica.pdf}
}

@article{schardtUtilizationPICOFramework2007,
  title = {Utilization of the {{PICO}} Framework to Improve Searching {{PubMed}} for Clinical Questions},
  author = {Schardt, Connie and Adams, Martha B. and Owens, Thomas and Keitz, Sheri and Fontelo, Paul},
  year = {2007},
  month = jun,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {7},
  number = {1},
  pages = {16},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-7-16},
  urldate = {2025-03-24},
  abstract = {Supporting 21st century health care and the practice of evidence-based medicine (EBM) requires ubiquitous access to clinical information and to knowledge-based resources to answer clinical questions. Many questions go unanswered, however, due to lack of skills in formulating questions, crafting effective search strategies, and accessing databases to identify best levels of evidence.},
  langid = {english},
  keywords = {Clinical Question,Percutaneous Endoscopic Gastrostomy,Percutaneous Endoscopic Gastrostomy Tube,Publication Type,Relevant Citation},
  file = {/Users/cristian/Zotero/storage/B4JZEEUA/Schardt et al. - 2007 - Utilization of the PICO framework to improve searc.pdf}
}

@article{schauerAssessingHeterogeneityPower2020,
  title = {Assessing Heterogeneity and Power in Replications of Psychological Experiments},
  author = {Schauer, Jacob M. and Hedges, Larry V.},
  year = {2020},
  month = aug,
  journal = {Psychological Bulletin},
  volume = {146},
  number = {8},
  pages = {701--719},
  issn = {1939-1455},
  doi = {10.1037/bul0000232},
  abstract = {In this study, we reanalyze recent empirical research on replication from a meta-analytic perspective. We argue that there are different ways to define "replication failure," and that analyses can focus on exploring variation among replication studies or assess whether their results contradict the findings of the original study. We apply this framework to a set of psychological findings that have been replicated and assess the sensitivity of these analyses. We find that tests for replication that involve only a single replication study are almost always severely underpowered. Among the 40 findings for which ensembles of multisite direct replications were conducted, we find that between 11 and 17 (28\% to 43\%) ensembles produced heterogeneous effects, depending on how replication is defined. This heterogeneity could not be completely explained by moderators documented by replication research programs. We also find that these ensembles were not always well-powered to detect potentially meaningful values of heterogeneity. Finally, we identify several discrepancies between the results of original studies and the distribution of effects found by multisite replications but note that these analyses also have low power. We conclude by arguing that efforts to assess replication would benefit from further methodological work on designing replication studies to ensure analyses are sufficiently sensitive. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
  langid = {english},
  pmid = {32271029},
  keywords = {Humans,Meta-Analysis as Topic,Psychology,Reproducibility of Results,Research Design}
}

@article{scheadlerGlycerolHyperhydrationEndurance2010,
  title = {Glycerol Hyperhydration and Endurance Running Performance in the Heat},
  author = {Scheadler, C. and Garver, M. and Kirby, T. and Devor, S.},
  year = {2010},
  journal = {Journal of Exercise Physiology Online},
  volume = {13},
  number = {3},
  pages = {1--11}
}

@article{scheel_excessive,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {1--12},
  doi = {10.1177/25152459211007467},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english},
  keywords = {hypothesis testing,open data,preregistered,publication bias,Registered Reports},
  annotation = {https://doi.org/10.1177/25152459211007467},
  file = {/Users/cristian/Zotero/storage/5DFUNCJU/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Stand.pdf;/Users/cristian/Zotero/storage/EHD3RBMW/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Stand.pdf}
}

@article{scheel_why_hypothesis,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = {2020},
  month = dec,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  pages = {1745691620966795},
  issn = {1745-6924},
  doi = {10.1177/1745691620966795},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound "derivation chain" between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  pmid = {33326363},
  keywords = {exploratory research,hypothesis testing,replication crisis},
  file = {/Users/cristian/Zotero/storage/56TKYQ2L/Scheel et al. - 2020 - Why Hypothesis Testers Should Spend Less Time Test.pdf}
}

@article{scheelWhyMostPsychological2022,
  title = {Why Most Psychological Research Findings Are Not Even Wrong},
  author = {Scheel, Anne M.},
  year = {2022},
  journal = {Infant and Child Development},
  volume = {31},
  number = {1},
  pages = {e2295},
  issn = {1522-7219},
  doi = {10.1002/icd.2295},
  urldate = {2022-09-06},
  abstract = {Psychology's replication crisis is typically conceptualized as the insight that the published literature contains a worrying amount of unreplicable, false-positive findings. At the same time, meta-scientific attempts to assess the crisis in more detail have reported substantial difficulties in identifying unambiguous definitions of the scientific claims in published articles and determining how they are connected to the presented evidence. I argue that most claims in the literature are so critically underspecified that attempts to empirically evaluate them are doomed to failure---they are not even wrong. Meta-scientists should beware of the flawed assumption that the psychological literature is a collection of well-defined claims. To move beyond the crisis, psychologists must reconsider and rebuild the conceptual basis of their hypotheses before trying to test them.},
  langid = {english},
  keywords = {falsification,hypothesis testing,replication crisis,reproducibility,scientific inference},
  file = {/Users/cristian/Zotero/storage/RBRTCHK2/Scheel - 2022 - Why most psychological research findings are not e.pdf;/Users/cristian/Zotero/storage/R3T3B9KV/icd.html}
}

@article{schiavonNoninvasiveDiagnosisLiver2014,
  title = {Non-Invasive Diagnosis of Liver Fibrosis in Chronic Hepatitis {{C}}},
  author = {Schiavon, L.L. and {Narciso-Schiavon}, J.L. and {de Carvalho-Filho}, R.J.},
  year = {2014},
  journal = {World Journal of Gastroenterology},
  volume = {20},
  number = {11},
  pages = {2854--2866},
  publisher = {Baishideng Publishing Group Co},
  doi = {10.3748/wjg.v20.i11.2854}
}

@unpublished{schimmackZCurveMethodEstimating2017,
  title = {Z-{{Curve}}: {{A Method}} for the {{Estimating Replicability Based}} on {{Test Statistics}} in {{Original Studies}}},
  author = {Schimmack, Ulrich and Brunner, Jerry},
  year = {2017},
  annotation = {preprint},
  file = {/Users/cristian/Zotero/storage/ZKFN8EPR/search.html}
}

@article{schonbrodtSequentialHypothesisTesting2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  issn = {1939-1463},
  doi = {10.1037/met0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference. (PsycINFO Database Record},
  langid = {english},
  pmid = {26651986},
  keywords = {{Data Interpretation, Statistical},Bayes Theorem,Humans,Probability,Research Design,Sample Size},
  file = {/Users/cristian/Zotero/storage/9ZZGCARZ/Schnbrodt et al. - 2017 - Sequential hypothesis testing with Bayes factors .pdf}
}

@article{schulzCONSORT2010Statement2010,
  title = {{{CONSORT}} 2010 {{Statement}}: Updated Guidelines for Reporting Parallel Group Randomised Trials},
  shorttitle = {{{CONSORT}} 2010 {{Statement}}},
  author = {Schulz, Kenneth F. and Altman, Douglas G. and Moher, David},
  year = {2010},
  month = mar,
  journal = {BMJ},
  volume = {340},
  pages = {c332},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.c332},
  urldate = {2024-08-29},
  abstract = {{$<$}p{$>$}The CONSORT statement is used worldwide to improve the reporting of randomised controlled trials. \textbf{Kenneth Schulz and colleagues} describe the latest version, CONSORT 2010, which updates the reporting guideline based on new methodological evidence and accumulating experience{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Schulz et al 2010. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  langid = {english},
  pmid = {20332509},
  file = {/Users/cristian/Zotero/storage/BMFR75JI/Schulz et al. - 2010 - CONSORT 2010 Statement updated guidelines for rep.pdf;/Users/cristian/Zotero/storage/6BCSD2MR/bmj.c332.html}
}

@article{schulzSampleSizeCalculations2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F. and Grimes, David A.},
  year = {2005},
  month = apr,
  journal = {The Lancet},
  volume = {365},
  number = {9467},
  pages = {1348--1353},
  publisher = {Elsevier},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(05)61034-3},
  urldate = {2023-01-09},
  langid = {english},
  pmid = {15823387}
}

@article{schumiLookingGlassUnderstanding2011,
  title = {Through the Looking Glass: Understanding Non-Inferiority},
  shorttitle = {Through the Looking Glass},
  author = {Schumi, Jennifer and Wittes, Janet T},
  year = {2011},
  month = may,
  journal = {Trials},
  volume = {12},
  pages = {106},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-106},
  urldate = {2023-08-08},
  abstract = {Non-inferiority trials test whether a new product is not unacceptably worse than a product already in use. This paper introduces concepts related to non-inferiority, and discusses the regulatory views of both the European Medicines Agency and the United States Food and Drug Administration.},
  pmcid = {PMC3113981},
  pmid = {21539749}
}

@article{schweizerReproducibleResearchSport2016,
  title = {Reproducible Research in Sport and Exercise Psychology: {{The}} Role of Sample Sizes},
  shorttitle = {Reproducible Research in Sport and Exercise Psychology},
  author = {Schweizer, Geoffrey and Furley, Philip},
  year = {2016},
  month = mar,
  journal = {Psychology of Sport and Exercise},
  volume = {23},
  pages = {114--122},
  issn = {1469-0292},
  doi = {10.1016/j.psychsport.2015.11.005},
  urldate = {2021-04-21},
  abstract = {Objectives We aim to introduce the discussion on the crisis of confidence to sport and exercise psychology. We focus on an important aspect of this debate, the impact of sample sizes, by assessing sample sizes within sport and exercise psychology. Researchers have argued that publications in psychological research contain numerous false-positive findings and inflated effect sizes due to small sample sizes. Method We analyse the four leading journals in sport and exercise psychology regarding sample sizes of all quantitative studies published in these journals between 2009 and 2013. Subsequently, we conduct power analyses. Results A substantial proportion of published studies does not have sufficient power to detect effect sizes typical for psychological research. Sample sizes and power vary between research designs. Although many correlational studies have adequate sample sizes, experimental studies are often underpowered to detect small-to-medium effects. Conclusions As sample sizes are small, research in sport and exercise psychology may suffer from false-positive results and inflated effect sizes, while at the same time failing to detect meaningful small effects. Larger sample sizes are warranted, particularly in experimental studies.},
  langid = {english},
  keywords = {Effect size,False positive,Power,Replicability,Research methods},
  file = {/Users/cristian/Zotero/storage/XA4VS5FF/S1469029215300273.html}
}

@article{schwieteEffectsMuscleFatigue2023,
  title = {Effects of Muscle Fatigue on Exercise-Induced Hamstring Muscle Damage: A Three-Armed Randomized Controlled Trial},
  shorttitle = {Effects of Muscle Fatigue on Exercise-Induced Hamstring Muscle Damage},
  author = {Schwiete, Carsten and Roth, Christian and Skutschik, Christoph and M{\"o}ck, Sebastian and Rettenmaier, Lukas and Happ, Kevin and Broich, Holger and Behringer, Michael},
  year = {2023},
  month = jun,
  journal = {European Journal of Applied Physiology},
  issn = {1439-6327},
  doi = {10.1007/s00421-023-05234-z},
  abstract = {PURPOSE: Hamstring injuries in soccer reportedly increase towards the end of the matches' halves as well as with increased match frequency in combination with short rest periods, possibly due to acute or residual fatigue. Therefore, this study aimed to investigate the effects of acute and residual muscle fatigue on exercise-induced hamstring muscle damage. METHODS: A three-armed randomized-controlled trial, including 24 resistance-trained males, was performed allocating subjects to either a training group with acute muscle fatigue\,+\,eccentric exercise (AF/ECC); residual muscle fatigue\,+\,eccentric exercise (RF/ECC) or a control group with only eccentric exercise (ECC). Muscle stiffness, thickness, contractility, peak torque, range of motion, pain perception, and creatine kinase were assessed as muscle damage markers pre, post, 1~h post, and on the consecutive three days. RESULTS: Significant group\,{\texttimes}\,time interactions were revealed for muscle thickness (p\,=\,0.02) and muscle contractility parameters radial displacement (Dm) and contraction velocity (Vc) (both p\,=\,0.01), with larger changes in the ECC group (partial {$\eta$}2\,=\,0.4). Peak torque dropped by an average of 22\% in all groups; stiffness only changed in the RF/ECC group (p\,=\,0.04). Muscle work during the damage protocol was lower for AF/ECC than for ECC and RF/ECC (p\,=\,0.005). CONCLUSION: Hamstring muscle damage was comparable between the three groups. However, the AF/ECC group resulted in the same amount of muscle damage while accumulating significantly less muscle work during the protocol of the damage exercise. TRIAL REGISTRATION: This study was preregistered in the international trial registration platform (WHO; registration number: DRKS00025243).},
  langid = {english},
  pmid = {37330434},
  keywords = {Damage markers,Hamstring injuries,Muscle damage,Muscle fatigue},
  file = {/Users/cristian/Zotero/storage/MVHGKWY3/Schwiete et al. - 2023 - Effects of muscle fatigue on exercise-induced hams.pdf}
}

@article{scogginsMeasuringTransparencySocial2024,
  title = {Measuring Transparency in the Social Sciences: Political Science and International Relations},
  shorttitle = {Measuring Transparency in the Social Sciences},
  author = {Scoggins, Bermond and Robertson, Matthew P.},
  year = {2024},
  month = jul,
  journal = {Royal Society Open Science},
  volume = {11},
  number = {7},
  pages = {240313},
  publisher = {Royal Society},
  doi = {10.1098/rsos.240313},
  urldate = {2025-02-11},
  abstract = {The scientific method is predicated on transparency---yet the pace at which transparent research practices are being adopted by the scientific community is slow. The replication crisis in psychology showed that published findings employing statistical inference are threatened by undetected errors, data manipulation and data falsification. To mitigate these problems and bolster research credibility, open data and preregistration practices have gained traction in the natural and social sciences. However, the extent of their adoption in different disciplines is unknown. We introduce computational procedures to identify the transparency of a research field using large-scale text analysis and machine learning classifiers. Using political science and international relations as an illustrative case, we examine 93 931 articles across the top 160 political science and international relations journals between 2010 and 2021. We find that approximately 21\% of all statistical inference papers have open data and 5\% of all experiments are preregistered. Despite this shortfall, the example of leading journals in the field shows that change is feasible and can be effected quickly.},
  keywords = {data sharing,journal policy,open science,preregistration},
  file = {/Users/cristian/Zotero/storage/ILYMY3BT/Scoggins and Robertson - 2024 - Measuring transparency in the social sciences pol.pdf}
}

@misc{ScopeScientificHypotheses,
  title = {On the Scope of Scientific Hypotheses {\textbar} {{Royal Society Open Science}}},
  urldate = {2023-09-08},
  howpublished = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.230607},
  file = {/Users/cristian/Zotero/storage/6YR4KPIV/rsos.html}
}

@incollection{senn_2021,
  title = {Determining the {{Sample Size}}},
  booktitle = {Statistical {{Issues}} in {{Drug Development}}},
  author = {Senn, Stephen J.},
  year = {2021},
  pages = {241--264},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119238614.ch13},
  urldate = {2025-08-22},
  abstract = {Clinical trials are expensive, whether the cost is counted in money or in human suffering, but they are capable of providing results which are extremely valuable, whether the value is measured in drug company profits or successful treatment of future patients. This chapter argues that sample size issues are sometimes over-stressed at expense of others in clinical trials. The usual point of view is that the sample size is the determined function of variability, statistical method, power, and difference sought. A very unsatisfactory feature of conventional approaches to sample size calculation is that there is no mention of cost. The chapter discusses that there are various Bayesian suggestions. It considers an alternative frequentist approach to going beyond the P-value, that of severity, an approach championed by Deborah Mayo. P-values are a perfectly reasonable way for scientists to communicate the results of a significance test even if decisions rather than inferences are the object.},
  chapter = {13},
  isbn = {978-1-119-23861-4},
  langid = {english},
  keywords = {Bayesian approach,clinical trials,frequentist approach,P-value,sample size issues},
  file = {/Users/cristian/Zotero/storage/GE5U9GQH/9781119238614.html}
}

@article{serlinHypothesisTestingTheory1987,
  title = {Hypothesis Testing, Theory Building, and the Philosophy of Science},
  author = {Serlin, Ronald C.},
  year = {1987},
  journal = {Journal of Counseling Psychology},
  volume = {34},
  number = {4},
  pages = {365--371},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2168},
  doi = {10.1037/0022-0167.34.4.365},
  abstract = {During the last 3 decades, behavioral science research methodologies have been subjected to criticism, with emphasis placed on inappropriate sampling procedures, hypothesis-testing procedures, and atheoretical research. I discuss these criticisms and offer solutions that emphasize that it is only on the basis of theory that one can decide on an appropriate hypothesis to be tested, on a correct method of statistical analysis, and on whether the experimental results can be generalized to a population of interest. I describe the implications of these considerations for counseling psychology research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Counseling Psychology,Hypothesis Testing,Methodology,Sampling (Experimental),Social Sciences,Statistical Analysis,Theory Formulation},
  file = {/Users/cristian/Zotero/storage/N88DQFW4/1988-18821-001.html}
}

@article{shahSystematicReviewMetaanalysis2021,
  title = {A Systematic Review and Meta-Analysis of Spermatozoa Cryopreservation, in Vitro and in Vivo Fertility Practices in Water Buffalo},
  author = {Shah, S.A.H. and Andrabi, S.M.H.},
  year = {2021},
  journal = {Veterinary Research Communications},
  volume = {45},
  number = {2-3},
  pages = {47--74},
  publisher = {{Springer Science and Business Media B.V.}},
  doi = {10.1007/s11259-021-09789-0}
}

@article{shantaImpactFactorScientific2013,
  title = {Impact Factor of a Scientific Journal: {{Is}} It a Measure of Quality of Research?},
  shorttitle = {Impact Factor of a Scientific Journal},
  author = {Shanta, A. and Pradhan, A. S. and Sharma, S. D.},
  year = {2013},
  journal = {Journal of Medical Physics / Association of Medical Physicists of India},
  volume = {38},
  number = {4},
  pages = {155--157},
  issn = {0971-6203},
  doi = {10.4103/0971-6203.121191},
  urldate = {2021-01-25},
  pmcid = {PMC3958993},
  pmid = {24672148}
}

@article{sharpe_transparency_2024,
  title = {Editor Bias and Transparency in Psychology's Open Science Era},
  author = {Sharpe, Donald},
  year = {2024},
  journal = {American Psychologist},
  volume = {79},
  number = {7},
  pages = {883--892},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/amp0001224},
  abstract = {In this open science era, psychology demands researchers be transparent in their research practices. In turn, researchers might ask if journal editors are being equally transparent in their editorial practices. Editor bias is when editors fail to be fair and impartial in their handling of articles. Editor bias can arise because of identity---who authors are---or because of content---what authors write. Proposed solutions to editor bias include masking author identity and increasing editor diversity. What is needed is greater transparency. By being more transparent, editors would be in a better position to encourage others to embrace open science. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Ethics,Open Science,Psychology,Publication Bias,Scientific Communication},
  file = {/Users/cristian/Zotero/storage/5HF8NPXJ/2025-32402-001.html}
}

@article{sharpeApplesOrangesFile1997,
  title = {Of Apples and Oranges, File Drawers and Garbage: Why Validity Issues in Meta-Analysis Will Not Go Away},
  shorttitle = {Of Apples and Oranges, File Drawers and Garbage},
  author = {Sharpe, D.},
  year = {1997},
  month = dec,
  journal = {Clinical Psychology Review},
  volume = {17},
  number = {8},
  pages = {881--901},
  issn = {0272-7358},
  doi = {10.1016/s0272-7358(97)00056-1},
  abstract = {This paper examines how threats to the validity of meta-analysis have been dealt with by clinical researchers employing this approach to literature review. Three validity threats were identified--mixing of dissimilar studies, publication bias, and inclusion of poor quality studies. Approaches to addressing these threats were evaluated for their effectiveness and popularity by surveying 32 published meta-analyses in clinical psychology. Distrust of meta-analysis, however, was found to transcend these validity threats. Other explanations for why this popular research strategy continues to receive widespread criticism were considered. Suggestions were made for how meta-analysis might better address these concerns.},
  langid = {english},
  pmid = {9439872},
  keywords = {Humans,Meta-Analysis as Topic,Reproducibility of Results,Research}
}

@article{sharpeApplesOrangesFile1997a,
  title = {Of Apples and Oranges, File Drawers and Garbage: {{Why}} Validity Issues in Meta-Analysis Will Not Go Away},
  shorttitle = {Of Apples and Oranges, File Drawers and Garbage},
  author = {Sharpe, Donald},
  year = {1997},
  month = dec,
  journal = {Clinical Psychology Review},
  volume = {17},
  number = {8},
  pages = {881--901},
  issn = {0272-7358},
  doi = {10.1016/S0272-7358(97)00056-1},
  urldate = {2025-06-15},
  abstract = {This paper examines how threats to the validity of meta-analysis have been dealt with by clinical researchers employing this approach to literature review. Three validity threats were identified --- mixing of dissimilar studies, publication bias, and inclusion of poor quality studies. Approaches to addressing these threats were evaluated for their effectiveness and popularity by surveying 32 published meta-analyses in clinical psychology. Distrust of meta-analysis, however, was found to transcend these validity threats. Other explanations for why this popular research strategy continues to receive widespread criticism were considered. Suggestions were made for how meta-analysis might better address these concerns.},
  file = {/Users/cristian/Zotero/storage/8FZEAN23/S0272735897000561.html}
}

@article{sharpeWhyResistanceStatistical2013,
  title = {Why the Resistance to Statistical Innovations? {{Bridging}} the Communication Gap},
  shorttitle = {Why the Resistance to Statistical Innovations?},
  author = {Sharpe, Donald},
  year = {2013},
  month = dec,
  journal = {Psychological Methods},
  volume = {18},
  number = {4},
  pages = {572--582},
  issn = {1939-1463},
  doi = {10.1037/a0034177},
  abstract = {While quantitative methodologists advance statistical theory and refine statistical methods, substantive researchers resist adopting many of these statistical innovations. Traditional explanations for this resistance are reviewed, specifically a lack of awareness of statistical developments, the failure of journal editors to mandate change, publish or perish pressures, the unavailability of user friendly software, inadequate education in statistics, and psychological factors. Resistance is reconsidered in light of the complexity of modern statistical methods and a communication gap between substantive researchers and quantitative methodologists. The concept of a Maven is introduced as a means to bridge the communication gap. On the basis of this review and reconsideration, recommendations are made to improve communication of statistical innovations.},
  langid = {english},
  pmid = {24079924},
  keywords = {Biomedical Research,Statistics as Topic}
}

@article{shawChangesSupplementationPractices2016,
  title = {Changes in the {{Supplementation Practices}} of {{Elite Australian Swimmers Over}} 11 {{Years}}},
  author = {Shaw, Gregory and Slater, Gary and Burke, Louise M.},
  year = {2016},
  month = dec,
  journal = {International Journal of Sport Nutrition and Exercise Metabolism},
  volume = {26},
  number = {6},
  pages = {565--571},
  issn = {1543-2742},
  doi = {10.1123/ijsnem.2016-0060},
  abstract = {Thirty nine elite Australian swimmers (13 AIS, 26 OTHER) completed a standardized questionnaire regarding their supplement use during a pre competition camp. The data were compared with a similar study conducted 11 years earlier (11 AIS, 23 OTHER) and framed around the classification system of the Sport Supplement Program of the Australian Institute of Sport. The prevalence of supplement use remained constant over time (2009: 97\%, 1998: 100\%). However, the current swimmers used a greater number of dietary supplements (9.2 {\textpm} 3.7 and 5.9 {\textpm} 2.9; p = .001), accounted for by an increase in the reported use of supplements with a greater evidence base (Sports Foods, Ergogenics, and Group B supplements). In contrast, fewer supplements considered less reputable (Group C and D) were reported by the 2009 cohort (0.7 {\textpm} 1.0 and 1.6 {\textpm} 1.3; p = .003). AIS swimmers reported a greater use of Ergogenics (4.3 {\textpm} 1.8 and 3.1 {\textpm} 1.7; p = .002), and less use of Group C and D supplements overall (0.8 {\textpm} 1.2 and 1.3 {\textpm} 1.2; p = .012), which was explained primarily by a smaller number of these supplements reported by the 2009 group (1998 AIS: 1.5 {\textpm} 1.4, 2009 AIS: 0.2 {\textpm} 0.6; p = .004). Although the prevalence of supplement use has not changed over time, there has been a significant increase in the number and type of products they are using. The potential that these changes can be attributed to a Sports Supplement Program merit investigation.},
  langid = {english},
  pmid = {27206222},
  keywords = {{Fatty Acids, Omega-3},Adolescent,Adult,Athletes,Australia,beta-Alanine,Caffeine,Cohort Studies,Dietary Proteins,Dietary Supplements,Electrolytes,ergogenic,Female,Glucosamine,Glutamine,Humans,Male,Micronutrients,performance,Pilot Projects,Probiotics,Sex Factors,Sodium Bicarbonate,sport food,Surveys and Questionnaires,Swimming,Young Adult,Zinc}
}

@article{shenSystematicReviewSorafenib2013,
  title = {A Systematic Review of Sorafenib in Child-Pugh {{A}} Patients with Unresectable Hepatocellular Carcinoma},
  author = {Shen, A. and Tang, C. and Wang, Y. and Chen, Y. and Yan, X. and Zhang, C. and Liu, R. and Wei, X. and Zhu, Y. and Zhang, H. and Wu, Z.},
  year = {2013},
  journal = {Journal of Clinical Gastroenterology},
  volume = {47},
  number = {10},
  pages = {871--880},
  doi = {10.1097/MCG.0b013e3182a87cfd}
}

@misc{ShiftingLevelSelection,
  title = {Shifting the {{Level}} of {{Selection}} in {{Science}} - {{Leo Tiokhin}}, {{Karthik Panchanathan}}, {{Paul E}}. {{Smaldino}}, {{Dani{\"e}l Lakens}}, 2024},
  urldate = {2025-06-05},
  howpublished = {https://journals.sagepub.com/doi/10.1177/17456916231182568},
  file = {/Users/cristian/Zotero/storage/GDGNAVLZ/17456916231182568.html}
}

@article{sigurdsonHomeopathyCanOffer2023,
  title = {Homeopathy Can Offer Empirical Insights on Treatment Effects in a Null Field},
  author = {Sigurdson, Matthew K. and Sainani, Kristin L. and Ioannidis, John P. A.},
  year = {2023},
  month = mar,
  journal = {Journal of Clinical Epidemiology},
  volume = {155},
  pages = {64--72},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2023.01.010},
  abstract = {OBJECTIVES: A "null field" is a scientific field where there is nothing to discover and where observed associations are thus expected to simply reflect the magnitude of bias. We aimed to characterize a null field using a known example, homeopathy (a pseudoscientific medical approach based on using highly diluted substances), as a prototype. STUDY DESIGN AND SETTING: We identified 50 randomized placebo-controlled trials of homeopathy interventions from highly cited meta-analyses. The primary outcome variable was the observed effect size in the studies. Variables related to study quality or impact were also extracted. RESULTS: The mean effect size for homeopathy was 0.36 standard deviations (Hedges' g; 95\% confidence interval: 0.21, 0.51) better than placebo, which corresponds to an odds ratio of 1.94 (95\% CI: 1.69, 2.23) in favor of homeopathy. 80\% of studies had positive effect sizes (favoring homeopathy). Effect size was significantly correlated with citation counts from journals in the directory of open-access journals and CiteWatch. We identified common statistical errors in 25 studies. CONCLUSION: A null field like homeopathy can exhibit large effect sizes, high rates of favorable results, and high citation impact in the published scientific literature. Null fields may represent a useful negative control for the scientific process.},
  langid = {english},
  pmid = {36736709},
  keywords = {Bias,Homeopathy,Humans,Meta-Research,Null field,Odds Ratio,Replication crisis,Research integrity,Treatment effects}
}

@article{silberzahnManyAnalystsOne2018,
  title = {Many Analysts, One Data Set: {{Making}} Transparent How Variations in Analytic Choices Affect Results},
  shorttitle = {Many Analysts, One Data Set},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn{\'i}k, {\v S}. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Cervantes, I. Flores and Fong, N. and {Gamez-Djokic}, M. and Glenz, A. and {Gordon-McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Mohr, A. J. Hofelich and H{\"o}gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl{\"u}ter, E. and Sch{\"o}nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp{\"o}rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E. -J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  publisher = {Sage Publications},
  address = {US},
  issn = {2515-2467},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 1(4) of Advances in Methods and Practices in Psychological Science (see record 2019-03399-011). This article was originally submitted for publication to the Editor of Advances in Methods and Practices in Psychological Science (AMPPS) in 2015.When the submitted manuscript was subsequently posted online (Silberzahn et al., 2015), it received some media attention, and two of the authors were invited to write a brief commentary in Nature advocating for greater crowdsourcing of data analysis by scientists. The authors forgot to add a citation of the Nature commentary to the final published version of the AMPPS article or to note that the main findings had been previously publicized via the commentary, the online preprint, research presentations at conferences and universities, and media reports by other people. The authors regret the oversight.] [Correction Notice: An Erratum for this article was reported in Vol 1(3) of Advances in Methods and Practices in Psychological Science (see record 2018-67153-004). When the submitted manuscript was subsequently posted online (Silberzahn et al., 2015), it received some media attention, and two of the authors were invited to write a brief commentary in Nature advocating for greater crowdsourcing of data analysis by scientists. This commentary included a new figure that displayed the analytic teams' effect-size estimates and cited the submitted manuscript as the source of the findings, with a link to the preprint. However, the authors forgot to add a citation of the Nature commentary to the final published version of the Advances in Methods and Practices in Psychological Science article or to note that the main findings had been previously publicized. The authors regret the oversight.] Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts' prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Crowdsourcing,Sciences,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/L85ZCAVK/2018-67153-005.html}
}

@article{simmonsFalsepositivePsychologyUndisclosed2011,
  title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {{Data Interpretation, Statistical},{Peer Review, Research},Adult,Computer Simulation,Data Collection,Humans,Practice Guidelines as Topic,Publications,Research Design,Research Personnel,Statistics as Topic,Young Adult},
  file = {/Users/cristian/Zotero/storage/DQFWSY2L/Simmons et al. - 2011 - False-positive psychology undisclosed flexibility.pdf}
}

@article{simmonsPowerPosingPCurving2017,
  title = {Power {{Posing}}: {{P-Curving}} the {{Evidence}}},
  author = {Simmons, Joseph P. and Simonsohn, Uri},
  year = {2017},
  month = may,
  journal = {Psychological Science},
  volume = {28},
  number = {5},
  pages = {687--693},
  doi = {10.1177/0956797616658563},
  keywords = {{Power, Psychological},Humans,Nonverbal Communication,Posture,Risk-Taking},
  file = {/Users/cristian/Zotero/storage/TQMG8DGM/Simmons and Simonsohn - 2017 - Power Posing P-Curving the Evidence.pdf}
}

@article{simonsohn_p-curve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer.},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  doi = {10.1037/a0033242}
}

@article{simonsohnOfficialUserGuidePcurve2015,
  title = {Official {{User-Guide}} to the {{P-curve}}},
  author = {Simonsohn, Uri and Nelson, Leif and Simmons, Joe},
  year = {2015},
  pages = {6},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/I686C6LE/Simonsohn et al. - Official User-Guide to the P-curve.pdf}
}

@article{simonsohnPCurveEffectSize2014,
  title = {P-{{Curve}} and {{Effect Size}}: {{Correcting}} for {{Publication Bias Using Only Significant Results}}},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {666--681},
  doi = {10.1177/1745691614553988},
  abstract = {Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by reanalyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by reanalyzing the meta-analysis of the "choice overload" literature.},
  langid = {english},
  pmid = {26186117},
  keywords = {Computer Simulation,p-curve,p-hacking,publication bias,Publication Bias,Statistics as Topic}
}

@article{simonsohnPcurveKeyFiledrawer2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves---containing more low (.01s) than high (.04s) significant p values---only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Psychology,Scientific Communication,Statistics},
  file = {/Users/cristian/Zotero/storage/IA23FTYH/2013-25331-001.html}
}

@article{simonsohnSmallTelescopesDetectability2015,
  title = {Small {{Telescopes}}: {{Detectability}} and the {{Evaluation}} of {{Replication Results}}},
  shorttitle = {Small {{Telescopes}}},
  author = {Simonsohn, Uri},
  year = {2015},
  month = may,
  journal = {Psychological Science},
  volume = {26},
  number = {5},
  pages = {559--569},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797614567341},
  urldate = {2021-02-16},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ``unsuccessful'' replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ``protecting'' true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  keywords = {hypothesis testing,open materials,replication,statistical power}
}

@misc{SimulationBasedPowerAnalysis,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}} - {{Dani{\"e}l Lakens}}, {{Aaron R}}. {{Caldwell}}, 2021},
  urldate = {2024-10-13},
  howpublished = {https://journals.sagepub.com/doi/10.1177/2515245920951503},
  file = {/Users/cristian/Zotero/storage/Q4FMKCL2/2515245920951503.html}
}

@article{sivannaComparativeStudyPharmacological2015,
  title = {A Comparative Study of Pharmacological Myocardial Protection between Sevoflurane and Desflurane at Anaesthestic Doses in Patients Undergoing off Pump Coronary Artery Bypass Grafting Surgery},
  author = {Sivanna, U. and Joshi, S. and Babu, B. and Jagadeesh, A.M.},
  year = {2015},
  journal = {Indian Journal of Anaesthesia},
  volume = {59},
  number = {5},
  pages = {282--286},
  publisher = {Indian Society of Anaesthetists},
  doi = {10.4103/0019-5049.156867}
}

@article{sladekovaEstimatingChangeMetaanalytic2022,
  title = {Estimating the Change in Meta-Analytic Effect Size Estimates after the Application of Publication Bias Adjustment Methods},
  author = {Sladekova, Martina and Webb, Lois E. A. and Field, Andy P.},
  year = {2022},
  month = apr,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000470},
  abstract = {Publication bias poses a challenge for accurately synthesizing research findings using meta-analysis. A number of statistical methods have been developed to combat this problem by adjusting the meta-analytic estimates. Previous studies tended to apply these methods without regard to optimal conditions for each method's performance. The present study sought to estimate the typical effect size attenuation of these methods when they are applied to real meta-analytic data sets that match the conditions under which each method is known to remain relatively unbiased (such as sample size, level of heterogeneity, population effect size, and the level of publication bias). Four-hundred and 33 data sets from 90 articles published in psychology journals were reanalyzed using a selection of publication bias adjustment methods. The downward adjustment found in our sample was minimal, with greatest identified attenuation of b = -.032, 95\% highest posterior density interval (HPD) ranging from -.055 to -.009, for the precision effect test (PET). Some methods tended to adjust upward, and this was especially true for data sets with a sample size smaller than 10. We propose that researchers should seek to explore the full range of plausible estimates for the effects they are studying and note that these methods may not be able to combat bias in small samples (with less than 10 primary studies). We argue that although the effect size attenuation we found tended to be minimal, this should not be taken as an indication of low levels of publication bias in psychology. We discuss the findings with reference to new developments in Bayesian methods for publication bias adjustment, and the recent methodological reforms in psychology. (PsycInfo Database Record (c) 2022 APA, all rights reserved).},
  langid = {english},
  pmid = {35446048},
  file = {/Users/cristian/Zotero/storage/UGTAV8I9/Sladekova et al. - 2022 - Estimating the change in meta-analytic effect size.pdf}
}

@article{smaldinoNaturalSelectionBad2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  doi = {10.1098/rsos.160384},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  keywords = {Campbell's Law,cultural evolution,incentives,metascience,replication,statistical power},
  file = {/Users/cristian/Zotero/storage/QRBWZWT9/Smaldino and McElreath - The natural selection of bad science.pdf}
}

@misc{SmallEffectsIndispensable,
  title = {Small {{Effects}}: {{The Indispensable Foundation}} for a {{Cumulative Psychological Science}} - {{Friedrich M}}. {{G{\"o}tz}}, {{Samuel D}}. {{Gosling}}, {{Peter J}}. {{Rentfrow}}, 2022},
  urldate = {2022-12-07},
  howpublished = {https://journals.sagepub.com/doi/abs/10.1177/1745691620984483},
  file = {/Users/cristian/Zotero/storage/53JE76LA/1745691620984483.html}
}

@article{smaragdovAssociationDGAT1Gene2011,
  title = {Association of the {{DGAT1 Gene Polymorphism}} in {{Bulls}} with {{Cow Milk Performance}}},
  author = {Smaragdov, M.G.},
  year = {2011},
  journal = {Russian Journal of Genetics},
  volume = {47},
  number = {1},
  pages = {110--115},
  doi = {10.1134/S1022795411010133}
}

@article{smithFieldHockeySportspecific2020,
  title = {Field Hockey Sport-Specific Postures during Unanticipated Sidestepping: {{Implications}} for Anterior Cruciate Ligament Injury Prevention},
  shorttitle = {Field Hockey Sport-Specific Postures during Unanticipated Sidestepping},
  author = {Smith, Marc and Weir, Gillian and Donnelly, Cyril J. and Alderson, Jacqueline},
  year = {2020},
  month = nov,
  journal = {Journal of Sports Sciences},
  volume = {38},
  number = {22},
  pages = {2603--2610},
  publisher = {Routledge},
  issn = {0264-0414},
  doi = {10.1080/02640414.2020.1794264},
  urldate = {2023-08-10},
  abstract = {Much research has investigated whole-body postures and associated knee joint loading during unanticipated sidestepping (UnSS). However, no research has considered sport-specific postures in field hockey. The purpose of this study was to investigate differences in trunk and lower limb angle and lower extremity joint moment waveforms during UnSS while holding a hockey stick in a flexed posture (HS-UnSS) and traditional UnSS. Additionally, we aimed to determine if differences in posture during HS-UnSS were associated with changes in knee joint moments. Twelve elite female field hockey athletes underwent 3D motion analysis during UnSS and HS-UnSS. Athletes increased trunk (0--100\% of stance phase, hip (0--15\%), knee (12--29\%; 39--59\%; 78--100\%) and ankle (41--57\%) flexion angles, and increased hip flexion (19--24\%; 42--45\%; 75--84\%) and external rotation moments (75--80\%) during HS-UnSS compared with UnSS (p {$<$} 0.05). Flexed postures observed during HS-UnSS did not influence knee flexion and valgus moments when compared with UnSS (p {$>$} 0.05), however knee external rotation moments reduced. Changes in trunk flexion were positively associated with peak knee internal rotation moments from UnSS to HS-UnSS (r = 0.779, p = 0.005). These findings indicate that field hockey players sidestep with significantly different techniques when holding a hockey stick, which should be considered in injury prevention training protocols.},
  pmid = {32734844},
  keywords = {athletes,Female,injury assessment,knee,screening}
}

@article{sommet_interaction,
  title = {How {{Many Participants Do I Need}} to {{Test}} an {{Interaction}}? {{Conducting}} an {{Appropriate Power Analysis}} and {{Achieving Sufficient Power}} to {{Detect}} an {{Interaction}}},
  shorttitle = {How {{Many Participants Do I Need}} to {{Test}} an {{Interaction}}?},
  author = {Sommet, Nicolas and Weissman, David L. and Cheutin, Nicolas and Elliot, Andrew J.},
  year = {2023},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {6},
  number = {3},
  pages = {25152459231178728},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231178728},
  urldate = {2024-09-30},
  abstract = {Power analysis for first-order interactions poses two challenges: (a) Conducting an appropriate power analysis is difficult because the typical expected effect size of an interaction depends on its shape, and (b) achieving sufficient power is difficult because interactions are often modest in size. This article consists of three parts. In the first part, we address the first challenge. We first use a fictional study to explain the difference between power analyses for interactions and main effects. Then, we introduce an intuitive taxonomy of 12 types of interactions based on the shape of the interaction (reversed, fully attenuated, partially attenuated) and the size of the simple slopes (median, smaller, larger), and we offer mathematically derived sample-size recommendations to detect each interaction with a power of .80/.90/.95 (for two-tailed tests in between-participants designs). In the second part, we address the second challenge. We first describe a preregistered metastudy (159 studies from recent articles in influential psychology journals) showing that the median power to detect interactions of a typical size is .18. Then, we use simulations ({$\approx$}900,000,000 data sets) to generate power curves for the 12 types of interactions and test three approaches to increase power without increasing sample size: (a) preregistering one-tailed tests (+21\% gain), (b) using a mixed design (+75\% gain), and (c) preregistering contrast analysis for a fully attenuated interaction (+62\% gain). In the third part, we introduce INT{\texttimes}Power (www.intxpower.com), a web application that enables users to draw their interaction and determine the sample size needed to reach the power of their choice with the option of using/combining these approaches.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/R3Y9I77I/Sommet et al. - 2023 - How Many Participants Do I Need to Test an Interac.pdf}
}

@article{sonderstrup-andersenInvestigationDiabetesResearcher2008,
  ids = {sonderstrup-andersenInvestigationDiabetesResearcher2008a},
  title = {An Investigation into Diabetes Researcher's Perceptions of the {{Journal Impact Factor}} --- Reconsidering Evaluating Research},
  author = {{S{\o}nderstrup-Andersen}, Eva M. and {S{\o}nderstrup-Andersen}, Hans H. K.},
  year = {2008},
  month = aug,
  journal = {Scientometrics},
  volume = {76},
  number = {2},
  pages = {391--406},
  issn = {1588-2861},
  doi = {10.1007/s11192-007-1924-4},
  urldate = {2021-09-01},
  abstract = {Currently the Journal Impact Factors (JIF) attracts considerable attention as components in the evaluation of the quality of research in and between institutions. This paper reports on a questionnaire study of the publishing behaviour and researchers preferences for seeking new knowledge information and the possible influence of JIF on these variables. 54 Danish medical researchers active in the field of Diabetes research took part. We asked the researchers to prioritise a series of scientific journals with respect to which journals they prefer for publishing research and gaining new knowledge. In addition we requested the researchers to indicate whether or not the JIF of the prioritised journals has had any influence on these decisions. Furthermore we explored the perception of the researchers as to what degree the JIF could be considered a reliable, stable or objective measure for determining the scientific quality of journals. Moreover we asked the researchers to judge the applicability of JIF as a measure for doing research evaluations. One remarkable result is that app. 80\% of the researchers share the opinion that JIF does indeed have an influence on which journals they would prefer for publishing. As such we found a statistically significant correlation between how the researchers ranked the journals and the JIF of the ranked journals. Another notable result is that no significant correlation exists between journals where the researchers actually have published papers and journals in which they would prefer to publish in the future measured by JIF. This could be taken as an indicator for the actual motivational influence on the publication behaviour of the researchers. That is, the impact factor actually works in our case. It seems that the researchers find it fair and reliable to use the Journal Impact Factor for research evaluation purposes.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UEZY38TH/Snderstrup-Andersen and Snderstrup-Andersen - 2008 - An investigation into diabetes researchers percep.pdf}
}

@article{songTranscriptomicAnalysisShortterm2020,
  title = {Transcriptomic Analysis of Short-Term Salt Stress Response in Watermelon Seedlings},
  author = {Song, Q. and Joshi, M. and Joshi, V.},
  year = {2020},
  journal = {International Journal of Molecular Sciences},
  volume = {21},
  number = {17},
  pages = {1--22},
  publisher = {MDPI AG},
  doi = {10.3390/ijms21176036}
}

@article{sotolaGarbageGarbageOut2022,
  title = {Garbage {{In}}, {{Garbage Out}}? {{Evaluating}} the {{Evidentiary Value}} of {{Published Meta-analyses Using Z-Curve Analysis}}},
  shorttitle = {Garbage {{In}}, {{Garbage Out}}?},
  author = {Sotola, Lukas K.},
  year = {2022},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {32571},
  issn = {2474-7394},
  doi = {10.1525/collabra.32571},
  urldate = {2022-10-11},
  abstract = {The purpose of the current work was to examine the evidentiary value of the studies that have been included in published meta-analyses as a way of investigating the evidentiary value of the meta-analyses themselves. The studies included in 25 meta-analyses published in the last 10 years in Psychological Bulletin that investigated experimental mean differences were z-curved. Z-curve is a meta-analytic technique that allows one to estimate the predicted replicability, average power, publication bias, and false discovery rate of a population of studies. The results of the z-curves estimated a substantial file drawer in three-quarters of the meta-analyses; and in one-third of the meta-analyses, up to half of the studies are not expected to replicate and up to one-fifth of the studies included could be false positives. Possible reasons for these findings are discussed, and caution in interpreting published meta-analyses is recommended.},
  file = {/Users/cristian/Zotero/storage/LHGAYAFS/Sotola - 2022 - Garbage In, Garbage Out Evaluating the Evidentiar.pdf;/Users/cristian/Zotero/storage/GRIPTGBP/Garbage-In-Garbage-Out-Evaluating-the-Evidentiary.html}
}

@article{sotolaPredictedReplicabilityTwo2022,
  title = {On the Predicted Replicability of Two Decades of Experimental Research on System Justification: {{A Z-curve}} Analysis},
  shorttitle = {On the Predicted Replicability of Two Decades of Experimental Research on System Justification},
  author = {Sotola, Lukas K. and Cred{\'e}, Marcus},
  year = {2022},
  journal = {European Journal of Social Psychology},
  volume = {n/a},
  number = {n/a},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2858},
  urldate = {2022-10-11},
  abstract = {We examine the predicted replicability of experimental research on system justification theory (SJT) by conducting a z-curve analysis. Z-curve is a meta-analytic technique similar to p-curve, but which performs better under conditions of heterogeneity. It estimates the predicted replication rate, average power, false discovery risk, and file drawer ratio (FDR) of a sample of studies. The z-curve based on 116 papers and 232 unique samples suggests that the experimental SJT literature is likely to show low rates of replicability, as indicated by an overall average statistical power of 16\%. Moderator analyses suggest that this may be driven in part by publication pressures, that the replicability of research in this area has improved since 2015, and that studies using system threat manipulations show particularly low estimated replication rates (ERR). Implications for the replicability and validity of the experimental SJT literature are discussed, and recommendations to increase the rigor of research are put forth.},
  langid = {english},
  keywords = {open science,publication bias,replicability,statistical power,system justification theory,z-curve},
  file = {/Users/cristian/Zotero/storage/L5Q247WN/Sotola and Cred - On the predicted replicability of two decades of e.pdf;/Users/cristian/Zotero/storage/KD99QP3R/ejsp.html}
}

@article{speedWhatExerciseSport2000,
  title = {What Exercise and Sport Scientists Don't Understand},
  author = {Speed, Harriet D and Andersen, Mark B},
  year = {2000},
  month = mar,
  journal = {Journal of Science and Medicine in Sport},
  volume = {3},
  number = {1},
  pages = {84--92},
  doi = {10.1016/S1440-2440(00)80051-1},
  abstract = {The power of research design in studies published in the Australian Journal of Science and Medicine in Sport (AJSMS; now the Journal of Science and Medicine in Sport) for the years 1996 and 1997 were analysed for their ability to detect small, medium, and large effects according to Cohen's (1988) conventions. Also examined were the reporting and interpreting of effect sizes and the control for experiment-wise (EW) Type I error rates. From the two years of articles, 29 studies were analysed, and power was computed on 108 different tests of significance. The median power of the studies to detect small, medium, and large effects were .14, .65 and .97, respectively, These results suggest that exercise and sport science research, at least as represented in AJSMS, is probably underpowered and may be limited in detecting small effects, has a better, but still underpowered, chance of detecting medium effects, and has adequate power principally for detecting large effects. The reporting of effect sizes was rare, and adequate interpretation of them was even rarer. The mean EW Type I error rate for all studies was .49. The analyses conducted suggest that much research in exercise science may have substantial Type I and Type II errors. An appeal is made for exercise scientists to conduct power analyses, control for EW error, exercise caution in the interpretation of nonsignificant results, and examine, report, and interpret effect sizes rather than solely rely on p values to determine whether significant changes occurred or significant relationships exist.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/9EDDPRT2/S1440244000800511.html}
}

@article{spencePredictionIntervalWhat2016,
  title = {Prediction {{Interval}}: {{What}} to {{Expect When You}}'re {{Expecting}} {\dots} {{A Replication}}},
  shorttitle = {Prediction {{Interval}}},
  author = {Spence, Jeffrey R. and Stanley, David J.},
  year = {2016},
  month = sep,
  journal = {PLOS ONE},
  volume = {11},
  number = {9},
  pages = {e0162874},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0162874},
  urldate = {2022-03-22},
  abstract = {A challenge when interpreting replications is determining whether the results of a replication ``successfully'' replicate the original study. Looking for consistency between two studies is challenging because individual studies are susceptible to many sources of error that can cause study results to deviate from each other and the population effect in unpredictable directions and magnitudes. In the current paper, we derive methods to compute a prediction interval, a range of results that can be expected in a replication due to chance (i.e., sampling error), for means and commonly used indexes of effect size: correlations and d-values. The prediction interval is calculable based on objective study characteristics (i.e., effect size of the original study and sample sizes of the original study and planned replication) even when sample sizes across studies are unequal. The prediction interval provides an a priori method for assessing if the difference between an original and replication result is consistent with what can be expected due to sample error alone. We provide open-source software tools that allow researchers, reviewers, replicators, and editors to easily calculate prediction intervals.},
  langid = {english},
  keywords = {Clinical psychology,Forecasting,Metaanalysis,Open source software,Reflection,Replication studies,Reproducibility,Social psychology},
  file = {/Users/cristian/Zotero/storage/Q4DBKESR/Spence and Stanley - 2016 - Prediction Interval What to Expect When Youre Ex.pdf;/Users/cristian/Zotero/storage/UBS3HQ4F/article.html}
}

@article{spurrsEffectPlyometricTraining2003,
  title = {The Effect of Plyometric Training on Distance Running Performance},
  author = {Spurrs, Robert W. and Murphy, Aron J. and Watsford, Mark L.},
  year = {2003},
  month = mar,
  journal = {European Journal of Applied Physiology},
  volume = {89},
  number = {1},
  pages = {1--7},
  issn = {1439-6319},
  doi = {10.1007/s00421-002-0741-y},
  abstract = {Previous research has reported that plyometric training improves running economy (RE) and ultimately distance-running performance, although the exact mechanism by which this occurs remains unclear. This study examined whether changes in running performance resulting from plyometric training were related to alterations in lower leg musculotendinous stiffness (MTS). Seventeen male runners were pre- and post-tested for lower leg MTS, maximum isometric force, rate of force development, 5-bound distance test (5BT), counter movement jump (CMJ) height, RE, VO(2max), lactate threshold (Th(la)), and 3-km time. Subjects were randomly split into an experimental (E) group which completed 6 weeks of plyometric training in conjunction with their normal running training, and a control (C) group which trained as normal. Following the training period, the E group significantly improved 3-km performance (2.7\%) and RE at each of the tested velocities, while no changes in VO(2max) or Th(la) were recorded. CMJ height, 5BT, and MTS also increased significantly. No significant changes were observed in any measures for the C group. The results clearly demonstrated that a 6-week plyometric programme led to improvements in 3-km running performance. It is postulated that the increase in MTS resulted in improved RE. We speculate that the improved RE led to changes in 3-km running performance, as there were no corresponding alterations in VO(2max) or Th(la).},
  langid = {english},
  pmid = {12627298},
  keywords = {{Muscle, Skeletal},{Stress, Mechanical},Achilles Tendon,Adult,Elasticity,Exercise,Exercise Test,Humans,Isometric Contraction,Lactic Acid,Male,Oxygen Consumption,Physical Education and Training,Physical Fitness,Running,Task Performance and Analysis,Track and Field}
}

@article{stanleyExpectationsReplicationsAre2014,
  title = {Expectations for {{Replications}}: {{Are Yours Realistic}}?},
  shorttitle = {Expectations for {{Replications}}},
  author = {Stanley, David J. and Spence, Jeffrey R.},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {9},
  number = {3},
  pages = {305--318},
  issn = {1745-6924},
  doi = {10.1177/1745691614528518},
  abstract = {Failures to replicate published psychological research findings have contributed to a "crisis of confidence." Several reasons for these failures have been proposed, the most notable being questionable research practices and data fraud. We examine replication from a different perspective and illustrate that current intuitive expectations for replication are unreasonable. We used computer simulations to create thousands of ideal replications, with the same participants, wherein the only difference across replications was random measurement error. In the first set of simulations, study results differed substantially across replications as a result of measurement error alone. This raises questions about how researchers should interpret failed replication attempts, given the large impact that even modest amounts of measurement error can have on observed associations. In the second set of simulations, we illustrated the difficulties that researchers face when trying to interpret and replicate a published finding. We also assessed the relative importance of both sampling error and measurement error in producing variability in replications. Conventionally, replication attempts are viewed through the lens of verifying or falsifying published findings. We suggest that this is a flawed perspective and that researchers should adjust their expectations concerning replications and shift to a meta-analytic mind-set.},
  langid = {english},
  pmid = {26173266},
  keywords = {individual differences,meta-analysis,methodology,reliability,replication,scientific}
}

@article{stanleyFindingPowerReduce2017,
  title = {Finding the Power to Reduce Publication Bias},
  author = {Stanley, Tom D. and Doucouliagos, Hristos and Ioannidis, John P. A.},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {10},
  pages = {1580--1598},
  issn = {1097-0258},
  doi = {10.1002/sim.7228},
  urldate = {2022-05-13},
  abstract = {The central purpose of this study is to document how a sharper focus upon statistical power may reduce the impact of selective reporting bias in meta-analyses. We introduce the weighted average of the adequately powered (WAAP) as an alternative to the conventional random-effects (RE) estimator. When the results of some of the studies have been selected to be positive and statistically significant (i.e. selective reporting), our simulations show that WAAP will have smaller bias than RE at no loss to its other statistical properties. When there is no selective reporting, the difference between RE's and WAAP's statistical properties is practically negligible. Nonetheless, when selective reporting is especially severe or heterogeneity is very large, notable bias can remain in all weighted averages. The main limitation of this approach is that the majority of meta-analyses of medical research do not contain any studies with adequate power (i.e. {$>$}80\%). For such areas of medical research, it remains important to document their low power, and, as we demonstrate, an alternative unrestricted weighted least squares weighted average can be used instead of WAAP. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {meta-analysis,publication bias,random-effects,statistical power,weighted least squares},
  file = {/Users/cristian/Zotero/storage/6SSCL3NM/sim.html}
}

@article{stanleyLimitationsPETPEESEOther2017,
  title = {Limitations of {{PET-PEESE}} and {{Other Meta-Analysis Methods}}},
  author = {Stanley, Tom D.},
  year = {2017},
  month = jul,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  pages = {194855061769306},
  doi = {10.1177/1948550617693062},
  abstract = {A novel meta-regression method, precision-effect test and precision-effect estimate with standard errors (PET-PEESE), predicts and explains recent high-profile failures to replicate in psychology. The central purpose of this article is to identify the limitations of PET-PEESE for application to social/personality psychology. Using typical conditions found in social/personality research, our simulations identify three areas of concern. PET-PEESE performs poorly in research areas where there are only a few studies, all studies use small samples, and where there is very high heterogeneity of results from study to study. Nonetheless, the statistical properties of conventional meta-analysis approaches are much worse than PET-PEESE under these same conditions. Our simulations suggest alterations to conventional research practice and ways to moderate PET-PEESE weaknesses.}
}

@article{stanleyMetaregressionApproximationsReduce2014,
  title = {Meta-Regression Approximations to Reduce Publication Selection Bias},
  author = {Stanley, Tom D. and Doucouliagos, Hristos},
  year = {2014},
  journal = {Research Synthesis Methods},
  volume = {5},
  number = {1},
  pages = {60--78},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1095},
  urldate = {2022-05-13},
  abstract = {Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations to the conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in most cases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulations also demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regression intercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accommodate systematic heterogeneity along with complex and differential publication selection bias that is related to moderator variables. By providing an intuitive reason for these approximations, we can also explain why the Egger regression works so well and when it does not. These meta-regression methods are applied to several policy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, the minimum wage, and nicotine replacement therapy. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {meta-regression,Methodology,Nicotine,publication selection bias,Simulation,Smoking Cessation,Statistical Regression,systematic reviews,truncation},
  file = {/Users/cristian/Zotero/storage/PQVGMBRC/jrsm.html}
}

@article{stanleyWhatMetaanalysesReveal2018,
  title = {What Meta-Analyses Reveal about the Replicability of Psychological Research},
  author = {Stanley, Tom D. and Carter, Evan C. and Doucouliagos, Hristos},
  year = {2018},
  month = dec,
  journal = {Psychological Bulletin},
  volume = {144},
  number = {12},
  pages = {1325--1346},
  doi = {10.1037/bul0000169},
  abstract = {Can recent failures to replicate psychological research be explained by typical magnitudes of statistical power, bias or heterogeneity? A large survey of 12,065 estimated effect sizes from 200 meta-analyses and nearly 8,000 papers is used to assess these key dimensions of replicability. First, our survey finds that psychological research is, on average, afflicted with low statistical power. The median of median power across these 200 areas of research is about 36\%, and only about 8\% of studies have adequate power (using Cohen's 80\% convention). Second, the median proportion of the observed variation among reported effect sizes attributed to heterogeneity is 74\% (I2). Heterogeneity of this magnitude makes it unlikely that the typical psychological study can be closely replicated when replication is defined as study-level null hypothesis significance testing. Third, the good news is that we find only a small amount of average residual reporting bias, allaying some of the often-expressed concerns about the reach of publication bias and questionable research practices. Nonetheless, the low power and high heterogeneity that our survey finds fully explain recent difficulties to replicate highly regarded psychological studies and reveal challenges for scientific progress in psychology. (PsycINFO Database Record (c) 2018 APA, all rights reserved).},
  langid = {english},
  keywords = {{Data Interpretation, Statistical},Behavioral Research,Humans,Meta-Analysis as Topic,Psychology,Publication Bias,Reproducibility of Results,Research Design}
}

@misc{StatisticalIssuesDrug,
  title = {Statistical {{Issues}} in {{Drug Development}} {\textbar} {{Wiley Online Books}}},
  urldate = {2025-08-22},
  howpublished = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119238614},
  file = {/Users/cristian/Zotero/storage/4PYZC8MG/9781119238614.html}
}

@article{steeleMythPeriodisation2023,
  ids = {steeleMythPeriodisation2023a},
  title = {The {{Myth}} of "{{Periodisation}}"},
  author = {Steele, James and Fisher, James and Loenneke, Jeremy and Buckner, Samuel},
  year = {2023},
  month = aug,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.323},
  urldate = {2023-09-06},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {competitive sport,periodisation,philosophy of science,strength and conditioning,theory},
  file = {/Users/cristian/Zotero/storage/5LN5E4U4/Steele et al. - 2023 - The Myth of Periodisation.pdf}
}

@article{stefan_big_little_lies,
  ids = {stefanBigLittleLies2022},
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, A. M. and Sch{\"o}nbrodt, Felix D.},
  year = {2023},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2023-02-13},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,Error Rates,False Positive Results,false-positive rate,Meta-science,p-curve,p-Curve,Quantitative Methods,questionable research practices,Questionable Research Practices,Shiny app,Shiny App,significance,Significance,simulation,Simulation,Social and Behavioral Sciences,Statistical Methods},
  file = {/Users/cristian/Zotero/storage/P5IJHMR6/Stefan and Schnbrodt - 2022 - Big Little Lies A Compendium and Simulation of p-.pdf;/Users/cristian/Zotero/storage/X7DJUPS5/Stefan and Schnbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf}
}

@article{sterlingPublicationDecisionsRevisited1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
  year = {1995},
  journal = {The American Statistician},
  volume = {49},
  number = {1},
  pages = {108--112},
  doi = {10.2307/2684823},
  abstract = {This article presents evidence that published results of scientific investigations are not a representative sample of results of all scientific studies. Research studies from 11 major journals demonstrate the existence of biases that favor studies that observe effects that, on statistical evaluation, have a low probability of erroneously rejecting the so-called null hypothesis (H 0). This practice makes the probability of erroneously rejecting H 0 different for the reader than for the investigator. It introduces two biases in the interpretation of the scientific literature: one due to multiple repetition of studies with false hypothesis, and one due to failure to publish smaller and less significant outcomes of tests of a true hypotheses. These practices distort the results of literature surveys and of meta-analyses. These results also indicate that practice leading to publication bias have not changed over a period of 30 years.},
  keywords = {Bias,Null results,Publication bias,Tests of significance},
  annotation = {https://doi.org/10.2307/2684823},
  file = {/Users/cristian/Zotero/storage/TVPKKJZX/00031305.1995.html}
}

@article{sterlingPublicationDecisionsTheir1959,
  title = {Publication Decisions and Their Possible Effects on Inferences Drawn from Tests of Significance---or Vice Versa},
  author = {Sterling, Theodore D.},
  year = {1959},
  journal = {Journal of the American Statistical Association},
  volume = {54},
  pages = {30--34},
  publisher = {American Statistical Association},
  address = {US},
  issn = {1537-274X},
  doi = {10.2307/2282137},
  abstract = {"There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs---an 'error of the first kind'---and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/cristian/Zotero/storage/W8RJY5EI/1960-05032-001.html}
}

@article{sternePublicationRelatedBias2000,
  title = {Publication and Related Bias in Meta-Analysis: Power of Statistical Tests and Prevalence in the Literature},
  shorttitle = {Publication and Related Bias in Meta-Analysis},
  author = {Sterne, J. A. and Gavaghan, D. and Egger, M.},
  year = {2000},
  month = nov,
  journal = {Journal of Clinical Epidemiology},
  volume = {53},
  number = {11},
  pages = {1119--1129},
  issn = {0895-4356},
  doi = {10.1016/s0895-4356(00)00242-0},
  abstract = {Publication and selection biases in meta-analysis are more likely to affect small studies, which also tend to be of lower methodological quality. This may lead to "small-study effects," where the smaller studies in a meta-analysis show larger treatment effects. Small-study effects may also arise because of between-trial heterogeneity. Statistical tests for small-study effects have been proposed, but their validity has been questioned. A set of typical meta-analyses containing 5, 10, 20, and 30 trials was defined based on the characteristics of 78 published meta-analyses identified in a hand search of eight journals from 1993 to 1997. Simulations were performed to assess the power of a weighted regression method and a rank correlation test in the presence of no bias, moderate bias or severe bias. We based evidence of small-study effects on P {$<$} 0.1. The power to detect bias increased with increasing numbers of trials. The rank correlation test was less powerful than the regression method. For example, assuming a control group event rate of 20\% and no treatment effect, moderate bias was detected with the regression test in 13.7\%, 23.5\%, 40.1\% and 51.6\% of meta-analyses with 5, 10, 20 and 30 trials. The corresponding figures for the correlation test were 8.5\%, 14.7\%, 20.4\% and 26.0\%, respectively. Severe bias was detected with the regression method in 23.5\%, 56.1\%, 88.3\% and 95.9\% of meta-analyses with 5, 10, 20 and 30 trials, as compared to 11.9\%, 31.1\%, 45.3\% and 65.4\% with the correlation test. Similar results were obtained in simulations incorporating moderate treatment effects. However the regression method gave false-positive rates which were too high in some situations (large treatment effects, or few events per trial, or all trials of similar sizes). Using the regression method, evidence of small-study effects was present in 21 (26.9\%) of the 78 published meta-analyses. Tests for small-study effects should routinely be performed in meta-analysis. Their power is however limited, particularly for moderate amounts of bias or meta-analyses based on a small number of small studies. When evidence of small-study effects is found, careful consideration should be given to possible explanations for these in the reporting of the meta-analysis.},
  langid = {english},
  pmid = {11106885},
  keywords = {{Statistics, Nonparametric},Bias,Clinical Trials as Topic,Meta-Analysis as Topic,Publication Bias,Regression Analysis,Reproducibility of Results,Statistics as Topic}
}

@article{stewartDetectingDoseResponse2000,
  title = {Detecting Dose Response with Contrasts},
  author = {Stewart, W. H. and Ruberg, S. J.},
  year = {2000},
  month = apr,
  journal = {Statistics in Medicine},
  volume = {19},
  number = {7},
  pages = {913--921},
  issn = {0277-6715},
  doi = {10.1002/(sici)1097-0258(20000415)19:7<913::aid-sim397>3.0.co;2-2},
  abstract = {Analyses of dose response studies should separate the question of the existence of a dose response relationship from questions of functional form and finding the optimal dose. A well-chosen contrast among the estimated effects of the studied doses can make a powerful test for detecting the existence of a dose response relationship. A contrast-based test attains its greatest power when the pattern of the coefficients has the same shape as the true dose response relationship. However, it loses power when the contrast shape and the true dose response shape are not similar. Thus, a primary test based on a single contrast is often risky. Two (or more) appropriately chosen contrasts can assure sufficient power to justify the cost of a multiplicity adjustment. An example shows the success of a two-contrast procedure in detecting dose response, which had frustrated several standard procedures.},
  langid = {english},
  pmid = {10750059},
  keywords = {{Data Interpretation, Statistical},{Dose-Response Relationship, Drug},{Drug Evaluation, Preclinical},Antiemetics,Clinical Trials as Topic,Humans,Nausea,Randomized Controlled Trials as Topic,Research Design,Sample Size,Vomiting}
}

@article{stewartSLCO1B1PolymorphismsStatinInduced2013,
  title = {{{SLCO1B1 Polymorphisms}} and {{Statin-Induced Myopathy}}},
  author = {Stewart, A.},
  year = {2013},
  journal = {PLoS Currents},
  number = {DEC},
  publisher = {Public Library of Science},
  doi = {10.1371/currents.eogt.d21e7f0c58463571bb0d9d3a19b82203}
}

@article{sullivanUsingEffectSize2012,
  title = {Using {{Effect Size}}---or {{Why}} the {{P Value Is Not Enough}}},
  author = {Sullivan, Gail M. and Feinn, Richard},
  year = {2012},
  month = sep,
  journal = {Journal of Graduate Medical Education},
  volume = {4},
  number = {3},
  pages = {279--282},
  doi = {10.4300/JGME-D-12-00156.1},
  file = {/Users/cristian/Zotero/storage/FXS3UYJR/Sullivan and Feinn - 2012 - Using Effect Sizeor Why the P Value Is Not Enough.pdf}
}

@article{sunEffectsCaffeineIngestion2022,
  title = {Effects of Caffeine Ingestion on Physiological Indexes of Human Neuromuscular Fatigue: {{A}} Systematic Review and Meta-Analysis},
  author = {Sun, R. and Sun, J. and Li, J. and Li, S.},
  year = {2022},
  journal = {Brain and Behavior},
  volume = {12},
  number = {4},
  publisher = {{John Wiley and Sons Ltd}},
  doi = {10.1002/brb3.2529}
}

@article{sunHowUseSubgroup2014,
  title = {How to Use a Subgroup Analysis: Users' Guide to the Medical Literature},
  shorttitle = {How to Use a Subgroup Analysis},
  author = {Sun, Xin and Ioannidis, John P. A. and Agoritsas, Thomas and Alba, Ana C. and Guyatt, Gordon},
  year = {2014-01-22/0029},
  journal = {JAMA},
  volume = {311},
  number = {4},
  pages = {405--411},
  issn = {1538-3598},
  doi = {10.1001/jama.2013.285063},
  abstract = {Clinicians, when trying to apply trial results to patient care, need to individualize patient care and, potentially, manage patients based on results of subgroup analyses. Apparently compelling subgroup effects often prove spurious, and guidance is needed to differentiate credible from less credible subgroup claims. We therefore provide 5 criteria to use when assessing the validity of subgroup analyses: (1) Can chance explain the apparent subgroup effect; (2) Is the effect consistent across studies; (3) Was the subgroup hypothesis one of a small number of hypotheses developed a priori with direction specified; (4) Is there strong preexisting biological support; and (5) Is the evidence supporting the effect based on within- or between-study comparisons. The first 4 criteria are applicable to individual studies or systematic reviews, the last only to systematic reviews of multiple studies. These criteria will help clinicians deciding whether to use subgroup analyses to guide their patient care.},
  langid = {english},
  pmid = {24449319},
  keywords = {{Data Interpretation, Statistical},Decision Making,Evidence-Based Medicine,Guidelines as Topic,Meta-Analysis as Topic,Patient Care,Randomized Controlled Trials as Topic,Research Design,Review Literature as Topic}
}

@article{sunMetaanalysisEvaluatingEffects2022,
  title = {A Meta-Analysis Evaluating Effects of the Rotigotine in {{Parkinson}}'s Disease, Focusing on Sleep Disturbances and Activities of Daily Living},
  author = {Sun, W. and Wang, Q. and Yang, T. and Feng, C. and Qu, Y. and Yang, Y. and Li, C. and Sun, Z. and Asakawa, T.},
  year = {2022},
  journal = {Neurological Sciences},
  volume = {43},
  number = {10},
  pages = {5821--5837},
  publisher = {Springer-Verlag Italia s.r.l.},
  doi = {10.1007/s10072-022-06159-9}
}

@article{swiatkowskiReplicabilityCrisisSocial2017,
  title = {Replicability Crisis in Social Psychology: {{Looking}} at the Past to Find New Pathways for the Future},
  shorttitle = {Replicability Crisis in Social Psychology},
  author = {{\'S}wi{\k a}tkowski, Wojciech and Dompnier, Beno{\^i}t},
  year = {2017},
  journal = {International Review of Social Psychology},
  volume = {30},
  number = {1},
  publisher = {Ubiquity Press},
  address = {United Kingdom},
  issn = {2397-8570},
  doi = {10.5334/irsp.66},
  abstract = {Over the last few years, psychology researchers have become increasingly preoccupied with the question of whether findings from psychological studies are generally replicable. The debates have originated from some unfortunate events of scientific misconduct in the field, and they have reached a climax with the recent discovery of a relatively weak rate of replicability of published literature, leading to the so-called replicability crisis in psychology. The present paper is concerned with examining the issue of replicability in the field of social psychology. We begin by drawing a state of the art of the crisis in this field. We then highlight some possible causes for the crisis, discussing topics of statistical power, questionable research practices, publication standards, and hidden auxiliary assumptions of context-dependency of social psychological theories. Finally, we argue that given the absence of absolute falsification in science, social psychology could greatly benefit from adopting McGuire's perspectivist approach to knowledge construction. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Interpersonal Control,Knowledge (General),Social Psychology},
  file = {/Users/cristian/Zotero/storage/YD3IVZCG/witkowski and Dompnier - 2017 - Replicability crisis in social psychology Looking.pdf;/Users/cristian/Zotero/storage/79P8PAEH/2017-30130-001.html}
}

@article{swiftQuestionableResearchPractices2022,
  title = {Questionable Research Practices among Faculty and Students in {{APA-accredited}} Clinical and Counseling Psychology Doctoral Programs},
  author = {Swift, Joshua K. and Christopherson, Cody D. and Bird, Megan O. and Z{\"o}ld, Amanda and Goode, Jonathan},
  year = {2022},
  journal = {Training and Education in Professional Psychology},
  volume = {16},
  pages = {299--305},
  publisher = {Educational Publishing Foundation},
  address = {US},
  issn = {1931-3926},
  doi = {10.1037/tep0000322},
  abstract = {This study examines self-reported engagement in questionable research practices (QRPs) by faculty (N = 164) and students (N = 110) in American Psychological Association--accredited clinical and counseling psychology doctoral programs. Both faculty and student participants were asked to report their own engagement as well as the engagement of their graduate school mentor in 12 QRPs. Nearly 65\% of the faculty participants and 50\% of the student participants reported engaging in at least one QRP. The most commonly reported QRP was selectively reporting findings that worked (35\% for faculty, 26\% for students), and the least commonly admitted was falsifying data (0\% for faculty, 1\% for students). Total number of QRPs engaged in was significantly predicted by knowledge of mentor engagement in QRPs (explaining 34\% of the variance for faculty and 19\% of the variance for students), but it was not predicted by degree year, number of publications, or self-reported researcher reputation. These results suggest that QRPs do occur in the field but perhaps at lower levels than had previously been thought. They also suggest that additional training in QRPs is needed. Training implications and future directions are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Clinical Psychology Graduate Training,College Teachers,Counseling Psychology,Ethics,Experimental Ethics,Experimentation,Graduate Psychology Education,Graduate Students,Mentor,Self-Report,Test Construction,Training},
  file = {/Users/cristian/Zotero/storage/HJ53LDUL/2020-29257-001.html}
}

@misc{swintonComparativeEffectSize2022,
  title = {Comparative Effect Size Distributions in Strength and Conditioning and Implications for Future Research: {{A}} Meta-Analysis},
  shorttitle = {Comparative Effect Size Distributions in Strength and Conditioning and Implications for Future Research},
  author = {Swinton, Paul A. and Murphy, Andrew},
  year = {2022},
  month = sep,
  publisher = {SportRxiv},
  doi = {10.51224/SRXIV.202},
  urldate = {2025-04-04},
  archiveprefix = {SportRxiv},
  langid = {english},
  keywords = {applied statistics,Bayesian,Power,S\&C,Sample size},
  file = {/Users/cristian/Zotero/storage/9NSHVE5H/Swinton and Murphy - 2022 - Comparative effect size distributions in strength .pdf}
}

@article{swintonInterpretingMagnitudeChange2022,
  title = {Interpreting Magnitude of Change in Strength and Conditioning: {{Effect}} Size Selection, Threshold Values and {{Bayesian}} Updating},
  shorttitle = {Interpreting Magnitude of Change in Strength and Conditioning},
  author = {Swinton, Paul A. and Burgess, Katherine and Hall, Andy and Greig, Leon and Psyllas, John and Aspe, Rodrigo and Maughan, Patrick and Murphy, Andrew},
  year = {2022},
  month = oct,
  journal = {Journal of Sports Sciences},
  pages = {1--8},
  issn = {1466-447X},
  doi = {10.1080/02640414.2022.2128548},
  abstract = {The magnitude of change following strength and conditioning (S\&C) training can be evaluated comparing effect sizes to thresholds. This study conducted a series of meta-analyses and compiled results to identify thresholds specific to S\&C, and create prior distributions for Bayesian updating. Pre- and post-training data from S\&C interventions were translated into standardised mean difference (SMDpre) and percentage improvement (\%Improve) effect sizes. Bayesian hierarchical meta-analysis models were conducted to compare effect sizes, develop prior distributions, and estimate 0.25-, 0.5-, and 0.75-quantiles to determine small, medium, and large thresholds, respectively. Data from 643 studies comprising 6574 effect sizes were included in the analyses. Large differences in distributions for both SMDpre and \%Improve were identified across outcome domains (strength, power, jump and sprint performance), with analyses of the tails of the distributions indicating potential large overestimations of SMDpre values. Future evaluations of S\&C training will be improved using Bayesian approaches featuring the information and priors developed in this study. To facilitate an uptake of Bayesian methods within S\&C, an easily accessible tool employing intuitive Bayesian updating was created. It is recommended that the tool and specific thresholds be used instead of isolated effect size calculations and Cohen's generic values when evaluating S\&C training.},
  langid = {english},
  pmid = {36184114},
  keywords = {Effect size,power,prior,S\&C},
  file = {/Users/cristian/Zotero/storage/MZML685M/Swinton et al. - 2022 - Interpreting magnitude of change in strength and c.pdf}
}

@article{swintonWhatAreSmall2023,
  title = {What Are Small, Medium and Large Effect Sizes for Exercise Treatments of Tendinopathy? {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {What Are Small, Medium and Large Effect Sizes for Exercise Treatments of Tendinopathy?},
  author = {Swinton, Paul A. and Shim, Joanna S. C. and Pavlova, Anastasia Vladimirovna and Moss, Rachel and Maclean, Colin and Brandie, David and Mitchell, Laura and Greig, Leon and Parkinson, Eva and Brown, Victoria Tzortziou and Morrissey, Dylan and Alexander, Lyndsay and Cooper, Kay},
  year = {2023},
  month = feb,
  journal = {BMJ Open Sport \& Exercise Medicine},
  volume = {9},
  number = {1},
  pages = {e001389},
  publisher = {BMJ Specialist Journals},
  issn = {2055-7647},
  doi = {10.1136/bmjsem-2022-001389},
  urldate = {2023-10-18},
  abstract = {Objective To quantify and describe effect size distributions from exercise therapies across a range of tendinopathies and outcome domains to inform future research and clinical practice through conducting a systematic review with meta-analysis. Design Systematic review with meta-analysis exploring moderating effects and context-specific small, medium and large thresholds. Eligibility criteria Randomised and quasi-randomised controlled trials involving any persons with a diagnosis of rotator cuff, lateral elbow, patellar, Achilles or gluteal tendinopathy of any severity or duration. Methods Common databases, six trial registries and six grey literature databases were searched on 18 January 2021 (PROSPERO: CRD42020168187). Standardised mean difference (SMDpre) effect sizes were used with Bayesian hierarchical meta-analysis models to calculate the 0.25 (small), 0.5 (medium) and 0.75 quantiles (large) and compare pooled means across potential moderators. Risk of bias was assessed with Cochrane's Risk of Bias tool. Results Data were obtained from 114 studies comprising 171 treatment arms 4104 participants. SMDpre effect sizes were similar across tendinopathies but varied across outcome domains. Greater threshold values were obtained for self-reported measures of pain (small=0.5, medium=0.9 and large=1.4), disability (small=0.6, medium=1.0 and large=1.5) and function (small=0.6, medium=1.1 and large=1.8) and lower threshold values obtained for quality of life (small=-0.2, medium=0.3 and large=0.7) and objective measures of physical function (small=0.2, medium=0.4 and large=0.7). Potential moderating effects of assessment duration, exercise supervision and symptom duration were also identified, with greater pooled mean effect sizes estimated for longer assessment durations, supervised therapies and studies comprising patients with shorter symptom durations. Conclusion The effect size of exercise on tendinopathy is dependent on the type of outcome measure assessed. Threshold values presented here can be used to guide interpretation and assist with further research better establishing minimal important change.},
  chapter = {Original research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See:~https://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  keywords = {meta-analysis,rehabilitation,statistics,tendinopathy},
  file = {/Users/cristian/Zotero/storage/NHM6KA97/Swinton et al. - 2023 - What are small, medium and large effect sizes for .pdf}
}

@article{syahniarGlycerineFeedSupplement2021,
  title = {Glycerine as a Feed Supplement for Beef and Dairy Cattle: {{A}} Meta-Analysis on Performance, Rumen Fermentation, Blood Metabolites and Product Characteristics},
  author = {Syahniar, T.M. and Andriani, M. and Ridla, M. and Laconi, E.B. and Nahrowi, N. and Jayanegara, A.},
  year = {2021},
  journal = {Journal of Animal Physiology and Animal Nutrition},
  volume = {105},
  number = {3},
  pages = {419--430},
  publisher = {Blackwell Publishing Ltd},
  doi = {10.1111/jpn.13468}
}

@article{szucsEmpiricalAssessmentPublished2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLoS Biology},
  volume = {19},
  number = {3},
  pages = {e3001151},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64--1.46) for nominally statistically significant results and D = 0.24 (0.11--0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience., Biomedical science, psychology, and many other fields may be suffering from a serious replication crisis. In order to gain insight into some factors behind this crisis, we have analyzed statistical information extracted from thousands of cognitive neuroscience and psychology research papers. We established that the statistical power to discover existing relationships has not improved during the past half century. A consequence of low statistical power is that research studies are likely to report many false positive findings. Using our large dataset, we estimated the probability that a statistically significant finding is false (called false report probability). With some reasonable assumptions about how often researchers come up with correct hypotheses, we conclude that more than 50\% of published findings deemed to be statistically significant are likely to be false. We also observed that cognitive neuroscience studies had higher false report probability than psychology studies, due to smaller sample sizes in cognitive neuroscience. In addition, the higher the impact factors of the journals in which the studies were published, the lower was the statistical power. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  file = {/Users/cristian/Zotero/storage/BW987VQP/Szucs and Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{szucsWhenNullHypothesis2017,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  pages = {390},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.},
  langid = {english},
  pmcid = {PMC5540883},
  pmid = {28824397},
  keywords = {Bayesian methods,false positive findings,null hypothesis significance testing,replication crisis,research methodology},
  file = {/Users/cristian/Zotero/storage/5PBLDGAY/Szucs and Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{takeiNoInfluenceAcute2021,
  ids = {takeiNoInfluenceAcute2020,takeiNoInfluenceAcute2021a},
  title = {No {{Influence}} of {{Acute Moderate Normobaric Hypoxia}} on {{Performance}} and {{Blood Lactate Concentration Responses}} to {{Repeated Wingates}}},
  author = {Takei, Naoya and Kakinoki, Katsuyuki and Girard, Olivier and Hatta, Hideo},
  year = {2021},
  month = jan,
  journal = {International Journal of Sports Physiology and Performance},
  volume = {16},
  number = {1},
  pages = {154--157},
  publisher = {Human Kinetics},
  issn = {1555-0273},
  doi = {10.1123/ijspp.2019-0933},
  abstract = {BACKGROUND: Training in hypoxia versus normoxia often induces larger physiological adaptations, while this does not always translate into additional performance benefits. A possible explanation is a reduced oxygen flux, negatively affecting training intensity and/or volume (decreasing training stimulus). Repeated Wingates (RW) in normoxia is an efficient training strategy for improving both physiological parameters and exercise capacity. However, it remains unclear whether the addition of hypoxia has a detrimental effect on RW performance. PURPOSE: To test the hypothesis that acute moderate hypoxia exposure has no detrimental effect on RW, while both metabolic and perceptual responses would be slightly higher. METHODS: On separate days, 7 male university sprinters performed 3 {\texttimes} 30-s Wingate efforts with 4.5-min passive recovery in either hypoxia (FiO2: 0.145) or normoxia (FiO2: 0.209). Arterial oxygen saturation was assessed before the first Wingate effort, while blood lactate concentration and ratings of perceived exertion were measured after each bout. RESULTS: Mean (P = .92) and peak (P = .63) power outputs, total work (P = .98), and the percentage decrement score (P = .25) were similar between conditions. Arterial oxygen saturation was significantly lower in hypoxia versus normoxia (92.0\%~[2.8\%] vs 98.1\% [0.4\%], P {$<$} .01), whereas blood lactate concentration (P = .78) and ratings of perceived exertion (P = .51) did not differ between conditions. CONCLUSION: In sprinters, acute exposure to moderate hypoxia had no detrimental effect on RW performance and associated metabolic and perceptual responses.},
  chapter = {International Journal of Sports Physiology and Performance},
  langid = {english},
  pmid = {33120358},
  keywords = {{Physical Conditioning, Human},Athletic Performance,exercise performance,exercise physiology,Exercise Test,Humans,Hypoxia,intermittent sprint,Lactic Acid,Male,Oxygen Consumption,Physical Exertion,Running}
}

@article{tamminenOpenScienceSport2018,
  title = {Open Science in Sport and Exercise Psychology: {{Review}} of Current Approaches and Considerations for Qualitative Inquiry},
  shorttitle = {Open Science in Sport and Exercise Psychology},
  author = {Tamminen, Katherine A. and Poucher, Zo{\"e} A.},
  year = {2018},
  month = may,
  journal = {Psychology of Sport and Exercise},
  volume = {36},
  pages = {17--28},
  issn = {1469-0292},
  doi = {10.1016/j.psychsport.2017.12.010},
  urldate = {2022-01-07},
  abstract = {Open science practices including open access (OA) publication, open methods, study preregistration, and open data are gaining acceptance across diverse fields of research. These practices are promoted as strategies to improve the reproducibility of research findings and the replicability of studies to accumulate knowledge and advance science. However, these arguments may raise concerns for qualitative researchers, and open science practices pose several challenges for qualitative researchers. The purpose of this paper is: (1) to review the state of open science practices within sport and exercise psychology, and (2) to discuss the implications of open science for qualitative inquiry. We examined open science practices across quantitative and qualitative articles in 11 sport and exercise psychology journals. While OA publication is a relatively recent phenomenon, OA articles were cited slightly more often than non-OA articles, although this difference was not significant. Some researchers provided supplementary materials alongside published articles, but researchers do not appear to be openly sharing the methods and data from their studies. No articles were published as preregistered studies at the time of our review. Some benefits of open science practices for qualitative inquiry include transparent documentation of the research process, opportunities for collaborative and pluralistic analyses, access to data across multiple research sites and from difficult-to-access settings and participants, and opportunities for teaching qualitative inquiry. We conclude by addressing several key questions including participant consent, confidentiality and anonymity, analyzing de-contextualized qualitative data, storing and accessing data, study preregistration, and the principle of emergent design within qualitative inquiry.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/V4TGWQU2/S1469029217305800.html}
}

@article{tanBayesianNetworkMetacomparison2015,
  title = {Bayesian Network Meta-Comparison of Maintenance Treatments for Stage {{IIIb}}/{{IV}} Non-Small-Cell Lung Cancer ({{NSCLC}}) Patients with Good Performance Status Not Progressing after First-Line Induction Chemotherapy: {{Results}} by Performance Status, {{EGFR}} Mutation, Histology and Response to Previous Induction},
  author = {Tan, P.S. and Lopes, G. and Acharyya, S. and Bilger, M. and Haaland, B.},
  year = {2015},
  journal = {European Journal of Cancer},
  volume = {51},
  number = {16},
  pages = {2330--2344},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.ejca.2015.07.007}
}

@article{tanEffectsPreexerciseAcute2022,
  title = {Effects of {{Pre-exercise Acute Vibration Training}} on {{Symptoms}} of {{Exercise-Induced Muscle Damage}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  author = {Tan, J. and Shi, X. and Witchalls, J. and Waddington, G. and Lun Fu, A.C. and Wu, S. and Tirosh, O. and Wu, X. and Han, J.},
  year = {2022},
  journal = {Journal of Strength and Conditioning Research},
  volume = {36},
  number = {8},
  pages = {2339--2348},
  publisher = {{NSCA National Strength and Conditioning Association}},
  doi = {10.1519/JSC.0000000000003789}
}

@article{tanSystematicReviewMetaanalysis2018,
  title = {Systematic Review and Meta-Analysis of Algorithms Used to Identify Drug-Induced Liver Injury ({{DILI}}) in Health Record Databases},
  author = {Tan, E.H. and Low, E.X.S. and Dan, Y.Y. and Tai, B.C.},
  year = {2018},
  journal = {Liver International},
  volume = {38},
  number = {4},
  pages = {742--753},
  publisher = {Blackwell Publishing Ltd},
  doi = {10.1111/liv.13646}
}

@article{taylorBIASLINEARMODEL1996,
  title = {{{BIAS IN LINEAR MODEL POWER AND SAMPLE SIZE CALCULATION DUE TO ESTIMATING NONCENTRALITY}}},
  author = {Taylor, Douglas J. and Muller, Keith E.},
  year = {1996},
  journal = {Communications in statistics: theory and methods},
  volume = {25},
  number = {7},
  issn = {0361-0926},
  doi = {10.1080/03610929608831787},
  urldate = {2021-04-19},
  abstract = {Data analysts frequently calculate power and sample size for a planned study using mean and variance estimates from an initial trial. Hence power, or the sample size needed to achieve a fixed power, varies randomly. Such calculations can be very inaccurate in the General Linear Univariate Model (GLUM). Biased noncentrality estimators and censored power calculations create inaccuracy. Censoring occurs if only certain outcomes of an initial trial lead to a power calculation. For example, a confirmatory study may be planned (and a sample size estimated) only following a significant result in the initial trial., Computing accurate point estimates or confidence bounds of GLUM noncentrality, power, or sample size in the presence of censoring involves truncated noncentral F distributions. We recommend confidence bounds, whether or not censoring occurs. A power analysis of data from humans exposed to carbon monoxide demonstrates the substantial impact on sample size that may occur. The results highlight potential biases and should aid study planning and interpretation.},
  pmcid = {PMC3867307},
  pmid = {24363486},
  file = {/Users/cristian/Zotero/storage/L5HHF6LE/Taylor and Muller - 1996 - BIAS IN LINEAR MODEL POWER AND SAMPLE SIZE CALCULA.pdf}
}

@article{tedersooDataSharingPractices2021,
  title = {Data Sharing Practices and Data Availability upon Request Differ across Scientific Disciplines},
  author = {Tedersoo, Leho and K{\"u}ngas, Rainer and Oras, Ester and K{\"o}ster, Kajar and Eenmaa, Helen and Leijen, {\"A}li and Pedaste, Margus and Raju, Marju and Astapova, Anastasiya and Lukner, Heli and Kogermann, Karin and Sepp, Tuul},
  year = {2021},
  month = jul,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {192},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00981-0},
  urldate = {2023-02-28},
  abstract = {Data sharing is one of the cornerstones of modern science that enables large-scale analyses and reproducibility. We evaluated data availability in research articles across nine disciplines in Nature and Science magazines and recorded corresponding authors' concerns, requests and reasons for declining data sharing. Although data sharing has improved in the last decade and particularly in recent years, data availability and willingness to share data still differ greatly among disciplines. We observed that statements of data availability upon (reasonable) request are inefficient and should not be allowed by journals. To improve data sharing at the time of manuscript acceptance, researchers should be better motivated to release their data with real benefits such as recognition, or bonus points in grant and job applications. We recommend that data management costs should be covered by funding agencies; publicly available research data ought to be included in the evaluation of applications; and surveillance of data sharing should be enforced by both academic publishers and funders. These cross-discipline survey data are available from the plutoF repository.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Genetic databases,Molecular ecology},
  file = {/Users/cristian/Zotero/storage/JT634LD9/Tedersoo et al. - 2021 - Data sharing practices and data availability upon .pdf}
}

@article{tennantLimitationsOurUnderstanding2020,
  title = {The Limitations to Our Understanding of Peer Review},
  author = {Tennant, Jonathan P. and {Ross-Hellauer}, Tony},
  year = {2020},
  month = apr,
  journal = {Research Integrity and Peer Review},
  volume = {5},
  number = {1},
  pages = {6},
  issn = {2058-8615},
  doi = {10.1186/s41073-020-00092-1},
  urldate = {2025-07-02},
  abstract = {Peer review is embedded in the core of our knowledge generation systems, perceived as a method for establishing quality or scholarly legitimacy for research, while also often distributing academic prestige and standing on individuals. Despite its critical importance, it curiously remains poorly understood in a number of dimensions. In order to address this, we have analysed peer review to assess where the major gaps in our theoretical and empirical understanding of it lie. We identify core themes including editorial responsibility, the subjectivity and bias of reviewers, the function and quality of peer review, and the social and epistemic implications of peer review. The high-priority gaps are focused around increased accountability and justification in decision-making processes for editors and developing a deeper, empirical understanding of the social impact of peer review. Addressing this at the bare minimum will require the design of a consensus for a minimal set of standards for what constitutes peer review, and the development of a shared data infrastructure to support this. Such a field requires sustained funding and commitment from publishers and research funders, who both have a commitment to uphold the integrity of the published scholarly record. We use this to present a guide for the future of peer review, and the development of a new research discipline based on the study of peer review.},
  langid = {english},
  keywords = {Biosciences and Society,Critical Thinking,Discourse Analysis,Open peer review,Peer review studies,Quality assurance,Quality control,Reproducibility,Research Ethics,Research impact,Research Skills,Scholarly communication,Scholarly publishing,Sociology of Science},
  file = {/Users/cristian/Zotero/storage/JTL3BB3I/Tennant and Ross-Hellauer - 2020 - The limitations to our understanding of peer revie.pdf}
}

@article{tenopirDataSharingScientists2011,
  title = {Data {{Sharing}} by {{Scientists}}: {{Practices}} and {{Perceptions}}},
  shorttitle = {Data {{Sharing}} by {{Scientists}}},
  author = {Tenopir, Carol and Allard, Suzie and Douglass, Kimberly and Aydinoglu, Arsev Umur and Wu, Lei and Read, Eleanor and Manoff, Maribeth and Frame, Mike},
  year = {2011},
  month = jun,
  journal = {PLOS ONE},
  volume = {6},
  number = {6},
  pages = {e21101},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0021101},
  urldate = {2023-02-28},
  abstract = {Background Scientific research in the 21st century is more data intensive and collaborative than in the past. It is important to study the data practices of researchers -- data accessibility, discovery, re-use, preservation and, particularly, data sharing. Data sharing is a valuable part of the scientific method allowing for verification of results and extending research from prior results. Methodology/Principal Findings A total of 1329 scientists participated in this survey exploring current data sharing practices and perceptions of the barriers and enablers of data sharing. Scientists do not make their data electronically available to others for various reasons, including insufficient time and lack of funding. Most respondents are satisfied with their current processes for the initial and short-term parts of the data or research lifecycle (collecting their research data; searching for, describing or cataloging, analyzing, and short-term storage of their data) but are not satisfied with long-term data preservation. Many organizations do not provide support to their researchers for data management both in the short- and long-term. If certain conditions are met (such as formal citation and sharing reprints) respondents agree they are willing to share their data. There are also significant differences and approaches in data management practices based on primary funding agency, subject discipline, age, work focus, and world region. Conclusions/Significance Barriers to effective data sharing and preservation are deeply rooted in the practices and culture of the research process as well as the researchers themselves. New mandates for data management plans from NSF and other federal agencies and world-wide attention to the need to share and preserve data could lead to changes. Large scale programs, such as the NSF-sponsored DataNET (including projects like DataONE) will both bring attention and resources to the issue and make it easier for scientists to apply sound data management principles.},
  langid = {english},
  keywords = {Data management,Ecology and environmental sciences,Europe,Medicine and health sciences,Metadata,Scientists,Social sciences,Surveys},
  file = {/Users/cristian/Zotero/storage/QNRSDV9J/Tenopir et al. - 2011 - Data Sharing by Scientists Practices and Percepti.pdf}
}

@article{thalheimerHowCalculateEffect2009,
  title = {How to Calculate Effect Sizes from Published Research: {{A}} Simplified Methodology},
  shorttitle = {How to Calculate Effect Sizes from Published Research},
  author = {Thalheimer, Will and Cook, Samantha},
  year = {2009},
  month = jan,
  volume = {1},
  doi = {http://work-learning.com/effect_sizes.htm.},
  file = {/Users/cristian/Zotero/storage/CWJLCMZ5/Thalheimer and Cook - 2009 - How to calculate effect sizes from published resea.pdf}
}

@misc{TheoryConstructionMethodology,
  title = {Theory {{Construction Methodology}}: {{A Practical Framework}} for {{Building Theories}} in {{Psychology}}},
  shorttitle = {Theory {{Construction Methodology}}},
  doi = {10.1177/1745691620969647},
  urldate = {2024-07-24},
  howpublished = {https://journals.sagepub.com/doi/epub/10.1177/1745691620969647},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/KKVUK2X2/Theory Construction Methodology A Practical Frame.pdf;/Users/cristian/Zotero/storage/YTJQRZXI/1745691620969647.html}
}

@misc{thibault_errors_power,
  title = {An Evaluation of Reproducibility and Errors in Published Sample Size Calculations Performed Using {{G}}*{{Power}}},
  author = {Thibault, Robert T. and Zavalis, Emmanuel A. and Mali{\v c}ki, Mario and Pedder, Hugo},
  year = {2024},
  month = jul,
  pages = {2024.07.15.24310458},
  publisher = {medRxiv},
  doi = {10.1101/2024.07.15.24310458},
  urldate = {2024-07-31},
  abstract = {Background Published studies in the life and health sciences often employ sample sizes that are too small to detect realistic effect sizes. This shortcoming increases the rate of false positives and false negatives, giving rise to a potentially misleading scientific record. To address this shortcoming, many researchers now use point-and-click software to run sample size calculations. Objective We aimed to (1) estimate how many published articles report using the G*Power sample size calculation software; (2) assess whether these calculations are reproducible and (3) error-free; and (4) assess how often these calculations use G*Power's default option for mixed-design ANOVAs---which can be misleading and output sample sizes that are too small for a researcher's intended purpose. Method We randomly sampled open access articles from PubMed Central published between 2017 and 2022 and used a coding form to manually assess 95 sample size calculations for reproducibility and errors. Results We estimate that more than 48,000 articles published between 2017 and 2022 and indexed in PubMed Central or PubMed report using G*Power (i.e., 0.65\% [95\% CI: 0.62\% - 0.67\%] of articles). We could reproduce 2\% (2/95) of the sample size calculations without making any assumptions, and likely reproduce another 28\% (27/95) after making assumptions. Many calculations were not reported transparently enough to assess whether an error was present (75\%; 71/95) or whether the sample size calculation was for a statistical test that appeared in the results section of the publication (48\%; 46/95). Few articles that performed a calculation for a mixed-design ANOVA unambiguously selected the non-default option (8\%; 3/36). Conclusion Published sample size calculations that use G*Power are not transparently reported and may not be well-informed. Given the popularity of software packages like G*Power, they present an intervention point to increase the prevalence of informative sample size calculations.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder has placed this preprint in the Public Domain. It is no longer restricted by copyright. Anyone can legally share, reuse, remix, or adapt this material for any purpose without crediting the original authors.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/E6BRJRLS/Thibault et al. - 2024 - An evaluation of reproducibility and errors in pub.pdf}
}

@article{thibaultExcessSignificancePower2022,
  title = {Excess Significance and Power Miscalculations in Neurofeedback Research},
  author = {Thibault, Robert T. and Pedder, Hugo},
  year = {2022},
  month = jan,
  journal = {NeuroImage: Clinical},
  volume = {35},
  pages = {103008},
  issn = {2213-1582},
  doi = {10.1016/j.nicl.2022.103008},
  urldate = {2024-08-01},
  keywords = {fMRI,fNIRS,Neurofeedback,Neuroimaging,Statistical power analysis},
  file = {/Users/cristian/Zotero/storage/YSQEW56W/Thibault and Pedder - 2022 - Excess significance and power miscalculations in n.pdf;/Users/cristian/Zotero/storage/8IZRR4HZ/S2213158222000730.html}
}

@article{thompsonScopeScientificHypotheses2023,
  ids = {thompsonScopeScientificHypotheses},
  title = {On the Scope of Scientific Hypotheses},
  author = {Thompson, William Hedley and Skau, Simon},
  year = {2023},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {8},
  pages = {230607},
  publisher = {Royal Society},
  doi = {10.1098/rsos.230607},
  urldate = {2023-09-08},
  abstract = {Hypotheses are frequently the starting point when undertaking the empirical portion of the scientific process. They state something that the scientific process will attempt to evaluate, corroborate, verify or falsify. Their purpose is to guide the types of data we collect, analyses we conduct, and inferences we would like to make. Over the last decade, metascience has advocated for hypotheses being in preregistrations or registered reports, but how to formulate these hypotheses has received less attention. Here, we argue that hypotheses can vary in specificity along at least three independent dimensions: the relationship, the variables, and the pipeline. Together, these dimensions form the scope of the hypothesis. We demonstrate how narrowing the scope of a hypothesis in any of these three ways reduces the hypothesis space and that this reduction is a type of novelty. Finally, we discuss how this formulation of hypotheses can guide researchers to formulate the appropriate scope for their hypotheses and should aim for neither too broad nor too narrow a scope. This framework can guide hypothesis-makers when formulating their hypotheses by helping clarify what is being tested, chaining results to previous known findings, and demarcating what is explicitly tested in the hypothesis.},
  pmcid = {PMC10465209},
  pmid = {37650069},
  keywords = {hypotheses,metascience,philosophy of science,psychology},
  file = {/Users/cristian/Zotero/storage/B5B5QYM5/Thompson and Skau - 2023 - On the scope of scientific hypotheses.pdf}
}

@article{tiokhinShiftingLevelSelection2024,
  title = {Shifting the {{Level}} of {{Selection}} in {{Science}}},
  author = {Tiokhin, Leo and Panchanathan, Karthik and Smaldino, Paul E. and Lakens, Dani{\"e}l},
  year = {2024},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {19},
  number = {6},
  pages = {908--920},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/17456916231182568},
  urldate = {2025-06-05},
  abstract = {Criteria for recognizing and rewarding scientists primarily focus on individual contributions. This creates a conflict between what is best for scientists' careers and what is best for science. In this article, we show how the theory of multilevel selection provides conceptual tools for modifying incentives to better align individual and collective interests. A core principle is the need to account for indirect effects by shifting the level at which selection operates from individuals to the groups in which individuals are embedded. This principle is used in several fields to improve collective outcomes, including animal husbandry, team sports, and professional organizations. Shifting the level of selection has the potential to ameliorate several problems in contemporary science, including accounting for scientists' diverse contributions to knowledge generation, reducing individual-level competition, and promoting specialization and team science. We discuss the difficulties associated with shifting the level of selection and outline directions for future development in this domain.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/RJE6L4BH/Tiokhin et al. - 2024 - Shifting the Level of Selection in Science.pdf}
}

@article{togunBiomarkersDiagnosisChildhood2018,
  title = {Biomarkers for Diagnosis of Childhood Tuberculosis: {{A}} Systematic Review},
  author = {Togun, T.O. and MacLean, E. and Kampmann, B. and Pai, M.},
  year = {2018},
  journal = {PLoS ONE},
  volume = {13},
  number = {9},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0204029}
}

@article{tomczykEffects12Wk2023,
  title = {Effects of 12 {{Wk}} of {{Omega-3 Fatty Acid Supplementation}} in {{Long-Distance Runners}}},
  author = {Tomczyk, Maja and Jost, Zbigniew and Chroboczek, Maciej and Urba{\'n}ski, Robert and Calder, Philip C. and Fisk, Helena L. and Sprengel, Mateusz and Antosiewicz, J{\k e}drzej},
  year = {2023},
  month = feb,
  journal = {Medicine and Science in Sports and Exercise},
  volume = {55},
  number = {2},
  pages = {216--224},
  issn = {1530-0315},
  doi = {10.1249/MSS.0000000000003038},
  abstract = {PURPOSE: This study aimed to investigate the effects of 12 wk of omega-3 fatty acid supplementation during endurance training on omega-3 index (O3I) and indicators of running performance in amateur long-distance runners. METHODS: Twenty-six amateur male long-distance runners {$\geq$}29 yr old supplemented omega-3 fatty acid capsules (OMEGA group, n = 14; 2234 mg of eicosapentaenoic acid and 916 mg of docosahexaenoic acid daily) or medium-chain triglycerides capsules as placebo (medium-chain triglyceride [MCT] group, n = 12; 4000 mg of MCT daily) during 12 wk of endurance training. Before and after intervention, blood samples were collected for O3I assessment, and an incremental test to exhaustion and a 1500-m run trial were performed. RESULTS: O3I was significantly increased in the OMEGA group (from 5.8\% to 11.6\%, P {$<$} 0.0001). A significant increase in {\.V}O 2peak was observed in the OMEGA group (from 53.6 {\textpm} 4.4 to 56.0 {\textpm} 3.7 mL{$\cdot$}kg -1 {$\cdot$}min -1 , P = 0.0219) without such change in MCT group (from 54.7 {\textpm} 6.8 to 56.4 {\textpm} 5.9 mL{$\cdot$}kg -1 {$\cdot$}min -1 , P = 0.1308). A positive correlation between the change in O3I and the change in running economy was observed when data of participants from both groups were combined (-0.1808 {\textpm} 1.917, P = 0.0020), without such an effect in OMEGA group alone ( P = 0.1741). No effect of omega-3 supplementation on 1500-m run results was observed. CONCLUSIONS: Twelve weeks of omega-3 fatty acid supplementation at a dose of 2234 mg of eicosapentaenoic acid and 916 mg of docosahexaenoic acid daily during endurance training resulted in the improvement of O3I and running economy and increased {\.V}O 2peak without improvement in the 1500-m run trial time in amateur runners.},
  langid = {english},
  pmcid = {PMC9815816},
  pmid = {36161864},
  keywords = {{Fatty Acids, Omega-3},Adult,Dietary Supplements,Docosahexaenoic Acids,Eicosapentaenoic Acid,Humans,Male,Running},
  file = {/Users/cristian/Zotero/storage/EQP3EQWM/Tomczyk et al. - 2023 - Effects of 12 Wk of Omega-3 Fatty Acid Supplementa.pdf}
}

@article{toraihAssociationCardiacBiomarkers2020,
  title = {Association of Cardiac Biomarkers and Comorbidities with Increased Mortality, Severity, and Cardiac Injury in {{COVID-19}} Patients: {{A}} Meta-Regression and Decision Tree Analysis},
  author = {Toraih, E.A. and Elshazli, R.M. and Hussein, M.H. and Elgaml, A. and Amin, M. and {El-Mowafy}, M. and {El-Mesery}, M. and Ellythy, A. and Duchesne, J. and Killackey, M.T. and Ferdinand, K.C. and Kandil, E. and Fawzy, M.S.},
  year = {2020},
  journal = {Journal of Medical Virology},
  volume = {92},
  number = {11},
  pages = {2473--2488},
  publisher = {{John Wiley and Sons Inc}},
  doi = {10.1002/jmv.26166}
}

@misc{TraditionalMultiplicityAdjustment,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials - {{Dmitrienko}} - 2013 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  urldate = {2023-07-06},
  howpublished = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5990},
  file = {/Users/cristian/Zotero/storage/7JFSR5P5/sim.html}
}

@misc{TransparencyConductingReporting,
  title = {Transparency in Conducting and Reporting Research: {{A}} Survey of Authors, Reviewers, and Editors across Scholarly Disciplines {\textbar} {{PLOS ONE}}},
  urldate = {2023-03-10},
  howpublished = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0270054},
  file = {/Users/cristian/Zotero/storage/K2F6I8LH/article.html}
}

@article{trungOptimisationQuantitativeMiRNA2018,
  title = {Optimisation of Quantitative {{miRNA}} Panels to Consolidate the Diagnostic Surveillance of {{HBV-related}} Hepatocellular Carcinoma},
  author = {Trung, N.T. and Duong, D.C. and Van Tong, H. and Hien, T.T.T. and Hoan, P.Q. and Bang, M.H. and Binh, M.T. and Ky, T.D. and Tung, N.L. and Thinh, N.T. and Sang, V.V. and Thao, L.T.P. and Bock, C.-T. and Velavan, T.P. and Meyer, C.G. and Song, L.H. and Toan, N.L.},
  year = {2018},
  journal = {PLoS ONE},
  volume = {13},
  number = {4},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0196081}
}

@article{tuncEpistemicPragmaticFunction2023,
  ids = {tuncEpistemicPragmaticFunction2021},
  title = {The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical Hypothesis Tests},
  author = {Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip and Lakens, Dani{\"e}l},
  year = {2023},
  month = jun,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {3},
  pages = {403--423},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/09593543231160112},
  urldate = {2023-08-09},
  abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as a misuse of statistics and criticize scientists for the widespread application of hypothesis tests to tentatively reject a hypothesis (or not) depending on whether a p-value is below or above an alpha level. Although dichotomous claims are rarely explicitly defended, we argue they play an important epistemological and pragmatic role in science. The epistemological function of dichotomous claims consists in transforming data into quasibasic statements, which are tentatively accepted singular facts that can corroborate or falsify theoretical claims. This transformation requires a prespecified methodological decision procedure such as Neyman-Pearson hypothesis tests. From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g., continuous test statistics) cannot function as falsifiers of substantive hypotheses. The pragmatic function of dichotomous claims is to facilitate scrutiny and criticism among peers by generating contestable claims, a process referred to by Popper as ``conjectures and refutations.'' We speculate about how the surprisingly widespread use of a 5\% alpha level might have facilitated this pragmatic function. Abandoning dichotomous claims, for example because researchers commonly misuse p-values, would sacrifice their crucial epistemic and pragmatic functions.},
  langid = {english},
  keywords = {basic statements,dichotomous claims,methodological falsificationism,Quantitative Methods,Social and Behavioral Sciences,statisitical hypothesis testing,Theory and Philosophy of Science,theory testing},
  file = {/Users/cristian/Zotero/storage/8HGI3MCE/Tun et al. - 2021 - The Epistemic and Pragmatic Function of Dichotomou.pdf;/Users/cristian/Zotero/storage/QI7Z5NHL/Uygun Tun et al. - 2023 - The epistemic and pragmatic function of dichotomou.pdf}
}

@article{turekPracticalApproachesDiagnosis2005,
  title = {Practical Approaches to the Diagnosis and Management of Male Infertility},
  author = {Turek, P.J.},
  year = {2005},
  journal = {Nature Clinical Practice Urology},
  volume = {2},
  number = {5},
  pages = {226--238},
  doi = {10.1038/ncpuro0166}
}

@article{twomeyNatureOurLiterature2021,
  title = {The {{Nature}} of {{Our Literature}}: {{A Registered Report}} on the {{Positive Result Rate}} and {{Reporting Practices}} in {{Kinesiology}}},
  author = {Twomey, Rosie and Yingling, Vanessa and Warne, Joe and Schneider, Christoph and McCrum, Christopher and Atkins, Whitley and Murphy, Jennifer and Medina, Claudia Romero and Harlley, Sena and Caldwell, Aaron},
  year = {2021},
  month = dec,
  journal = {Communications in Kinesiology},
  volume = {1},
  number = {3},
  pages = {1--17},
  doi = {10.51224/cik.v1i3.43},
  abstract = {Scientists rely upon an accurate scientific literature in order to build and test new theories about the natural world. In the past decade, observational studies of the scientific literature have indicated that numerous questionable research practices and poor reporting practices may be hindering scientific progress. In particular, 3 recent studies have indicated an implausibly high rate of studies with positive (i.e., hypothesis confirming) results. In sports medicine, a field closely related to kinesiology, studies that tested a hypothesis indicated support for their primary hypothesis {\textasciitilde}70\% of the time. However, a study of journals that cover the entire field of kinesiology has yet to be completed, and the quality of other reporting practices, such as clinical trial registration, has not been evaluated. In this study we retrospectively evaluated 300 original research articles from the flagship journals of North America (Medicine and Science in Sports and Exercise), Europe (European Journal of Sport Science), and Australia (Journal of Science and Medicine in Sport). The hypothesis testing rate ({\textasciitilde}64\%) and positive result rate ({\textasciitilde}81\%) were much lower than what has been reported in other fields (e.g., psychology), and there was only weak evidence for our hypothesis that the positive result rate exceeded 80\%. However, the positive result rate is still considered unreasonably high. Additionally, most studies did not report trial registration, and rarely included accessible data indicating rather poor reporting practices. The majority of studies relied upon significance testing ({\textasciitilde}92\%), but it was more concerning that a majority of studies ({\textasciitilde}82\%) without a stated hypothesis still relied upon significance testing. Overall, the positive result rate in kinesiology is unacceptably high, despite being lower than other fields such as psychology, and most published manuscripts demonstrated subpar reporting practices},
  langid = {english},
  keywords = {sport and exercise science},
  annotation = {https://doi.org/10.51224/cik.v1i3.43},
  file = {/Users/cristian/Zotero/storage/XCTLFNDG/Twomey et al. - 2021 - The Nature of Our Literature A Registered Report .pdf}
}

@article{twycross-lewisEffectsCreatineSupplementation2016,
  title = {The Effects of Creatine Supplementation on Thermoregulation and Physical (Cognitive) Performance: A Review and Future Prospects},
  author = {{Twycross-Lewis}, R. and Kilduff, L.P. and Wang, G. and Pitsiladis, Y.P.},
  year = {2016},
  journal = {Amino Acids},
  volume = {48},
  number = {8},
  pages = {1843--1855},
  publisher = {Springer-Verlag Wien},
  doi = {10.1007/s00726-016-2237-9}
}

@article{ulrichQuestionableResearchPractices2020,
  title = {Questionable Research Practices May Have Little Effect on Replicability},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2020},
  journal = {eLife},
  volume = {9},
  pages = {e58237},
  issn = {2050-084X},
  doi = {10.7554/eLife.58237},
  urldate = {2022-03-14},
  abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.},
  pmcid = {PMC7561355},
  pmid = {32930092},
  file = {/Users/cristian/Zotero/storage/TTPGKGBV/Ulrich and Miller - Questionable research practices may have little ef.pdf}
}

@misc{UnderstandingMixedEffectsModels,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}} - {{Lisa M}}. {{DeBruine}}, {{Dale J}}. {{Barr}}, 2021},
  urldate = {2025-07-06},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/2515245920965119},
  file = {/Users/cristian/Zotero/storage/NQLRDANE/2515245920965119.html}
}

@misc{UnderstandingMixedEffectsModelsa,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}} - {{Lisa M}}. {{DeBruine}}, {{Dale J}}. {{Barr}}, 2021},
  urldate = {2025-07-06},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/2515245920965119},
  file = {/Users/cristian/Zotero/storage/FHXZKNMC/2515245920965119.html}
}

@misc{UnderstandingMixedEffectsModelsb,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}} - {{Lisa M}}. {{DeBruine}}, {{Dale J}}. {{Barr}}, 2021},
  urldate = {2025-09-01},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/2515245920965119},
  file = {/Users/cristian/Zotero/storage/Q4KIN4PC/2515245920965119.html}
}

@article{usachevaPharmacologicalAspectsUse2022,
  title = {Pharmacological {{Aspects}} of the {{Use}} of {{Lipoic Acid}} ({{Review}})},
  author = {Usacheva, A.M. and Chernikov, A.V. and Karmanova, E.E. and Bruskov, V.I.},
  year = {2022},
  journal = {Pharmaceutical Chemistry Journal},
  volume = {55},
  number = {11},
  pages = {1138--1146},
  publisher = {Springer},
  doi = {10.1007/s11094-022-02549-7}
}

@misc{UtilizationPICOFramework,
  title = {Utilization of the {{PICO}} Framework to Improve Searching {{PubMed}} for Clinical Questions {\textbar} {{BMC Medical Informatics}} and {{Decision Making}}},
  urldate = {2025-03-24},
  howpublished = {https://link.springer.com/article/10.1186/1472-6947-7-16},
  file = {/Users/cristian/Zotero/storage/P5DE8SWI/1472-6947-7-16.html}
}

@article{uttleyPowerAnalysisSample2019,
  title = {Power {{Analysis}}, {{Sample Size}}, and {{Assessment}} of {{Statistical Assumptions}}---{{Improving}} the {{Evidential Value}} of {{Lighting Research}}},
  author = {Uttley, J.},
  year = {2019},
  month = jul,
  journal = {LEUKOS},
  volume = {15},
  number = {2-3},
  pages = {143--162},
  issn = {1550-2724, 1550-2716},
  doi = {10.1080/15502724.2018.1533851},
  urldate = {2021-03-12},
  abstract = {The reporting of accurate and appropriate conclusions is an essential aspect of scientific research, and failure in this endeavor can threaten the progress of cumulative knowledge. This is highlighted by the current reproducibility crisis, and this crisis disproportionately affects fields that use behavioral research methods, as in much lighting research. A sample of general and topic-specific lighting research papers was reviewed for information about sample sizes and statistical reporting. This highlighted that lighting research is generally underpowered and, given median sample sizes, is unlikely to be able to reveal small effects. Lighting research most commonly uses parametric statistical tests, but assessment of test assumptions is rarely carried out. This risks the inappropriate use of statistical tests, potentially leading to type I and type II errors. Lighting research papers also rarely report measures of effect size, and this can hamper cumulative science and power analyses required to determine appropriate sample sizes for future research studies. Addressing the issues raised in this article related to sample sizes, statistical test assumptions, and reporting of effect sizes can improve the evidential value of lighting research.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/MVLZCCGD/Uttley - 2019 - Power Analysis, Sample Size, and Assessment of Sta.pdf}
}

@article{vagenasThirtyyearTrendsStudy2018a,
  ids = {vagenasThirtyyearTrendsStudy2018},
  title = {Thirty-Year {{Trends}} of {{Study Design}} and {{Statistics}} in {{Applied Sports}} and {{Exercise Biomechanics Research}}},
  author = {Vagenas, George and Palaiothodorou, Dimitria and Knudson, Duane},
  year = {2018},
  month = jan,
  journal = {International Journal of Exercise Science},
  volume = {11},
  number = {1},
  pages = {239--259},
  issn = {1939-795X},
  file = {/Users/cristian/Zotero/storage/T92REN4K/Vagenas et al. - 2018 - Thirty-year Trends of Study Design and Statistics .pdf;/Users/cristian/Zotero/storage/VTNMIY2B/3.html}
}

@article{valentineReplicationPreventionScience2011,
  title = {Replication in Prevention Science},
  author = {Valentine, Jeffrey C. and Biglan, Anthony and Boruch, Robert F. and Castro, Felipe Gonz{\'a}lez and Collins, Linda M. and Flay, Brian R. and Kellam, Sheppard and Mo{\'s}cicki, Eve K. and Schinke, Steven P.},
  year = {2011},
  month = jun,
  journal = {Prevention Science: The Official Journal of the Society for Prevention Research},
  volume = {12},
  number = {2},
  pages = {103--117},
  issn = {1573-6695},
  doi = {10.1007/s11121-011-0217-6},
  abstract = {Replication research is essential for the advancement of any scientific field. In this paper, we argue that prevention science will be better positioned to help improve public health if (a) more replications are conducted; (b) those replications are systematic, thoughtful, and conducted with full knowledge of the trials that have preceded them; and (c) state-of-the art techniques are used to summarize the body of evidence on the effects of the interventions. Under real-world demands it is often not feasible to wait for multiple replications to accumulate before making decisions about intervention adoption. To help individuals and agencies make better decisions about intervention utility, we outline strategies that can be used to help understand the likely direction, size, and range of intervention effects as suggested by the current knowledge base. We also suggest structural changes that could increase the amount and quality of replication research, such as the provision of incentives and a more vigorous pursuit of prospective research registers. Finally, we discuss methods for integrating replications into the roll-out of a program and suggest that strong partnerships with local decision makers are a key component of success in replication research. Our hope is that this paper can highlight the importance of replication and stimulate more discussion of the important elements of the replication process. We are confident that, armed with more and better replications and state-of-the-art review methods, prevention science will be in a better position to positively impact public health.},
  langid = {english},
  pmid = {21541692},
  keywords = {Health Services Research,Preventive Medicine,Reproducibility of Results}
}

@article{valenzuelaPrenatalInterventionsFetal2022,
  title = {Prenatal Interventions for Fetal Growth Restriction in Animal Models: {{A}} Systematic Review},
  author = {Valenzuela, I. and Kinoshita, M. and {van der Merwe}, J. and Mar{\v s}{\'a}l, K. and Deprest, J.},
  year = {2022},
  journal = {Placenta},
  volume = {126},
  pages = {90--113},
  publisher = {W.B. Saunders Ltd},
  doi = {10.1016/j.placenta.2022.06.007}
}

@article{valenzuelaSupplementsPurportedEffects2019,
  title = {Supplements with Purported Effects on Muscle Mass and Strength},
  author = {Valenzuela, P.L. and Morales, J.S. and Emanuele, E. and {Pareja-Galeano}, H. and Lucia, A.},
  year = {2019},
  journal = {European Journal of Nutrition},
  volume = {58},
  number = {8},
  pages = {2983--3008},
  publisher = {{Dr. Dietrich Steinkopff Verlag GmbH and Co. KG}},
  doi = {10.1007/s00394-018-1882-z}
}

@article{valiFibrotestEvaluatingFibrosis2021,
  title = {Fibrotest for Evaluating Fibrosis in Non-Alcoholic Fatty Liver Disease Patients: {{A}} Systematic Review and Meta-Analysis},
  author = {Vali, Y. and Lee, J. and Boursier, J. and Spijker, R. and Verheij, J. and Brosnan, M.J. and Anstee, Q.M. and Bossuyt, P.M. and Zafarmand, M.H. and {LITMUS Systematic Review Team}},
  year = {2021},
  journal = {Journal of Clinical Medicine},
  volume = {10},
  number = {11},
  publisher = {MDPI},
  doi = {10.3390/jcm10112415}
}

@article{vanaertConductingMetaAnalysesBased2016,
  title = {Conducting {{Meta-Analyses Based}} on p {{Values}}: {{Reservations}} and {{Recommendations}} for {{Applying}} p-{{Uniform}} and p-{{Curve}}},
  shorttitle = {Conducting {{Meta-Analyses Based}} on p {{Values}}},
  author = {{van Aert}, Robbie C. M. and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {713--729},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691616650874},
  urldate = {2021-09-23},
  abstract = {Because of overwhelming evidence of publication bias in psychology, techniques to correct meta-analytic estimates for such bias are greatly needed. The methodology on which the p-uniform and p-curve methods are based has great promise for providing accurate meta-analytic estimates in the presence of publication bias. However, in this article, we show that in some situations, p-curve behaves erratically, whereas p-uniform may yield implausible estimates of negative effect size. Moreover, we show that (and explain why) p-curve and p-uniform result in overestimation of effect size under moderate-to-large heterogeneity and may yield unpredictable bias when researchers employ p-hacking. We offer hands-on recommendations on applying and interpreting results of meta-analyses in general and p-uniform and p-curve in particular. Both methods as well as traditional methods are applied to a meta-analysis on the effect of weight on judgments of importance. We offer guidance for applying p-uniform or p-curve using R and a user-friendly web application for applying p-uniform.},
  langid = {english},
  keywords = {heterogeneity,meta-analysis,p-curve,p-hacking,p-uniform},
  file = {/Users/cristian/Zotero/storage/HTH7SLSJ/van Aert et al. - 2016 - Conducting Meta-Analyses Based on p Values Reserv.pdf}
}

@article{vanaertPublicationBiasExamined2019,
  ids = {aertPublicationBiasExamined2019a},
  title = {Publication Bias Examined in Meta-Analyses from Psychology and Medicine: {{A}} Meta-Meta-Analysis},
  shorttitle = {Publication Bias Examined in Meta-Analyses from Psychology and Medicine},
  author = {{van Aert}, Robbie C. M. and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2019},
  journal = {PloS One},
  volume = {14},
  number = {4},
  pages = {e0215052},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0215052},
  abstract = {Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8\% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5\% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9\% and 18.9\% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.},
  langid = {english},
  pmcid = {PMC6461282},
  pmid = {30978228},
  keywords = {{Data Interpretation, Statistical},{Databases, Factual},Data Management,Forecasting,Humans,Medicine,Medicine and health sciences,Mental health and psychiatry,Metaanalysis,Monte Carlo method,Monte Carlo Method,Psychology,Publication Bias,Publication ethics,Quality Control,Selection Bias,Systematic reviews},
  file = {/Users/cristian/Zotero/storage/TMCXJA8V/van Aert et al. - 2019 - Publication bias examined in meta-analyses from ps.pdf}
}

@misc{vanaertPuniformMetaAnalysisMethods2022,
  title = {Puniform: {{Meta-Analysis Methods Correcting}} for {{Publication Bias}}},
  shorttitle = {Puniform},
  author = {{van Aert}, Robbie C. M.},
  year = {2022},
  urldate = {2022-05-23},
  abstract = {Provides meta-analysis methods that correct for publication bias and outcome reporting bias. Four methods and a visual tool are currently included in the package. The p-uniform method as described in van Assen, van Aert, and Wicherts (2015) {$<$}https:psycnet.apa.org/record/2014-48759-001{$>$} can be used for estimating the average effect size, testing the null hypothesis of no effect, and testing for publication bias using only the statistically significant effect sizes of primary studies. The second method in the package is the p-uniform* method as described in van Aert and van Assen (2019) {$<$}doi:10.31222/osf.io/zqjr9{$>$}. This method is an extension of the p-uniform method that allows for estimation of the average effect size and the between-study variance in a meta-analysis, and uses both the statistically significant and nonsignificant effect sizes. The third method in the package is the hybrid method as described in van Aert and van Assen (2017) {$<$}doi:10.3758/s13428-017-0967-6{$>$}. The hybrid method is a meta-analysis method for combining an original study and replication and while taking into account statistical significance of the original study. The p-uniform and hybrid method are based on the statistical theory that the distribution of p-values is uniform conditional on the population effect size. The fourth method in the package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert and van Assen (2018) {$<$}doi:10.1371/journal.pone.0175302{$>$}. This method computes posterior probabilities for four true effect sizes (no, small, medium, and large) based on an original study and replication while taking into account publication bias in the original study. The method can also be used for computing the required sample size of the replication akin to power analysis in null hypothesis significance testing. The meta-plot is a visual tool for meta-analysis that provides information on the primary studies in the meta-analysis, the results of the meta-analysis, and characteristics of the research on the effect under study (van Assen et al., 2021). Helper functions to apply the Correcting for Outcome Reporting Bias (CORB) method to correct for outcome reporting bias in a meta-analysis (van Aert \& Wicherts, 2021).},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {MetaAnalysis}
}

@article{vanassenMetaanalysisUsingEffect2015,
  title = {Meta-Analysis Using Effect Size Distributions of Only Statistically Significant Studies},
  author = {{van Assen}, Marcel A. L. M. and {van Aert}, Robbie C. M. and Wicherts, Jelte M.},
  year = {2015},
  journal = {Psychological Methods},
  volume = {20},
  pages = {293--309},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000025},
  abstract = {Publication bias threatens the validity of meta-analytic results and leads to overestimation of the effect size in traditional meta-analysis. This particularly applies to meta-analyses that feature small studies, which are ubiquitous in psychology. Here we develop a new method for meta-analysis that deals with publication bias. This method, p-uniform, enables (a) testing of publication bias, (b) effect size estimation, and (c) testing of the null-hypothesis of no effect. No current method for meta-analysis possesses all 3 qualities. Application of p-uniform is straightforward because no additional data on missing studies are needed and no sophisticated assumptions or choices need to be made before applying it. Simulations show that p-uniform generally outperforms the trim-and-fill method and the test of excess significance (TES; Ioannidis \& Trikalinos, 2007b) if publication bias exists and population effect size is homogenous or heterogeneity is slight. For illustration, p-uniform and other publication bias analyses are applied to the meta-analysis of McCall and Carriger (1993) examining the association between infants' habituation to a stimulus and their later cognitive ability (IQ). We conclude that p-uniform is a valuable technique for examining publication bias and estimating population effects in fixed-effect meta-analyses, and as sensitivity analysis to draw inferences about publication bias. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Ability,Effect Size (Statistical),Meta Analysis,Scientific Communication,Statistical Analysis},
  file = {/Users/cristian/Zotero/storage/BYAVHNDV/doiLanding.html}
}

@article{vandenakker_potential_2024,
  title = {The Potential of Preregistration in Psychology: {{Assessing}} Preregistration Producibility and Preregistration-Study Consistency},
  author = {{van den Akker}, Olmo R. and Bakker, Marjan and {van Assen}, Marcel A. L. M. and Pennington, Charlotte R. and Verweij, Leone and Elsherif, Mahmoud M. and Claesen, Aline and Gaillard, Stefan D. M. and Yeung, Siu Kit and Frankenberger, Jan-Luca and {al.}, et},
  year = {2024},
  month = oct,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000687}
}

@article{vandennoortgateMetaanalysisMultipleOutcomes2015,
  title = {Meta-Analysis of Multiple Outcomes: A Multilevel Approach},
  shorttitle = {Meta-Analysis of Multiple Outcomes},
  author = {{Van den Noortgate}, Wim and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {S{\'a}nchez-Meca}, Julio},
  year = {2015},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {47},
  number = {4},
  pages = {1274--1294},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0527-2},
  urldate = {2024-06-07},
  abstract = {In meta-analysis, dependent effect sizes are very common. An example is where in one or more studies the effect of an intervention is evaluated on multiple outcome variables for the same sample of participants. In this paper, we evaluate a three-level meta-analytic model to account for this kind of dependence, extending the simulation results of Van den Noortgate, L{\'o}pez-L{\'o}pez, Mar{\'i}n-Mart{\'i}nez, and S{\'a}nchez-Meca Behavior Research Methods, 45, 576--594 (2013) by allowing for a variation in the number of effect sizes per study, in the between-study variance, in the correlations between pairs of outcomes, and in the sample size of the studies. At the same time, we explore the performance of the approach if the outcomes used in a study can be regarded as a random sample from a population of outcomes. We conclude that although this approach is relatively simple and does not require prior estimates of the sampling covariances between effect sizes, it gives appropriate mean effect size estimates, standard error estimates, and confidence interval coverage proportions in a variety of realistic situations.},
  langid = {english},
  keywords = {Dependence,Meta-analysis,Multilevel,Multiple outcomes},
  file = {/Users/cristian/Zotero/storage/K5BKA6G9/Van den Noortgate et al. - 2015 - Meta-analysis of multiple outcomes a multilevel a.pdf}
}

@article{vandennoortgateThreelevelMetaanalysisDependent2013,
  title = {Three-Level Meta-Analysis of Dependent Effect Sizes},
  author = {{Van den Noortgate}, Wim and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and {S{\'a}nchez-Meca}, Julio},
  year = {2013},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {2},
  pages = {576--594},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0261-6},
  abstract = {Although dependence in effect sizes is ubiquitous, commonly used meta-analytic methods assume independent effect sizes. We describe and illustrate three-level extensions of a mixed effects meta-analytic model that accounts for various sources of dependence within and across studies, because multilevel extensions of meta-analytic models still are not well known. We also present a three-level model for the common case where, within studies, multiple effect sizes are calculated using the same sample. Whereas this approach is relatively simple and does not require imputing values for the unknown sampling covariances, it has hardly been used, and its performance has not been empirically investigated. Therefore, we set up a simulation study, showing that also in this situation, a three-level approach yields valid results: Estimates of the treatment effects and the corresponding standard errors are unbiased.},
  langid = {english},
  pmid = {23055166},
  keywords = {{Models, Psychological},{Models, Statistical},{Models, Theoretical},Meta-Analysis as Topic,Multivariate Analysis,Sampling Studies},
  file = {/Users/cristian/Zotero/storage/L7Y7N7R2/Van den Noortgate et al. - 2013 - Three-level meta-analysis of dependent effect size.pdf}
}

@article{vanderheydenGreaterLactateAccumulation2020,
  title = {Greater Lactate Accumulation Following an Acute Bout of High-Intensity Exercise in Males Suppresses Acylated Ghrelin and Appetite Postexercise},
  author = {Vanderheyden, Luke W. and McKie, Greg L. and Howe, Greg J. and Hazell, Tom J.},
  year = {2020},
  month = may,
  journal = {Journal of Applied Physiology},
  volume = {128},
  number = {5},
  pages = {1321--1328},
  issn = {8750-7587},
  doi = {10.1152/japplphysiol.00081.2020},
  urldate = {2023-07-06},
  abstract = {High-intensity exercise inhibits appetite, in part, via alterations in the peripheral concentrations of the appetite-regulating hormones acylated ghrelin, active glucagon-like peptide-1 (GLP-1), and active peptide tyrosine-tyrosine (PYY). Given lactate may mediate these effects, we used sodium bicarbonate (NaHCO3) supplementation in a double-blind, placebo-controlled, crossover design to investigate lactate's purported role in exercise-induced appetite suppression. Eleven males completed two identical high-intensity interval training sessions (10 {\texttimes} 1 min cycling bouts at {\textasciitilde}90\% heart rate maximum interspersed with 1-min recovery), where they ingested either NaHCO3 (BICARB) or sodium chloride (NaCl) as a placebo (PLACEBO) preexercise. Blood lactate, acylated ghrelin, GLP-1, and PYY concentrations, as well as overall appetite were assessed preexercise and 0, 30, 60, and 90 min postexercise. Blood lactate was greater immediately (P {$<$} 0.001) and 30 min postexercise (P = 0.049) in the BICARB session with an increased (P = 0.009) area under the curve (AUC). The BICARB session had lower acylated ghrelin at 60 (P = 0.014) and 90 min postexercise (P = 0.016), with a decreased AUC (P = 0.039). The BICARB session had increased PYY (P = 0.034) with an increased AUC (P = 0.031). The BICARB session also tended (P = 0.060) to have increased GLP-1 at 30 (P = 0.003) and 60 min postexercise (P {$<$} 0.001), with an increased AUC (P = 0.030). The BICARB session tended (P = 0.059) to reduce overall appetite, although there was no difference in AUC (P = 0.149). These findings support a potential role for lactate in the high-intensity exercise-induced appetite-suppression., NEW \& NOTEWORTHY We used sodium bicarbonate to increase lactate accumulation or sodium chloride as a placebo. Our findings further implicate lactate as a mediator of exercise-induced appetite suppression, given exercise-induced increases in lactate during the sodium bicarbonate session altered peripheral concentrations of appetite-regulating hormones, culminating in a reduction of appetite. This supports a lactate-dependent mechanism of appetite suppression following high-intensity exercise and highlights the potential of using lactate as a means of inducing a caloric deficit.},
  pmcid = {PMC7272750},
  pmid = {32240018}
}

@article{vanderpluymAcuteTreatmentsEpisodic2021,
  ids = {vanderpluymAcuteTreatmentsEpisodic2021a},
  title = {Acute {{Treatments}} for {{Episodic Migraine}} in {{Adults}}: {{A Systematic Review}} and {{Meta-analysis}}.},
  author = {VanderPluym, Juliana H. and Halker Singh, Rashmi B. and Urtecho, Meritxell and Morrow, Allison S. and Nayfeh, Tarek and Torres Roldan, Victor D. and Farah, Magdoleen H. and Hasan, Bashar and Saadi, Samer and Shah, Sahrish and {Abd-Rabu}, Rami and Daraz, Lubna and Prokop, Larry J. and Murad, Mohammad Hassan and Wang, Zhen},
  year = {2021},
  month = jun,
  journal = {JAMA},
  volume = {325},
  number = {23},
  pages = {2357--2369},
  publisher = {American Medical Association},
  address = {United States},
  issn = {1538-3598 0098-7484},
  doi = {10.1001/jama.2021.7939},
  abstract = {IMPORTANCE: Migraine is common and can be associated with significant morbidity, and several treatment options exist for acute therapy. OBJECTIVE: To evaluate the  benefits and harms associated with acute treatments for episodic migraine in  adults. DATA SOURCES: Multiple databases from database inception to February 24,  2021. STUDY SELECTION: Randomized clinical trials and systematic reviews that  assessed effectiveness or harms of acute therapy for migraine attacks. DATA  EXTRACTION AND SYNTHESIS: Independent reviewers selected studies and extracted  data. Meta-analysis was performed with the DerSimonian-Laird random-effects model  with Hartung-Knapp-Sidik-Jonkman variance correction or by using a fixed-effect  model based on the Mantel-Haenszel method if the number of studies was small.  MAIN OUTCOMES AND MEASURES: The main outcomes included pain freedom, pain relief,  sustained pain freedom, sustained pain relief, and adverse events. The strength  of evidence (SOE) was graded with the Agency for Healthcare Research and Quality  Methods Guide for Effectiveness and Comparative Effectiveness Reviews. FINDINGS:  Evidence on triptans and nonsteroidal anti-inflammatory drugs was summarized from  15 systematic reviews. For other interventions, 115 randomized clinical trials  with 28\,803 patients were included. Compared with placebo, triptans and  nonsteroidal anti-inflammatory drugs used individually were significantly  associated with reduced pain at 2 hours and 1 day (moderate to high SOE) and  increased risk of mild and transient adverse events. Compared with placebo,  calcitonin gene-related peptide receptor antagonists (low to high SOE),  lasmiditan (5-HT1F receptor agonist; high SOE), dihydroergotamine (moderate to  high SOE), ergotamine plus caffeine (moderate SOE), acetaminophen (moderate SOE),  antiemetics (low SOE), butorphanol (low SOE), and tramadol in combination with  acetaminophen (low SOE) were significantly associated with pain reduction and  increase in mild adverse events. The findings for opioids were based on low or  insufficient SOE. Several nonpharmacologic treatments were significantly  associated with improved pain, including remote electrical neuromodulation  (moderate SOE), transcranial magnetic stimulation (low SOE), external trigeminal  nerve stimulation (low SOE), and noninvasive vagus nerve stimulation (moderate  SOE). No significant difference in adverse events was found between  nonpharmacologic treatments and sham. CONCLUSIONS AND RELEVANCE: There are  several acute treatments for migraine, with varying strength of supporting  evidence. Use of triptans, nonsteroidal anti-inflammatory drugs, acetaminophen,  dihydroergotamine, calcitonin gene-related peptide antagonists, lasmiditan, and  some nonpharmacologic treatments was associated with improved pain and function.  The evidence for many other interventions, including opioids, was limited.},
  langid = {english},
  pmcid = {PMC8207243},
  pmid = {34128998},
  keywords = {{Analgesics, Opioid/therapeutic use},{Anti-Inflammatory Agents, Non-Steroidal/therapeutic use},*Electric Stimulation Therapy/adverse effects,Analgesics/adverse effects/*therapeutic use,Antiemetics/therapeutic use,Calcitonin Gene-Related Peptide Receptor Antagonists/therapeutic use,Ergot Alkaloids/therapeutic use,Evidence-Based Medicine,Humans,Migraine Disorders/*drug therapy/therapy,Pain Measurement,Serotonin Receptor Agonists/therapeutic use,Tryptamines/therapeutic use}
}

@article{vanerpEstimatesBetweenstudyHeterogeneity2017,
  title = {Estimates of Between-Study Heterogeneity for 705 Meta-Analyses Reported in {{Psychological Bulletin}} from 1990--2013},
  author = {{van Erp}, Sara and Verhagen, Josine and Grasman, Raoul P. P. P. and Wagenmakers, Eric-Jan},
  year = {2017},
  journal = {Journal of Open Psychology Data},
  volume = {5},
  number = {1},
  publisher = {Ubiquity Press},
  address = {United Kingdom},
  issn = {2050-9863},
  doi = {10.5334/jopd.33},
  abstract = {We present a data set containing 705 between-study heterogeneity estimates T{$^2$} as reported in 61 articles published in Psychological Bulletin from 1990--2013. The data set also includes information about the number and type of effect sizes, the Q- and I{$^2$}-statistics, and publication bias. The data set is stored in the Open Science Framework repository (https://osf.io/wyhve/) and can be used for several purposes: (1) to compare a specific heterogeneity estimate to the distribution of between-study heterogeneity estimates in psychology; (2) to construct an informed prior distribution for the between-study heterogeneity in psychology; (3) to obtain realistic population values for Monte Carlo simulations investigating the performance of meta-analytic methods. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Homogeneity of Variance,Meta Analysis,Statistics},
  file = {/Users/cristian/Zotero/storage/HNV6C6UU/van Erp et al. - 2017 - Estimates of between-study heterogeneity for 705 m.pdf;/Users/cristian/Zotero/storage/6Y27LY8X/2017-56410-001.html}
}

@article{vanhoveAnalyzingRandomizedControlled2015,
  title = {Analyzing Randomized Controlled Interventions: {{Three}} Notes for Applied Linguists},
  shorttitle = {Analyzing Randomized Controlled Interventions},
  author = {Vanhove, Jan},
  year = {2015},
  month = jan,
  journal = {Studies in Second Language Learning and Teaching},
  volume = {5},
  number = {1},
  pages = {135--152},
  issn = {2084-1965, 2083-5205},
  doi = {10.14746/ssllt.2015.5.1.7},
  urldate = {2024-05-14},
  abstract = {I discuss three practices common in analyses of randomized controlled interventions in applied linguistics. These are (a) checking whether randomization produced groups that are balanced on a number of possibly relevant covariates, (b) using repeated-measures ANOVA to analyze pretest-posttest designs, and (c) using traditional significance tests to analyze interventions in which whole groups were assigned to the conditions. These practices are labeled superfluous, overcomplicated, and inappropriate, respectively, and I suggest alternatives for each of them that will be useful to applied linguists and educators who need to design, analyze, or assess intervention studies or other randomized controlled trials. The statistical formalism is kept to a minimum throughout.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/FDX2H5NE/Vanhove - 2015 - Analyzing randomized controlled interventions Thr.pdf}
}

@article{vaninPhotobiomodulationTherapyImprovement2018,
  title = {Photobiomodulation Therapy for the Improvement of Muscular Performance and Reduction of Muscular Fatigue Associated with Exercise in Healthy People: A Systematic Review and Meta-Analysis},
  author = {Vanin, A.A. and Verhagen, E. and Barboza, S.D. and Costa, L.O.P. and {Leal-Junior}, E.C.P.},
  year = {2018},
  journal = {Lasers in Medical Science},
  volume = {33},
  number = {1},
  pages = {181--214},
  publisher = {Springer London},
  doi = {10.1007/s10103-017-2368-6}
}

@misc{vanlissaBeFAIRTheory2025,
  title = {To Be {{FAIR}}: {{Theory Specification Needs}} an {{Update}}},
  shorttitle = {To Be {{FAIR}}},
  author = {Van Lissa, Caspar and Peikert, Aaron and Ernst, Maximilian and Van Dongen, Noah and Sch{\"o}nbrodt, Felix and Brandmaier, Andreas},
  year = {2025},
  month = mar,
  publisher = {OSF},
  doi = {10.31234/osf.io/t53np_v1},
  urldate = {2025-06-18},
  abstract = {Innovations in open science and meta-science have focused on rigorous *theory testing*, yet methods for specifying, sharing, and iteratively improving theories remain underdeveloped. To address these limitations, we introduce *FAIR theory*: A standard for specifying theories as Findable, Accessible, Interoperable, and Reusable information artifacts. FAIR theories are Findable in well-established archives, Accessible in practical terms and in terms of their ability to be understood, Interoperable for specific purposes, e.g., to guide control variable selection, and Reusable so that they can be iteratively improved through collaborative efforts. This paper adapts the FAIR principles for theory, reflects on the FAIRness of contemporary theoretical practices in psychology, introduces a workflow for FAIRifying theory, and explores FAIR theories' potential impact in terms of reducing research waste, enabling meta-research on the structure and development of theories, and incorporating theory into reproducible research workflows -- from hypothesis generation to simulation studies. We make use of well-established open science infrastructure, including Git for version control, GitHub for collaboration, and Zenodo for archival and search indexing. By applying the principles and infrastructure that have already revolutionized sharing of data and publications to theory,  we establish a sustainable, transparent, and collaborative approach to theory development. FAIR theory equips scholars with a standard for systematically specifying and refining theories, bridging a critical gap in open research practices and supporting the renewed interest in theory development in psychology and beyond. FAIR theory provides a structured, cumulative framework for theory development, increasing efficiency and potentially accelerating the pace of cumulative knowledge acquisition},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {fairtheory,formal models,meta science,open science,theory formation},
  file = {/Users/cristian/Zotero/storage/V2KLYDCU/Van Lissa et al. - 2025 - To be FAIR Theory Specification Needs an Update.pdf}
}

@article{vazireImplicationsCredibilityRevolution2018,
  title = {Implications of the {{Credibility Revolution}} for {{Productivity}}, {{Creativity}}, and {{Progress}}},
  author = {Vazire, Simine},
  year = {2018},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {4},
  pages = {411--417},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691617751884},
  urldate = {2021-11-15},
  abstract = {The credibility revolution (sometimes referred to as the ``replicability crisis'') in psychology has brought about many changes in the standards by which psychological science is evaluated. These changes include (a) greater emphasis on transparency and openness, (b) a move toward preregistration of research, (c) more direct-replication studies, and (d) higher standards for the quality and quantity of evidence needed to make strong scientific claims. What are the implications of these changes for productivity, creativity, and progress in psychological science? These questions can and should be studied empirically, and I present my predictions here. The productivity of individual researchers is likely to decline, although some changes (e.g., greater collaboration, data sharing) may mitigate this effect. The effects of these changes on creativity are likely to be mixed: Researchers will be less likely to pursue risky questions; more likely to use a broad range of methods, designs, and populations; and less free to define their own best practices and standards of evidence. Finally, the rate of scientific progress---the most important shared goal of scientists---is likely to increase as a result of these changes, although one's subjective experience of making progress will likely become rarer.},
  langid = {english},
  keywords = {creativity,credibility,productivity,scientific progress,transparency}
}

@article{vazireQualityUncertaintyErodes2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  author = {Vazire, Simine},
  year = {2017},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {3},
  number = {1},
  pages = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.74},
  urldate = {2022-10-28},
  abstract = {When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars -- they cannot reliably tell the difference between lemons and high quality findings. This phenomenon -- quality uncertainty -- has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals' reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so -- we should be held to a higher standard than used car salespeople.},
  file = {/Users/cristian/Zotero/storage/J6II97DZ/Vazire - 2017 - Quality Uncertainty Erodes Trust in Science.pdf;/Users/cristian/Zotero/storage/R3WX6RMZ/Quality-Uncertainty-Erodes-Trust-in-Science.html}
}

@article{vazireWhereAreSelfCorrecting2021,
  title = {Where Are the {{Self-Correcting Mechanisms}} in {{Science}}?},
  author = {Vazire, Simine and Holcombe, Alex O.},
  year = {2021},
  month = aug,
  journal = {Review of General Psychology},
  pages = {10892680211033912},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1177/10892680211033912},
  urldate = {2022-03-15},
  abstract = {It is often said that science is self-correcting, but the replication crisis suggests that self-correction mechanisms have fallen short. How can we know whether a particular scientific field has effective self-correction mechanisms, that is, whether its findings are credible? The usual processes that supposedly provide mechanisms for scientific self-correction, such as journal-based peer review and institutional committees, have been inadequate. We describe more verifiable indicators of a field's commitment to self-correction. These fall under the broad headings of 1) transparency, which is already the subject of many reform efforts and 2) critical appraisal, which has received less attention and which we focus on here. Only by obtaining Observable Self-Correction Indicators (OSCIs) can we begin to evaluate the claim that ``science is self-correcting.'' We expect that the veracity of this claim varies across fields and subfields, and suggest that some fields, such as psychology and biomedicine, fall far short of an appropriate level of transparency and, especially, critical appraisal. Fields without robust, verifiable mechanisms for transparency and critical appraisal cannot reasonably be said to be self-correcting, and thus do not warrant the credibility often imputed to science as a whole.},
  langid = {english},
  keywords = {epistemology,history of psychology,metascience,replication}
}

@article{veldkampStatisticalReportingErrors2014,
  title = {Statistical {{Reporting Errors}} and {{Collaboration}} on {{Statistical Analyses}} in {{Psychological Science}}},
  author = {Veldkamp, Coosje L. S. and Nuijten, Mich{\`e}le B. and {Dominguez-Alvarez}, Linda and van Assen, Marcel A. L. M. and Wicherts, Jelte M.},
  year = {2014},
  month = dec,
  journal = {PLOS ONE},
  volume = {9},
  number = {12},
  pages = {e114876},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0114876},
  urldate = {2022-08-31},
  abstract = {Statistical analysis is error prone. A best practice for researchers using statistics would therefore be to share data among co-authors, allowing double-checking of executed tasks just as co-pilots do in aviation. To document the extent to which this `co-piloting' currently occurs in psychology, we surveyed the authors of 697 articles published in six top psychology journals and asked them whether they had collaborated on four aspects of analyzing data and reporting results, and whether the described data had been shared between the authors. We acquired responses for 49.6\% of the articles and found that co-piloting on statistical analysis and reporting results is quite uncommon among psychologists, while data sharing among co-authors seems reasonably but not completely standard. We then used an automated procedure to study the prevalence of statistical reporting errors in the articles in our sample and examined the relationship between reporting errors and co-piloting. Overall, 63\% of the articles contained at least one p-value that was inconsistent with the reported test statistic and the accompanying degrees of freedom, and 20\% of the articles contained at least one p-value that was inconsistent to such a degree that it may have affected decisions about statistical significance. Overall, the probability that a given p-value was inconsistent was over 10\%. Co-piloting was not found to be associated with reporting errors.},
  langid = {english},
  keywords = {Applied psychology,Clinical psychology,Developmental psychology,Experimental psychology,Psychology,Social psychology,Statistical data,Surveys},
  file = {/Users/cristian/Zotero/storage/78HRL84Z/Veldkamp et al. - 2014 - Statistical Reporting Errors and Collaboration on .pdf;/Users/cristian/Zotero/storage/LMNXM95F/article.html}
}

@article{venturelliEFFECTLEGPREFERENCE2021,
  title = {{{THE EFFECT OF LEG PREFERENCE ON MECHANICAL EFFICIENCY DURING SINGLE-LEG EXTENSION EXERCISE}}},
  author = {Venturelli, Massimo and Tarperi, Cantor and Milanese, Chiara and Festa, Luca and Toniolo, Luana and Reggiani, Carlo and Schena, Federico F.},
  year = {2021},
  month = jun,
  journal = {Journal of Applied Physiology},
  publisher = {American Physiological Society Rockville, MD},
  doi = {10.1152/japplphysiol.01002.2020},
  urldate = {2023-08-16},
  abstract = {To investigate how leg preference affects net efficiency ({$\eta$}net), we examined central and peripheral hemodynamics, muscle fiber type, activation and force of preferred (PL) and nonpreferred (NPL) leg. Our hypothesis was that PL greater efficiency could be explained by adaptations and interactions between central, peripheral factors, and force. Fifteen young participants performed single-leg extension exercise at absolute (35\,W) and relative [50\% peak power-output (Wpeak)] workloads with PL and NPL. Oxygen uptake, photoplethysmography, Doppler ultrasound, near-infrared-spectroscopy deoxyhemoglobin [HHb], integrated electromyography (iEMG), maximal isometric force (MVC), rate of force development (RFD50--100), and muscle biopsies of both vastus lateralis were studied to assess central and peripheral determinants of {$\eta$}net. During exercise executed at 35\,W, {$\eta$}net was 17.5\,{\textpm}\,5.1\% and 11.9\,{\textpm}\,2.1\% (P {$<$} 0.01) in PL and NPL respectively, whereas during exercise at the 50\% of Wpeak was in PL\,=\,18.1\,{\textpm}\,5.1\% and in NPL\,=\,12.5\,{\textpm}\,1.9 (P {$<$} 0.01). The only parameter correlated with {$\eta$}net was iEMG, which showed an inverse correlation for absolute (r = -0.83 and -0.69 for PL and NPL) and relative workloads (r = -0.92 and -0.79 for PL and NPL). MVC and RFD50--100 were higher in PL than in NPL but not correlated to {$\eta$}net. This study identified a critical role of leg preference in the efficiency during single-leg extension exercise. The whole spectrum of the central and peripheral, circulatory, and muscular determinants of {$\eta$}net did not explain the difference between PL and NPL efficiency. Therefore, the lower muscle activation exhibited by the PL is likely the primary determinant of this physiological phenomenon. NEW \& NOTEWORTHY This study examined the impact of leg preference on efficiency during single-leg exercise. The results revealed lower efficiency of the nonpreferred leg during exercises performed at absolute and relative workloads. Central (cardiac output) and peripheral (fiber typing) determinants of efficiency did not explain the difference between the legs. However, the lower muscle activation of the preferred leg that was inversely correlated with efficiency is likely the primary determinant of this physiological feature.},
  copyright = {Copyright {\copyright} 2021, Journal of Applied Physiology},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/PJLQM2I3/japplphysiol.01002.html}
}

@article{veronesiAssociationKlothoPhysical2021,
  title = {Association of {{Klotho}} with Physical Performance and Frailty in Middle-Aged and Older Adults: {{A}} Systematic Review},
  author = {Veronesi, F. and Borsari, V. and Cherubini, A. and Fini, M.},
  year = {2021},
  journal = {Experimental Gerontology},
  volume = {154},
  publisher = {Elsevier Inc.},
  doi = {10.1016/j.exger.2021.111518}
}

@article{veveaGeneralLinearModel1995,
  title = {A General Linear Model for Estimating Effect Size in the Presence of Publication Bias},
  author = {Vevea, Jack L. and Hedges, Larry V.},
  year = {1995},
  month = sep,
  journal = {Psychometrika},
  volume = {60},
  number = {3},
  pages = {419--435},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02294384},
  urldate = {2022-04-12},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UTTFFI8L/Vevea and Hedges - 1995 - A general linear model for estimating effect size .pdf}
}

@article{vickerstaffMethodsAdjustMultiple2019,
  title = {Methods to Adjust for Multiple Comparisons in the Analysis and Sample Size Calculation of Randomised Controlled Trials with Multiple Primary Outcomes},
  author = {Vickerstaff, Victoria and Omar, Rumana Z. and Ambler, Gareth},
  year = {2019},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {129},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0754-4},
  urldate = {2022-09-17},
  abstract = {Multiple primary outcomes may be specified in randomised controlled trials (RCTs). When analysing multiple outcomes it's important to control the family wise error rate (FWER). A popular approach to do this is to adjust the p-values corresponding to each statistical test used to investigate the intervention effects by using the Bonferroni correction. It's also important to consider the power~of the trial to detect true intervention effects. In the context of multiple outcomes, depending on the clinical objective, the power can be defined as: `disjunctive power', the probability of detecting at least one true intervention effect across all the outcomes or `marginal power' the probability of finding a true intervention effect on a nominated outcome.},
  keywords = {Multiple comparison methods,Multiple outcome,Randomised controlled trials,Sample size,Statistical analysis},
  file = {/Users/cristian/Zotero/storage/EE99PVKH/Vickerstaff et al. - 2019 - Methods to adjust for multiple comparisons in the .pdf}
}

@article{viechtbauerAccountingHeterogeneityRandomeffects2007,
  title = {Accounting for Heterogeneity via Random-Effects Models and Moderator Analyses in Meta-Analysis},
  author = {Viechtbauer, Wolfgang},
  year = {2007},
  journal = {Zeitschrift f{\"u}r Psychologie/Journal of Psychology},
  volume = {215},
  number = {2},
  pages = {104--121},
  publisher = {Hogrefe \& Huber Publishers},
  address = {Germany},
  issn = {0044-3409},
  doi = {10.1027/0044-3409.215.2.104},
  abstract = {To conduct a meta-analysis, one needs to express the results from a set of related studies in terms of an outcome measure, such as a standardized mean difference, correlation coefficient, or odds ratio. The observed outcome from a single study will differ from the true value of the outcome measure because of sampling variability. The observed outcomes from a set of related studies measuring the same outcome will, therefore, not coincide. However, one often finds that the observed outcomes differ more from each other than would be expected based on sampling variability alone. A likely explanation for this phenomenon is that the true values of the outcome measure are heterogeneous. One way to account for the heterogeneity is to assume that the heterogeneity is entirely random. Another approach is to examine whether the heterogeneity in the outcomes can be accounted for, at least in part, by a set of study-level variables describing the methods, procedures, and samples used in the different studies. The purpose of the present paper is to discuss these different approaches with particular emphasis on the interpretation of the results and practical issues. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Homogeneity of Variance,Meta Analysis,Models,Statistical Regression,Statistical Variables,Treatment Effectiveness Evaluation,Treatment Outcomes},
  file = {/Users/cristian/Zotero/storage/T4H2QRYK/2007-19368-002.html}
}

@article{viechtbauerComparisonProceduresTest2015,
  title = {A Comparison of Procedures to Test for Moderators in Mixed-Effects Meta-Regression Models},
  author = {Viechtbauer, Wolfgang and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2015},
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {360--374},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000023},
  abstract = {Several alternative methods are available when testing for moderators in mixed-effects meta-regression models. A simulation study was carried out to compare different methods in terms of their Type I error and statistical power rates. We included the standard (Wald-type) test, the method proposed by Knapp and Hartung (2003) in 2 different versions, the Huber--White method, the likelihood ratio test, and the permutation test in the simulation study. These methods were combined with 7 estimators for the amount of residual heterogeneity in the effect sizes. Our results show that the standard method, applied in most meta-analyses up to date, does not control the Type I error rate adequately, sometimes leading to overly conservative, but usually to inflated, Type I error rates. Of the different methods evaluated, only the Knapp and Hartung method and the permutation test provide adequate control of the Type I error rate across all conditions. Due to its computational simplicity, the Knapp and Hartung method is recommended as a suitable option for most meta-analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Meta Analysis,Statistical Estimation,Statistical Power,Testing,Type I Errors},
  file = {/Users/cristian/Zotero/storage/EUZQBWJK/Viechtbauer et al. - 2015 - A comparison of procedures to test for moderators .pdf}
}

@article{viechtbauerConductingMetaAnalysesMetafor2010,
  title = {Conducting {{Meta-Analyses}} in {{R}} with the Metafor {{Package}}},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  volume = {36},
  number = {3},
  pages = {1--48},
  doi = {10.18637/jss.v036.i03},
  urldate = {2022-04-11},
  file = {/Users/cristian/Zotero/storage/29AWJKBY/v036i03.html}
}

@article{viechtbauerOutlierInfluenceDiagnostics2010,
  title = {Outlier and Influence Diagnostics for Meta-Analysis},
  author = {Viechtbauer, Wolfgang and Cheung, Mike W.-L.},
  year = {2010},
  month = apr,
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {2},
  pages = {112--125},
  issn = {1759-2879},
  doi = {10.1002/jrsm.11},
  abstract = {The presence of outliers and influential cases may affect the validity and robustness of the conclusions from a meta-analysis. While researchers generally agree that it is necessary to examine outlier and influential case diagnostics when conducting a meta-analysis, limited studies have addressed how to obtain such diagnostic measures in the context of a meta-analysis. The present paper extends standard diagnostic procedures developed for linear regression analyses to the meta-analytic fixed- and random/mixed-effects models. Three examples are used to illustrate the usefulness of these procedures in various research settings. Issues related to these diagnostic procedures in meta-analysis are also discussed. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  pmid = {26061377},
  keywords = {influence diagnostics,meta-analysis,mixed-effects model,outliers}
}

@article{wacholderAssessingProbabilityThat2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, Sholom and Chanock, Stephen and {Garcia-Closas}, Montserrat and {El ghormli}, Laure and Rothman, Nathaniel},
  year = {2004},
  month = mar,
  journal = {JNCI: Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874},
  doi = {10.1093/jnci/djh075},
  urldate = {2021-01-25},
  abstract = {Too many reports of associations between genetic variants and common cancer sites and other complex diseases are false positives. A major reason for this unfortunate situation is the strategy of declaring statistical significance based on a P value alone, particularly, any P value below .05. The false positive report probability (FPRP), the probability of no true association between a genetic variant and disease given a statistically significant finding, depends not only on the observed P value but also on both the prior probability that the association between the genetic variant and the disease is real and the statistical power of the test. In this commentary, we show how to assess the FPRP and how to use it to decide whether a finding is deserving of attention or ``noteworthy.'' We show how this approach can lead to improvements in the design, analysis, and interpretation of molecular epidemiology studies. Our proposal can help investigators, editors, and readers of research articles to protect themselves from overinterpreting statistically significant findings that are not likely to signify a true association. An FPRP-based criterion for deciding whether to call a finding noteworthy formalizes the process already used informally by investigators---that is, tempering enthusiasm for remarkable study findings with considerations of plausibility.},
  file = {/Users/cristian/Zotero/storage/UGUN32DK/Wacholder et al. - 2004 - Assessing the Probability That a Positive Report i.pdf;/Users/cristian/Zotero/storage/4L7W24WC/2606750.html}
}

@article{wagenmakersAgendaPurelyConfirmatory2012,
  title = {An {{Agenda}} for {{Purely Confirmatory Research}}},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J. and Kievit, Rogier A.},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {632--638},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691612463078},
  urldate = {2022-10-07},
  abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology?s academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result?a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label ?confirmatory,? and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled ?exploratory.? We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/M8BGSVZL/Wagenmakers et al. - 2012 - An Agenda for Purely Confirmatory Research.pdf}
}

@article{wagenmakersPracticalSolutionPervasive2007,
  ids = {wagenmakersPracticalSolutionPervasive2007a},
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384},
  doi = {10.3758/bf03194105},
  abstract = {In the field of psychology, the practice of p value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of the p value procedure. In particular, p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover, p values do not quantify statistical evidence. This article reviews these p value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to these p value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  langid = {english},
  pmid = {18087943},
  keywords = {{Models, Psychological},Bayes Theorem,Bayesian Information Criterion,Humans,Null Hypothesis,Posterior Probability,Prior Distribution,Psychology,Statistical Inference},
  file = {/Users/cristian/Zotero/storage/ZD9N7I6Z/Wagenmakers - 2007 - A practical solution to the pervasive problems of .pdf}
}

@article{wampoldHypothesisValidityClinical1990,
  title = {Hypothesis Validity of Clinical Research},
  author = {Wampold, Bruce E. and Davis, Betsy and Good III, Roland H.},
  year = {1990},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {58},
  number = {3},
  pages = {360--367},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2117},
  doi = {10.1037/0022-006X.58.3.360},
  abstract = {Hypothesis validity refers to the extent to which research results reflect theoretically derived predictions about the relations between or among constructs. The role of hypotheses in theory testing is discussed. Four threats to hypothesis validity are presented: (a) inconsequential research hypotheses, (b) ambiguous research hypotheses, (c) noncongruence of research hypotheses and statistical hypotheses, and (d) diffuse statistical hypotheses and tests. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Clinical Psychology,Experimentation,Hypothesis Testing,Theories},
  file = {/Users/cristian/Zotero/storage/LPYNB6EB/1990-28928-001.html}
}

@article{wangComparisonContinuousSubcutaneous2021,
  title = {Comparison of {{Continuous Subcutaneous Insulin Infusion}} and {{Multiple Daily Injections}} in {{Pediatric Type}} 1 {{Diabetes}}: {{A Meta}}-{{Analysis}} and {{Prospective Cohort Study}}},
  author = {Wang, X. and Zhao, X. and Chen, D. and Zhang, M. and Gu, W.},
  year = {2021},
  journal = {Frontiers in Endocrinology},
  volume = {12},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fendo.2021.608232}
}

@article{wangDevelopmentValidationRisk2022,
  title = {Development and {{Validation}} of {{Risk Prediction Models}} for {{Gestational Diabetes Mellitus Using Four Different Methods}}},
  author = {Wang, N. and Guo, H. and Jing, Y. and Song, L. and Chen, H. and Wang, M. and Gao, L. and Huang, L. and Song, Y. and Sun, B. and Cui, W. and Xu, J.},
  year = {2022},
  journal = {Metabolites},
  volume = {12},
  number = {11},
  publisher = {MDPI},
  doi = {10.3390/metabo12111040}
}

@article{wangDiagnosticPerformanceMagnetic2016,
  title = {The Diagnostic Performance of Magnetic Resonance Spectroscopy in Differentiating High-from Low-Grade Gliomas: {{A}} Systematic Review and Meta-Analysis},
  author = {Wang, Q. and Zhang, H. and Zhang, J.S. and Wu, C. and Zhu, W.J. and Li, F.Y. and Chen, X.L. and Xu, B.N.},
  year = {2016},
  journal = {European Radiology},
  volume = {26},
  number = {8},
  pages = {2670--2684},
  publisher = {Springer Verlag},
  doi = {10.1007/s00330-015-4046-z}
}

@article{wangEfficacySafetyImmune2018,
  title = {Efficacy and Safety of Immune Checkpoint Inhibitors in Non-Small Cell Lung Cancer},
  author = {Wang, S. and Hao, J. and Wang, H. and Fang, Y. and Tan, L.},
  year = {2018},
  journal = {OncoImmunology},
  volume = {7},
  number = {8},
  publisher = {{Taylor and Francis Inc.}},
  doi = {10.1080/2162402X.2018.1457600}
}

@article{warmenhovenSportsMetaresearchEmerging2025,
  title = {Sports {{Metaresearch}}: {{An Emerging Discipline}} of {{Sport Science}} and {{Medicine}}},
  shorttitle = {Sports {{Metaresearch}}},
  author = {Warmenhoven, John and Menasp{\`a}, Paolo and Borg, David N. and Vazire, Simine and White, Nicole and Sainani, Kristin and Nimphius, Sophia and Coutts, Aaron J. and Impellizzeri, Franco M.},
  year = {2025},
  month = apr,
  journal = {Sports Medicine},
  volume = {55},
  number = {4},
  pages = {845--856},
  issn = {1179-2035},
  doi = {10.1007/s40279-025-02181-x},
  urldate = {2025-06-08},
  abstract = {Inadequacies in the conduct and quality of research are well established across many research domains, including sport science and medicine. Metaresearch---the practice of performing research on research---is presented as a practical vehicle for improving research quality through evaluating the research processes. This article introduces the concept of metaresearch to sport as a new sub-field of sport science. The broad types of metaresearch are introduced, with a mapping of current sports metaresearch activity across these areas. Interdisciplinary centres aimed at improving scientific quality across other fields are also introduced to sport, and specific considerations for beginning metaresearch are provided for sport. This includes, for example, not performing metaresearch poorly, beginning evaluative metaresearch early to intervene before bad practice becomes normalised, leveraging required interdisciplinary expertise depending on the metaresearch question and undertaking an ethical approach for carrying out evaluation of research quality.},
  langid = {english},
  keywords = {Sport Analytics,Sport Physiology,Sport Psychology,Sport Science,Sport Sociology,Sport Theory},
  file = {/Users/cristian/Zotero/storage/DEGJRZYT/Warmenhoven et al. - 2025 - Sports Metaresearch An Emerging Discipline of Spo.pdf}
}

@article{warrenFirstAnalysisPreregistered2018,
  title = {First Analysis of `Pre-Registered' Studies Shows Sharp Rise in Null Findings},
  author = {Warren, Matthew},
  year = {2018},
  month = oct,
  journal = {Nature},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-018-07118-1},
  urldate = {2021-02-06},
  abstract = {Logging hypotheses and protocols before performing research seems to work as intended: to reduce publication bias for positive results.},
  copyright = {2020 Nature},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/WZNH9NLW/d41586-018-07118-1.html}
}

@article{weiEffectSGLT2Inhibitors2021,
  title = {Effect of {{SGLT2 Inhibitors}} on {{Type}} 2 {{Diabetes Mellitus With Non-Alcoholic Fatty Liver Disease}}: {{A Meta-Analysis}} of {{Randomized Controlled Trials}}},
  author = {Wei, Q. and Xu, X. and Guo, L. and Li, J. and Li, L.},
  year = {2021},
  journal = {Frontiers in Endocrinology},
  volume = {12},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fendo.2021.635556}
}

@article{weiNitratesStableAngina2011,
  title = {Nitrates for Stable Angina: {{A}} Systematic Review and Meta-Analysis of Randomized Clinical Trials},
  author = {Wei, J. and Wu, T. and Yang, Q. and Chen, M. and Ni, J. and Huang, D.},
  year = {2011},
  journal = {International Journal of Cardiology},
  volume = {146},
  number = {1},
  pages = {4--12},
  doi = {10.1016/j.ijcard.2010.05.019}
}

@article{weiselCarfilzomib56Mg2022,
  title = {Carfilzomib 56 Mg/M2 Twice-Weekly in Combination with Dexamethasone and Daratumumab ({{KdD}}) versus Daratumumab in Combination with Bortezomib and Dexamethasone ({{DVd}}): A Matching-Adjusted Indirect Treatment Comparison},
  author = {Weisel, K. and Nooka, A.K. and Terpos, E. and Spencer, A. and Goldschmidt, H. and Dirnberger, F. and DeCosta, L. and Yusuf, A. and Kumar, S.},
  year = {2022},
  journal = {Leukemia and Lymphoma},
  volume = {63},
  number = {8},
  pages = {1887--1896},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/10428194.2022.2047962}
}

@article{westfall_2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  month = oct,
  journal = {Journal of Experimental Psychology. General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli.},
  langid = {english},
  pmid = {25111580},
  keywords = {{Models, Statistical},Humans,Research Design,Sample Size,Statistics as Topic}
}

@article{westPCurveAnalysisTaylor2021,
  title = {P-{{Curve}} Analysis of the {{Taylor Aggression Paradigm}}: {{Estimating}} Evidentiary Value and Statistical Power across 50 Years of Research},
  shorttitle = {P-{{Curve}} Analysis of the {{Taylor Aggression Paradigm}}},
  author = {West, Samuel J. and Hyatt, Courtland S. and Miller, Joshua D. and Chester, David S.},
  year = {2021},
  journal = {Aggressive Behavior},
  volume = {47},
  number = {2},
  pages = {183--193},
  issn = {1098-2337},
  doi = {10.1002/ab.21937},
  urldate = {2021-04-13},
  abstract = {The overall reliability or evidentiary value of any body of literature is established in part by ruling out publication bias for any observed effects. Questionable research practices have potentially undermined the evidentiary value of commonly used research paradigms in psychological science. Subsequently, the evidentiary value of these common methodologies remains uncertain. To quantify the severity of these issues in the literature, we selected the Taylor Aggression Paradigm (TAP) as a case study and submitted 170 hypothesis tests spanning over 50 years of research to a preregistered p-curve analysis. The TAP literature (N = 24,685) demonstrated significant evidentiary value but yielded a small average effect size (d = 0.29) and inadequate power (38\%). The main effects demonstrated greater evidentiary value, power, and effect sizes than interactions. Studies that tested the effects of measured traits did not differ in evidentiary value or power to those that tested the effects of experimentally manipulated states. Exploratory analyses revealed that evidentiary value, statistical power, and effect sizes have improved over time. We provide recommendations for researchers who seek to maximize the evidentiary value of their psychological measures.},
  copyright = {{\copyright} 2020 Wiley Periodicals LLC},
  langid = {english},
  keywords = {aggressive behavior,evidentiary value,meta-analysis,p-curve,Taylor Aggression Paradigm},
  file = {/Users/cristian/Zotero/storage/CBIXAUEY/West et al. - 2021 - p-Curve analysis of the Taylor Aggression Paradigm.pdf;/Users/cristian/Zotero/storage/ZG3YZQT3/10.1007_s11229-021-03276-4-citation.ris;/Users/cristian/Zotero/storage/DEG5ZH6R/ab.html}
}

@misc{WhenHowDeviate,
  title = {When and {{How}} to {{Deviate From}} a {{Preregistration}} {\textbar} {{Collabra}}: {{Psychology}} {\textbar} {{University}} of {{California Press}}},
  urldate = {2025-03-13},
  howpublished = {https://online.ucpress.edu/collabra/article/10/1/117094/200749/When-and-How-to-Deviate-From-a-Preregistration},
  file = {/Users/cristian/Zotero/storage/9YHVLY8V/When-and-How-to-Deviate-From-a-Preregistration.html}
}

@article{wichertsDegreesFreedomPlanning2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2016},
  month = nov,
  journal = {Frontiers in Psychology},
  volume = {7},
  pages = {1832},
  doi = {10.3389/fpsyg.2016.01832},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  file = {/Users/cristian/Zotero/storage/ZSV6HSSE/Wicherts et al. - 2016 - Degrees of Freedom in Planning, Running, Analyzing.pdf}
}

@article{wichertsWillingnessShareResearch2011,
  title = {Willingness to {{Share Research Data Is Related}} to the {{Strength}} of the {{Evidence}} and the {{Quality}} of {{Reporting}} of {{Statistical Results}}},
  author = {Wicherts, Jelte M. and Bakker, Marjan and Molenaar, Dylan},
  year = {2011},
  month = nov,
  journal = {PLOS ONE},
  volume = {6},
  number = {11},
  pages = {e26828},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0026828},
  abstract = {Background The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.},
  langid = {english},
  keywords = {Data management,Experimental design,Medical journals,Psychologists,Psychology,Statistical data,Statistical distributions,Test statistics},
  file = {/Users/cristian/Zotero/storage/M55S7JX5/Wicherts et al. - 2011 - Willingness to Share Research Data Is Related to t.pdf;/Users/cristian/Zotero/storage/FQ5P2GV4/article.html}
}

@article{wilbornPilotStudyExamining2017,
  title = {A {{Pilot Study Examining}} the {{Effects}} of 8-{{Week Whey Protein}} versus {{Whey Protein Plus Creatine Supplementation}} on {{Body Composition}} and {{Performance Variables}} in {{Resistance-Trained Women}}},
  author = {Wilborn, C.D. and Outlaw, J.J. and Mumford, P.W. and Urbina, S.L. and Hayward, S. and Roberts, M.D. and Taylor, L.W. and Foster, C.A.},
  year = {2017},
  journal = {Annals of Nutrition and Metabolism},
  volume = {69},
  number = {3-4},
  pages = {190--199},
  publisher = {S. Karger AG},
  doi = {10.1159/000452845}
}

@article{wilkinson_fair,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {1--9},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  urldate = {2021-11-05},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders---representing academia, industry, funding agencies, and scholarly publishers---have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {/Users/cristian/Zotero/storage/UHWCWVPU/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;/Users/cristian/Zotero/storage/JJBXHHJ5/sdata201618.html}
}

@article{williamsonSubgroupAnalysesRandomized2022,
  ids = {williamsonSubgroupAnalysesRandomized2022a},
  title = {Subgroup Analyses in Randomized Controlled Trials Frequently Categorized Continuous Subgroup Information},
  author = {Williamson, S. Faye and Grayling, Michael J. and Mander, Adrian P. and Noor, Nurulamin M. and Savage, Joshua S. and Yap, Christina and Wason, James M. S.},
  year = {2022},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {150},
  pages = {72--79},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2022.06.017},
  urldate = {2025-05-21},
  abstract = {Background and Objectives To investigate how subgroup analyses of published Randomized Controlled Trials (RCTs) are performed when subgroups are created from continuous variables. Methods We carried out a review of RCTs published in 2016--2021 that included subgroup analyses. Information was extracted on whether any of the subgroups were based on continuous variables and, if so, how they were analyzed. Results Out of 428 reviewed papers, 258 (60.4\%) reported RCTs with a subgroup analysis. Of these, 178/258 (69\%) had at least one subgroup formed from a continuous variable and 14/258 (5.4\%) were unclear. The vast majority (169/178, 94.9\%) dichotomized the continuous variable and treated the subgroup as categorical. The most common way of dichotomizing was using a pre-specified cutpoint (129/169, 76.3\%), followed by a data-driven cutpoint (26/169, 15.4\%), such as the median. Conclusion It is common for subgroup analyses to use continuous variables to define subgroups. The vast majority dichotomize the continuous variable and, consequently, may lose substantial amounts of statistical information (equivalent to reducing the sample size by at least a third). More advanced methods that can improve efficiency, through optimally choosing cutpoints or directly using the continuous information, are rarely used.},
  keywords = {Categorization,Continuous variables,Dichotomization,Moderator analysis,Randomized controlled trials,Subgroup analysis},
  file = {/Users/cristian/Zotero/storage/A2WEIYNK/Williamson et al. - 2022 - Subgroup analyses in randomized controlled trials .pdf;/Users/cristian/Zotero/storage/LAN3YJAL/S0895435622001688.html}
}

@article{wilsonPriorOddsTesting2018,
  title = {The {{Prior Odds}} of {{Testing}} a {{True Effect}} in {{Cognitive}} and {{Social Psychology}}},
  author = {Wilson, Brent M. and Wixted, John T.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {186--197},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918767122},
  urldate = {2023-01-13},
  abstract = {Efforts to increase replication rates in psychology generally consist of recommended improvements to methodology, such as increasing sample sizes to increase power or using a lower alpha level. However, little attention has been paid to how the prior odds (R) that a tested effect is true can affect the probability that a significant result will be replicable. The lower R is, the less likely a published result will be replicable even if power is high. It follows that if R is lower in one set of studies than in another, then all else being equal, published results will be less replicable in the set with lower R. We illustrate this point by presenting an analysis of data from the social-psychology and cognitive-psychology studies that were included in the Open Science Collaboration?s (2015) replication project. We found that R was lower for the social-psychology studies than for the cognitive-psychology studies, which might explain why the rate of successful replications differed between these two sets of studies. This difference in replication rates may reflect the degree to which scientists in the two fields value risky but potentially groundbreaking (i.e., low-R) research. Critically, high-R research is not inherently better or worse than low-R research for advancing knowledge. However, if they wish to achieve replication rates comparable to those of high-R fields (a judgment call), researchers in low-R fields would need to use an especially low alpha level, conduct experiments that have especially high power, or both.},
  langid = {english},
  file = {/Users/cristian/Zotero/storage/UE8HD3XX/Wilson and Wixted - 2018 - The Prior Odds of Testing a True Effect in Cogniti.pdf}
}

@article{wingoInfluencePreexerciseGlycerol2004,
  ids = {wingoInfluencePreExerciseGlycerol2004,wingoInfluencePreExerciseGlycerol2004a},
  title = {Influence of a Pre-Exercise Glycerol Hydration Beverage on Performance and Physiologic Function during Mountain-Bike Races in Heat},
  author = {Wingo, J.E. and Casa, D.J. and Berger, E.M. and Dellis, W.O. and Knight, J.C. and McClung, J.M.},
  year = {2004},
  journal = {Journal of Athletic Training},
  volume = {39},
  number = {2},
  pages = {169--175},
  file = {/Users/cristian/Zotero/storage/PII27ZBK/Wingo et al. - 2004 - Influence of a Pre-Exercise Glycerol Hydration Bev.pdf}
}

@article{withamSodiumBicarbonateImprove2020,
  title = {Sodium Bicarbonate to Improve Physical Function in Patients over 60 Years with Advanced Chronic Kidney Disease: The {{BiCARB RCT}}.},
  author = {Witham, Miles D. and Band, Margaret and Chong, Huey and Donnan, Peter T. and Hampson, Geeta and Hu, May Khei and Littleford, Roberta and Lamb, Edmund and Kalra, Philip A. and Kennedy, Gwen and McNamee, Paul and Plews, Deirdre and Rauchhaus, Petra and Soiza, Roy L. and Sumukadas, Deepa and Warwick, Graham and Avenell, Alison},
  year = {2020},
  month = jun,
  journal = {Health technology assessment (Winchester, England)},
  volume = {24},
  number = {27},
  pages = {1--90},
  address = {England},
  issn = {2046-4924 1366-5278},
  doi = {10.3310/hta24270},
  abstract = {BACKGROUND: Advanced chronic kidney disease is common in older people and is frequently accompanied by metabolic acidosis. Oral sodium bicarbonate is used to  treat this acidosis, but evidence is lacking on whether or not this provides a  net gain in health or quality of life for older people. OBJECTIVES: The  objectives were to determine whether or not oral bicarbonate therapy improves  physical function, quality of life, markers of renal function, bone turnover and  vascular health compared with placebo in older people with chronic kidney disease  and mild acidosis; to assess the safety of oral bicarbonate; and to establish  whether or not oral bicarbonate therapy is cost-effective in this setting.  DESIGN: A parallel-group, double-blind, placebo-controlled randomised trial.  SETTING: The setting was nephrology and geriatric medicine outpatient departments  in 27 UK hospitals. PARTICIPANTS: Participants were adults aged {$\geq$}\,60 years with  advanced chronic kidney disease (glomerular filtration rate category 4 or 5, not  on dialysis) with a serum bicarbonate concentration of {$<$}\,22\,mmol/l.  INTERVENTIONS: Eligible participants were randomised 1\,:\,1 to oral sodium  bicarbonate or matching placebo. Dosing started at 500\,mg three times daily,  increasing to 1\,g three times daily if the serum bicarbonate concentration was  {$<$}\,22\,mmol/l at 3 months. MAIN OUTCOME MEASURES: The primary outcome was the  between-group difference in the Short Physical Performance Battery score at 12  months, adjusted for baseline. Other outcome measures included generic and  disease-specific health-related quality of life, anthropometry, 6-minute walk  speed, grip strength, renal function, markers of bone turnover, blood pressure  and brain natriuretic peptide. All adverse events were recorded, including  commencement of renal replacement therapy. For the health economic analysis, the  incremental cost per quality-adjusted life-year was the main outcome. RESULTS: In  total, 300 participants were randomised, 152 to bicarbonate and 148 to placebo.  The mean age of participants was 74 years and 86 (29\%) were female. Adherence to  study medication was 73\% in both groups. A total of 220 (73\%) participants were  assessed at the 12-month visit. No significant treatment effect was evident for  the primary outcome of the between-group difference in the Short Physical  Performance Battery score at 12 months (-0.4 points, 95\% confidence interval -0.9  to 0.1 points; p\,=\,0.15). No significant treatment benefit was seen for any of  the secondary outcomes. Adverse events were more frequent in the bicarbonate arm  (457 vs. 400). Time to commencement of renal replacement therapy was similar in  both groups (hazard ratio 1.22, 95\% confidence interval 0.74 to 2.02; p\,=\,0.43).  Health economic analysis showed higher costs and lower quality of life in the  bicarbonate arm at 1 year, with additional costs of {\pounds}564 (95\% confidence interval  {\pounds}88 to {\pounds}1154) and a quality-adjusted life-year difference of -0.05 (95\%  confidence interval -0.08 to -0.01); placebo dominated bicarbonate under all  sensitivity analyses for incremental cost-effectiveness. LIMITATIONS: The trial  population was predominantly white and male, limiting generalisability. The  increment in serum bicarbonate concentrations achieved was small and a benefit  from larger doses of bicarbonate cannot be excluded. CONCLUSIONS: Oral sodium  bicarbonate did not improve a range of health measures in people aged {$\geq$}\,60 years  with chronic kidney disease category 4 or 5 and mild acidosis, and is unlikely to  be cost-effective for use in the NHS in this patient group. Once other current  trials of bicarbonate therapy in chronic kidney disease are complete, an  individual participant meta-analysis would be helpful to determine which  subgroups, if any, are more likely to benefit and which treatment regimens are  more beneficial. TRIAL REGISTRATION: Current Controlled Trials ISRCTN09486651 and  EudraCT 2011-005271-16. The systematic review is registered as PROSPERO  CRD42018112908. FUNDING: This project was funded by the National Institute for  Health Research (NIHR) Health Technology Assessment programme and will be  published in full in Health Technology Assessment; Vol. 24, No. 27. See the NIHR  Journals Library website for further project information.},
  langid = {english},
  pmcid = {PMC7336221},
  pmid = {32568065},
  keywords = {{Renal Insufficiency, Chronic/*drug therapy},*Exercise,ACIDOSIS,Aged,Biomarkers/*blood,CHRONIC,Cost-Benefit Analysis,Double-Blind Method,Female,Humans,Male,Quality of Life/*psychology,RANDOMISED CONTROLLED TRIAL,RENAL INSUFFICIENCY,SODIUM BICARBONATE,Sodium Bicarbonate/*administration \& dosage,United Kingdom}
}

@article{worldmedicalassociationWorldMedicalAssociation2013,
  title = {World {{Medical Association Declaration}} of {{Helsinki}}: {{Ethical Principles}} for {{Medical Research Involving Human Subjects}}},
  shorttitle = {World {{Medical Association Declaration}} of {{Helsinki}}},
  author = {{World Medical Association}},
  year = {2013},
  month = nov,
  journal = {JAMA},
  volume = {310},
  number = {20},
  pages = {2191--2194},
  issn = {0098-7484},
  doi = {10.1001/jama.2013.281053},
  urldate = {2025-06-08}
}

@article{worldmedicalassociationWorldMedicalAssociation2025a,
  title = {World {{Medical Association Declaration}} of {{Helsinki}}: {{Ethical Principles}} for {{Medical Research Involving Human Participants}}},
  shorttitle = {World {{Medical Association Declaration}} of {{Helsinki}}},
  author = {{World Medical Association}},
  year = {2025},
  month = jan,
  journal = {JAMA},
  volume = {333},
  number = {1},
  pages = {71--74},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.21972},
  urldate = {2025-05-29},
  file = {/Users/cristian/Zotero/storage/TXKLZ3H6/2825290.html}
}

@article{wuEffectsCreatineTrained2020,
  title = {Effects of {{Creatine}} in {{Trained Athletes}}: {{A Meta-analysis}} of 21 {{Randomized Placebo-Controlled Trials}}},
  author = {Wu, Y. and Hu, X. and Chen, L.},
  year = {2020},
  journal = {American journal of therapeutics},
  volume = {27},
  number = {5},
  pages = {e519-e523},
  publisher = {NLM (Medline)},
  doi = {10.1097/MJT.0000000000000974}
}

@article{wuShengmaiInjectionHypoxicischemic2008,
  title = {Shengmai Injection for Hypoxic-Ischemic Encephalopathy: {{A}} Systematic Review},
  author = {Wu, M. and Li, G. and Shen, Q.},
  year = {2008},
  journal = {Chinese Journal of Evidence-Based Medicine},
  volume = {8},
  number = {3},
  pages = {190--195}
}

@article{xuMETtargetedTherapiesTreatment2022,
  title = {{{MET-targeted}} Therapies for the Treatment of Non-Small-Cell Lung Cancer: {{A}} Systematic Review and Meta-Analysis},
  author = {Xu, L. and Wang, F. and Luo, F.},
  year = {2022},
  journal = {Frontiers in Oncology},
  volume = {12},
  publisher = {Frontiers Media S.A.},
  doi = {10.3389/fonc.2022.1013299}
}

@article{yangEfficacySafetyFingolimod2020,
  title = {The Efficacy and Safety of Fingolimod in Patients with Relapsing Multiple Sclerosis: {{A}} Meta-Analysis},
  author = {Yang, T. and Tian, X. and Chen, C.-Y. and Ma, L.-Y. and Zhou, S. and Li, M. and Wu, Y. and Zhou, Y. and Cui, Y.-M.},
  year = {2020},
  journal = {British Journal of Clinical Pharmacology},
  volume = {86},
  number = {4},
  pages = {637--645},
  publisher = {Blackwell Publishing Ltd},
  doi = {10.1111/bcp.14198}
}

@article{yangReportingPerformanceHepatocellular2021,
  title = {Reporting and {{Performance}} of {{Hepatocellular Carcinoma Risk Prediction Models}}: {{Based}} on {{TRIPOD Statement}} and {{Meta-Analysis}}},
  author = {Yang, L. and Wang, Q. and Cui, T. and Huang, J. and Jin, H.},
  year = {2021},
  journal = {Canadian Journal of Gastroenterology and Hepatology},
  volume = {2021},
  publisher = {Hindawi Limited},
  doi = {10.1155/2021/9996358}
}

@article{yaoIncludingNonrandomisedStudies2025,
  title = {Including Non-Randomised Studies of Interventions in Meta-Analyses of Randomised Controlled Trials Changed the Estimates in More than a Third of the Studies: {{Evidence}} from an Empirical Analysis},
  shorttitle = {Including Non-Randomised Studies of Interventions in Meta-Analyses of Randomised Controlled Trials Changed the Estimates in More than a Third of the Studies},
  author = {Yao, Minghong and Mei, Fan and Ma, Yu and Qin, Xuan and Huan, Jiayidaer and Zou, Kang and Li, Ling and Sun, Xin},
  year = {2025},
  month = may,
  journal = {Journal of Clinical Epidemiology},
  volume = {0},
  number = {0},
  publisher = {Elsevier},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2025.111815},
  urldate = {2025-05-06},
  langid = {english},
  keywords = {Consistency,Meta-analysis,Meta-epidemiology,Non-randomised studies of interventions,Randomised controlled trials,Statistical heterogeneity}
}

@article{yarizadhEffectLCarnitineSupplementation2020,
  title = {The {{Effect}} of {{L-Carnitine Supplementation}} on {{Exercise-Induced Muscle Damage}}: {{A Systematic Review}} and {{Meta-Analysis}} of {{Randomized Clinical Trials}}},
  author = {Yarizadh, H. and {Shab-Bidar}, S. and Zamani, B. and Vanani, A.N. and Baharlooi, H. and Djafarian, K.},
  year = {2020},
  journal = {Journal of the American College of Nutrition},
  volume = {39},
  number = {5},
  pages = {457--468},
  publisher = {Routledge},
  doi = {10.1080/07315724.2019.1661804}
}

@article{yarkoniGeneralizabilityCrisis2020,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2020},
  month = dec,
  journal = {The Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  issn = {1469-1825},
  doi = {10.1017/S0140525X20001685},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned - that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology - the linear mixed model - I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the "random effect" formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  pmid = {33342451},
  keywords = {Generalization,Humans,inference,Intention,philosophy of science,psychology,Psychology,random effects,statistics},
  file = {/Users/cristian/Zotero/storage/ER9EFK2E/Yarkoni - 2020 - The generalizability crisis.pdf}
}

@article{yauPerformanceParacetamolaminotransferaseMultiplication2023,
  title = {Performance of the Paracetamol-Aminotransferase Multiplication Product in Risk Stratification after Paracetamol (Acetaminophen) Poisoning: A Systematic Review and Meta-Analysis},
  author = {Yau, C.E. and Chen, H. and Lim, B.P.-Y. and Ng, M. and Ponampalam, R. and Lim, D.Y.Z. and Chin, Y.H. and Ho, A.F.W.},
  year = {2023},
  journal = {Clinical Toxicology},
  volume = {61},
  number = {1},
  pages = {1--11},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/15563650.2022.2152350}
}

@article{youngModelUncertaintyCrisis2018,
  title = {Model {{Uncertainty}} and the {{Crisis}} in {{Science}}},
  author = {Young, Cristobal},
  year = {2018},
  journal = {Socius},
  volume = {4},
  pages = {2378023117737206},
  doi = {10.1177/2378023117737206},
  abstract = {The ``crisis in science'' today is rooted in genuine problems of model uncertainty and lack of transparency. Researchers estimate a large number of models in the course of their research but only publish a small number of preferred results. Authors have much influence on the results of an empirical study through their choices about model specification. I advance methods to quantify the influence of the author---or at least demonstrate the scope an author has to choose a preferred result. Multimodel analysis, combined with modern computational power, allows authors to present their preferred estimate alongside a distribution of estimates from many other plausible models. I demonstrate the method using new software and applied empirical examples. When evaluating research results, accounting for model uncertainty and model robustness is at least as important as statistical significance.}
}

@article{yuan_posthoc,
  title = {On the {{Post Hoc Power}} in {{Testing Mean Differences}}},
  author = {Yuan, Ke-Hai and Maxwell, Scott},
  year = {2005},
  journal = {Journal of Educational and Behavioral Statistics Summer},
  volume = {30},
  pages = {141--167},
  doi = {10.3102/10769986030002141},
  abstract = {Retrospective or post hoc power analysis is recommended by reviewers and editors of many journals. Little literature has been found that gave a serious study of the post hoc power. When the sample size is large, the observed effect size is a good estimator of the true effect size. One would hope that the post hoc power is also a good estimator of the true power. This article studies whether such a power estimator provides valuable information about the true power. Using analytical, numerical, and Monte Carlo approaches, our results show that the estimated power does not provide useful information when the true power is small. It is almost always a biased estimator of the true power. The bias can be negative or positive. Large sample size alone does not guarantee the post hoc power to be a good estimator of the true power. Actually, when the population variance is known, the cumulative distribution function of the post hoc power is solely a function of the population power. This distribution is uniform when the true power equals 0.5 and highly skewed when the true power is near 0 or 1. When the population variance is unknown, the post hoc power behaves essentially the same as when the variance is known.},
  file = {/Users/cristian/Zotero/storage/W4VU43XT/Yuan and Maxwell - 2005 - On the Post Hoc Power in Testing Mean Differences.pdf}
}

@article{yuePeerAssessmentJournal2007,
  ids = {yuePeerAssessmentJournal2007a},
  title = {Peer Assessment of Journal Quality in Clinical Neurology},
  author = {Yue, Weiping and Wilson, Concepci{\'o}n S. and Boller, Francois},
  year = {2007},
  month = jan,
  journal = {Journal of the Medical Library Association: JMLA},
  volume = {95},
  number = {1},
  pages = {70--76},
  issn = {1558-9439},
  abstract = {OBJECTIVE: To explore journal quality as perceived by clinicians and researchers in clinical neurology. METHODS: A survey was conducted from August 2003 to January 2004. Ratings for 41 selected clinical neurology journals were obtained from 254 members of the World Federation of Neurology (1,500 solicited; response rate 17\%). Participants provided demographic information and rated each journal on a 5-point Likert scale. Average ratings for all journals were compared with the ISI's journal impact factors. Ratings for each journal were also compared across geographic regions and respondent publication productivity. RESULTS: The top 5 journals were rated much more highly than the others, with mean ratings greater than 4. Mean journal ratings were highly correlated with journal impact factors (r = 0.67). Most of the top 10 journal ratings were consistent across the subgroups of geographic regions and journal paper productivity. However, significant differences among the different geographical regions and respondent productivity groups were also found for a few journals. CONCLUSIONS: The results provide valuable insight on how neurological experts perceive journals in clinical neurology. These results will likely aid researchers and clinicians in identifying potentially desirable research outlets and indicate journal status for editors. Likewise, biomedical librarians may use these results for serials collection development.},
  langid = {english},
  pmcid = {PMC1773051},
  pmid = {17252069},
  keywords = {{Journalism, Medical},{Peer Review, Research},Attitude of Health Personnel,Evidence-Based Medicine,Humans,Neurology,Periodicals as Topic,Professional Competence,Surveys and Questionnaires},
  file = {/Users/cristian/Zotero/storage/WIESYZWH/Yue et al. - 2007 - Peer assessment of journal quality in clinical neu.pdf}
}

@article{yusufIntravenousMagnesiumAcute1993,
  title = {Intravenous Magnesium in Acute Myocardial Infarction. {{An}} Effective, Safe, Simple, and Inexpensive Intervention},
  author = {Yusuf, S. and Teo, K. and Woods, K.},
  year = {1993},
  month = jun,
  journal = {Circulation},
  volume = {87},
  number = {6},
  pages = {2043--2046},
  issn = {0009-7322},
  doi = {10.1161/01.cir.87.6.2043},
  langid = {english},
  pmid = {8504519},
  keywords = {{Infusions, Intravenous},{Injections, Intravenous},Clinical Trials as Topic,Humans,Magnesium Sulfate,Meta-Analysis as Topic,Myocardial Infarction},
  file = {/Users/cristian/Zotero/storage/L53E7ZT4/Yusuf et al. - 1993 - Intravenous magnesium in acute myocardial infarcti.pdf}
}

@article{zanettiImpactProteinSupplementation2020,
  title = {The Impact of Protein Supplementation Targeted at Improving Muscle Mass on Strength in Cancer Patients: {{A}} Scoping Review},
  author = {Zanetti, M. and Cappellari, G.G. and Barazzoni, R. and Sanson, G.},
  year = {2020},
  journal = {Nutrients},
  volume = {12},
  number = {7},
  pages = {1--16},
  publisher = {MDPI AG},
  doi = {10.3390/nu12072099}
}

@article{zarinHarmsUninformativeClinical2019,
  title = {Harms {{From Uninformative Clinical Trials}}},
  author = {Zarin, Deborah A. and Goodman, Steven N. and Kimmelman, Jonathan},
  year = {2019},
  month = sep,
  journal = {JAMA},
  volume = {322},
  number = {9},
  pages = {813},
  issn = {0098-7484},
  doi = {10.1001/jama.2019.9892},
  urldate = {2025-06-11},
  langid = {english}
}

@article{zengRegistrationPhase32020,
  title = {Registration of Phase 3 Crossover Trials on {{ClinicalTrials}}.Gov},
  author = {Zeng, Lijuan and Qureshi, Riaz and Viswanathan, Shilpa and Drye, Lea and Li, Tianjing},
  year = {2020},
  month = jul,
  journal = {Trials},
  volume = {21},
  number = {1},
  pages = {613},
  issn = {1745-6215},
  doi = {10.1186/s13063-020-04545-2},
  urldate = {2025-05-12},
  abstract = {In a randomized crossover trial, each participant is randomized to a sequence of treatments and treatment effect is estimated based on within-individual difference because each participant serves as his/her own control. This feature makes the design and reporting of randomized crossover trials different from that of parallel trials. Our objective was to characterize phase 3 crossover trials with results reported on ClinicalTrials.govand identify issues and best practices for reporting.},
  keywords = {ClinicalTrials.gov,Crossover trials,Data sharing,Longitudinal trial,Trial registration,Trial reporting},
  file = {/Users/cristian/Zotero/storage/SHQNZJQJ/Zeng et al. - 2020 - Registration of phase 3 crossover trials on Clinic.pdf;/Users/cristian/Zotero/storage/ZA82XZYU/s13063-020-04545-2.html}
}

@article{zhangCanControlledreleaseUrea2022,
  title = {Can Controlled-Release Urea Replace the Split Application of Normal Urea in {{China}}? {{A}} Meta-Analysis Based on Crop Grain Yield and Nitrogen Use Efficiency},
  author = {Zhang, G. and Zhao, D. and Liu, S. and Liao, Y. and Han, J.},
  year = {2022},
  journal = {Field Crops Research},
  volume = {275},
  publisher = {Elsevier B.V.},
  doi = {10.1016/j.fcr.2021.108343}
}

@article{zurawlewPostexerciseHotWater2016,
  title = {Post-Exercise Hot Water Immersion Induces Heat Acclimation and Improves Endurance Exercise Performance in the Heat},
  author = {Zurawlew, M. J. and Walsh, N. P. and Fortes, M. B. and Potter, C.},
  year = {2016},
  journal = {Scandinavian Journal of Medicine \& Science in Sports},
  volume = {26},
  number = {7},
  pages = {745--754},
  issn = {1600-0838},
  doi = {10.1111/sms.12638},
  urldate = {2023-07-20},
  abstract = {We examined whether daily hot water immersion (HWI) after exercise in temperate conditions induces heat acclimation and improves endurance performance in temperate and hot conditions. Seventeen non-heat-acclimatized males performed a 6-day intervention involving a daily treadmill run for 40 min at 65\% {\.V}O2max in temperate conditions (18 {$^\circ$}C) followed immediately by either HWI (N = 10; 40 {$^\circ$}C) or thermoneutral (CON, N = 7; 34 {$^\circ$}C) immersion for 40 min. Before and after the 6-day intervention, participants performed a treadmill run for 40 min at 65\% {\.V}O2max followed by a 5-km treadmill time trial (TT) in temperate (18 {$^\circ$}C, 40\% humidity) and hot (33 {$^\circ$}C, 40\% humidity) conditions. HWI induced heat acclimation demonstrated by lower resting rectal temperature (Tre, mean, -0.27 {$^\circ$}C, P {$<$} 0.01), and final Tre during submaximal exercise in 18 {$^\circ$}C (-0.28 {$^\circ$}C, P {$<$} 0.01) and 33 {$^\circ$}C (-0.36 {$^\circ$}C, P {$<$} 0.01). Skin temperature, Tre at sweating onset and RPE were lower during submaximal exercise in 18 {$^\circ$}C and 33 {$^\circ$}C after 6 days in HWI (P {$<$} 0.05). Physiological strain and thermal sensation were also lower during submaximal exercise in 33 {$^\circ$}C after 6 days in HWI (P {$<$} 0.05). HWI improved TT performance in 33 {$^\circ$}C (4.9\%, P {$<$} 0.01) but not in 18 {$^\circ$}C. Thermoregulatory measures and performance did not change in CON. Hot water immersion after exercise on 6 days presents a simple, practical, and effective heat acclimation strategy to improve endurance performance in the heat.},
  copyright = {{\copyright} 2015 John Wiley \& Sons A/S. Published by John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {heat illness,hot bath,hyperthermia,perfor-mance,running,thermoregulation},
  file = {/Users/cristian/Zotero/storage/LBXC9ZWP/Zurawlew et al. - 2016 - Post-exercise hot water immersion induces heat acc.pdf;/Users/cristian/Zotero/storage/3UA7LYKX/sms.html}
}
